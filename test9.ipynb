{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin_europe</th>\n",
       "      <th>origin_japan</th>\n",
       "      <th>origin_usa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0  18.0          8         307.0       130.0    3504          12.0   \n",
       "1  15.0          8         350.0       165.0    3693          11.5   \n",
       "2  18.0          8         318.0       150.0    3436          11.0   \n",
       "3  16.0          8         304.0       150.0    3433          12.0   \n",
       "4  17.0          8         302.0       140.0    3449          10.5   \n",
       "\n",
       "   model_year  origin_europe  origin_japan  origin_usa  \n",
       "0          70              0             0           1  \n",
       "1          70              0             0           1  \n",
       "2          70              0             0           1  \n",
       "3          70              0             0           1  \n",
       "4          70              0             0           1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset(\"mpg\")\n",
    "\n",
    "X_train, y_train = df[~df[\"horsepower\"].isna()][[\"displacement\", \"acceleration\"]], df[~df[\"horsepower\"].isna()][\"horsepower\"]\n",
    "X_pred = df[df[\"horsepower\"].isna()][[\"displacement\", \"acceleration\"]]\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_pred)\n",
    "y_pred = np.round(y_pred)\n",
    "df.loc[X_pred.index, \"horsepower\"] = y_pred\n",
    "df = pd.get_dummies(df.drop(\"name\", axis = 1), columns = [\"origin\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop([\"mpg\"], axis = 1).values, df[\"mpg\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_val = scaler.transform(X_val)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorRegressor:\n",
    "    def __init__(self, n = 100, hidden_layers = False, activation = \"sigmoid\", random_state = None, verbose = 0):\n",
    "\n",
    "        self.n = n // 2 * 2\n",
    "        self.nets = []\n",
    "        self.best_net = -1\n",
    "        self.best_result = None\n",
    "        self.validation_loss_history = []\n",
    "        self.training_loss_history = []\n",
    "        self.mutation_sigma = 0\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif activation == \"relu\":\n",
    "            self.activation_function = lambda x: np.maximum(0, x)\n",
    "        elif activation == \"leaky_relu\":\n",
    "            self.activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        \n",
    "        if hidden_layers:\n",
    "            self.layers = hidden_layers + [1]\n",
    "        else:\n",
    "            self.layers = [1]\n",
    "        \n",
    "        if random_state != None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "        self.verbose = verbose\n",
    "\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs = 100, validation_data = False, verbose = 0):\n",
    "        X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "\n",
    "        if validation_data:\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        self.layers = [X_train.shape[1]] + self.layers\n",
    "\n",
    "        self.y_preds = np.zeros((self.n, y_train.shape[0]))\n",
    "        self.nets_loss = np.zeros(self.n)\n",
    "        self.sorted_indecies = np.zeros(self.n)\n",
    "\n",
    "        self.weights = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.weights += [np.random.uniform(-3, 3, (self.n, self.layers[i], self.layers[i + 1]))]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            forward_pass = X_train.T\n",
    "            \n",
    "            for j in range(0, len(self.layers) - 2):\n",
    "                forward_pass = self.activation_function(self.weights[j].transpose(0, 2, 1) @ forward_pass)\n",
    "\n",
    "            forward_pass = self.weights[-1].transpose(0, 2, 1) @ forward_pass\n",
    "            self.y_preds = forward_pass.reshape(self.n, -1)\n",
    "\n",
    "            self.nets_loss = np.mean(np.abs(self.y_preds - y_train), axis = 1)\n",
    "\n",
    "            self.sorted_indecies = np.argsort(self.nets_loss)\n",
    "\n",
    "            self.mutation_sigma = 0.1 + 5 * 1 / math.exp(epoch / ((epochs + 1) / (100 * math.log10(epochs + 1))))\n",
    "\n",
    "            for j in range(0, len(self.layers) - 1):\n",
    "                self.weights[j][self.sorted_indecies[50::2]] = np.mean((self.weights[j][self.sorted_indecies[:50:2]], self.weights[j][self.sorted_indecies[1:51:2]]), axis = 0) + np.random.normal(0, self.mutation_sigma, (self.n // 4, self.layers[j], self.layers[j + 1]))\n",
    "                self.weights[j][self.sorted_indecies[51::2]] = np.mean((self.weights[j][self.sorted_indecies[:50:2]], self.weights[j][self.sorted_indecies[1:51:2]]), axis = 0) + np.random.normal(0, self.mutation_sigma, (self.n // 4, self.layers[j], self.layers[j + 1]))\n",
    "\n",
    "            if self.best_net != self.sorted_indecies[0]:\n",
    "                self.best_net = self.sorted_indecies[0]\n",
    "                self.training_loss_history += [self.nets_loss[self.best_net]]\n",
    "                \n",
    "                if validation_data:\n",
    "                    self.validation_loss_history += [np.mean(np.abs(y_val - self.predict(X_val)))]\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]}\")\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        pass\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        forward_pass = X.T\n",
    "        for j in range(0, len(self.layers) - 2):\n",
    "            forward_pass = self.activation_function(self.weights[j][self.best_net].T @ forward_pass)\n",
    "\n",
    "        forward_pass = self.weights[-1][self.best_net].T @ forward_pass\n",
    "        return forward_pass.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 19.5278995791053 - val_loss: 17.85078778100287\n",
      "Epoch 1 - loss: 8.53922295450218 - val_loss: 7.749019880166648\n",
      "Epoch 2 - loss: 8.105124356386156 - val_loss: 7.355032976950125\n",
      "Epoch 3 - loss: 7.6355299064000235 - val_loss: 6.825480188004034\n",
      "Epoch 4 - loss: 7.248511940427422 - val_loss: 6.119392352756424\n",
      "Epoch 5 - loss: 6.595255763238522 - val_loss: 6.216913137674284\n",
      "Epoch 6 - loss: 5.962352925178464 - val_loss: 5.909876561064507\n",
      "Epoch 7 - loss: 5.705672637323295 - val_loss: 5.2959872859993125\n",
      "Epoch 8 - loss: 5.08855254840753 - val_loss: 4.811643063551418\n",
      "Epoch 9 - loss: 4.9575643088985855 - val_loss: 4.705323341573136\n",
      "Epoch 10 - loss: 4.866797189808337 - val_loss: 4.212939746113839\n",
      "Epoch 11 - loss: 4.619727559057142 - val_loss: 4.069375569182223\n",
      "Epoch 12 - loss: 4.597776310430236 - val_loss: 4.0540780530942415\n",
      "Epoch 13 - loss: 4.378934033595037 - val_loss: 3.696208166631709\n",
      "Epoch 14 - loss: 4.326582479741205 - val_loss: 3.8482042155777463\n",
      "Epoch 17 - loss: 4.158824498685969 - val_loss: 3.528976744939483\n",
      "Epoch 20 - loss: 4.087910800953734 - val_loss: 3.524550749894987\n",
      "Epoch 21 - loss: 4.059449600356452 - val_loss: 3.4394072961515043\n",
      "Epoch 22 - loss: 4.01544275209108 - val_loss: 3.445385833543662\n",
      "Epoch 23 - loss: 3.945229073174046 - val_loss: 3.2792714038446094\n",
      "Epoch 25 - loss: 3.9395440060170643 - val_loss: 3.3727345596849885\n",
      "Epoch 26 - loss: 3.90175826324371 - val_loss: 3.429420910687285\n",
      "Epoch 27 - loss: 3.840184144625945 - val_loss: 3.2942490970485236\n",
      "Epoch 29 - loss: 3.781735745195247 - val_loss: 3.190083744360426\n",
      "Epoch 30 - loss: 3.755152361515854 - val_loss: 3.0590442227722043\n",
      "Epoch 31 - loss: 3.7490382239771898 - val_loss: 3.1345230498350847\n",
      "Epoch 32 - loss: 3.716736732245237 - val_loss: 3.207994476319029\n",
      "Epoch 33 - loss: 3.6665088971384128 - val_loss: 2.9819943774150723\n",
      "Epoch 34 - loss: 3.63149943395545 - val_loss: 3.066293713743568\n",
      "Epoch 36 - loss: 3.583495282622703 - val_loss: 2.8375434226884053\n",
      "Epoch 37 - loss: 3.5570378098534605 - val_loss: 2.9488156529537095\n",
      "Epoch 39 - loss: 3.522086641293718 - val_loss: 2.9315749790149805\n",
      "Epoch 40 - loss: 3.4954515629368736 - val_loss: 2.885269960581232\n",
      "Epoch 41 - loss: 3.4457043519773234 - val_loss: 2.8635548036106333\n",
      "Epoch 44 - loss: 3.4113106529196595 - val_loss: 2.7612716926828123\n",
      "Epoch 46 - loss: 3.389242361424722 - val_loss: 2.8036435417845444\n",
      "Epoch 47 - loss: 3.373462794846166 - val_loss: 2.951353899074902\n",
      "Epoch 48 - loss: 3.3348112955005695 - val_loss: 2.7454681553257516\n",
      "Epoch 49 - loss: 3.2903808308762725 - val_loss: 2.7454835206788295\n",
      "Epoch 50 - loss: 3.2892336528462267 - val_loss: 2.789903146410734\n",
      "Epoch 51 - loss: 3.256821447906593 - val_loss: 2.580404451425861\n",
      "Epoch 52 - loss: 3.1921522607061363 - val_loss: 2.545144035892309\n",
      "Epoch 54 - loss: 3.105908275337245 - val_loss: 2.6806470107830007\n",
      "Epoch 57 - loss: 3.0652297452152344 - val_loss: 2.4730036215006588\n",
      "Epoch 59 - loss: 3.0572204032021384 - val_loss: 2.5203082642711765\n",
      "Epoch 60 - loss: 3.038732947704117 - val_loss: 2.4268340283429346\n",
      "Epoch 61 - loss: 3.034312094551235 - val_loss: 2.510602731415223\n",
      "Epoch 62 - loss: 2.985263561986264 - val_loss: 2.508788642138632\n",
      "Epoch 63 - loss: 2.9835913635143156 - val_loss: 2.5345904001701607\n",
      "Epoch 64 - loss: 2.9784229152329407 - val_loss: 2.4615543587611555\n",
      "Epoch 65 - loss: 2.954540591943685 - val_loss: 2.427604734416615\n",
      "Epoch 67 - loss: 2.916587878290556 - val_loss: 2.4490653911141544\n",
      "Epoch 68 - loss: 2.9068022660448674 - val_loss: 2.4347856679409206\n",
      "Epoch 70 - loss: 2.8772668790264926 - val_loss: 2.4012510106415177\n",
      "Epoch 71 - loss: 2.86128202516585 - val_loss: 2.38648036414163\n",
      "Epoch 73 - loss: 2.8591963013064166 - val_loss: 2.4155144660547725\n",
      "Epoch 74 - loss: 2.8354720385282657 - val_loss: 2.3992553560812553\n",
      "Epoch 75 - loss: 2.828867010048118 - val_loss: 2.380073027808927\n",
      "Epoch 76 - loss: 2.8073823460189034 - val_loss: 2.370789442496866\n",
      "Epoch 77 - loss: 2.8065819471732394 - val_loss: 2.3418989291189556\n",
      "Epoch 78 - loss: 2.7961594677012243 - val_loss: 2.394364292411327\n",
      "Epoch 79 - loss: 2.7720624422939597 - val_loss: 2.357309699472178\n",
      "Epoch 80 - loss: 2.76955098855352 - val_loss: 2.3926906381357957\n",
      "Epoch 81 - loss: 2.7567972754667465 - val_loss: 2.3416632928648804\n",
      "Epoch 83 - loss: 2.741612633495084 - val_loss: 2.31455584124206\n",
      "Epoch 85 - loss: 2.7397967477005833 - val_loss: 2.291263928651148\n",
      "Epoch 86 - loss: 2.731263783112486 - val_loss: 2.3114998627789\n",
      "Epoch 87 - loss: 2.7162185827537204 - val_loss: 2.3453698672431122\n",
      "Epoch 88 - loss: 2.7077203147532094 - val_loss: 2.2976752591891096\n",
      "Epoch 89 - loss: 2.6902993123828463 - val_loss: 2.285498405789157\n",
      "Epoch 91 - loss: 2.672808054667506 - val_loss: 2.2680294924588225\n",
      "Epoch 92 - loss: 2.6620639497939806 - val_loss: 2.2924029783615447\n",
      "Epoch 93 - loss: 2.653448804391869 - val_loss: 2.2456684903681428\n",
      "Epoch 94 - loss: 2.6476903804276795 - val_loss: 2.2795494821941187\n",
      "Epoch 95 - loss: 2.626964124190305 - val_loss: 2.2048617282052154\n",
      "Epoch 97 - loss: 2.6216189459480175 - val_loss: 2.212746190563161\n",
      "Epoch 99 - loss: 2.600763357691429 - val_loss: 2.253390213096501\n",
      "Epoch 100 - loss: 2.5580719037871633 - val_loss: 2.153343464797764\n",
      "Epoch 103 - loss: 2.5541476849781524 - val_loss: 2.1667391886242333\n",
      "Epoch 104 - loss: 2.552817925732028 - val_loss: 2.157943685968502\n",
      "Epoch 105 - loss: 2.5524999469669774 - val_loss: 2.152706122357374\n",
      "Epoch 106 - loss: 2.533755716635649 - val_loss: 2.1381853913485487\n",
      "Epoch 108 - loss: 2.5260016833471766 - val_loss: 2.146901756251469\n",
      "Epoch 111 - loss: 2.516553847061663 - val_loss: 2.0837737304399555\n",
      "Epoch 112 - loss: 2.5149840180029397 - val_loss: 2.142569032945139\n",
      "Epoch 113 - loss: 2.4985326292304246 - val_loss: 2.105854145797059\n",
      "Epoch 115 - loss: 2.4826882432676975 - val_loss: 2.094200243107383\n",
      "Epoch 116 - loss: 2.470226811075512 - val_loss: 2.0990883602977526\n",
      "Epoch 119 - loss: 2.4677172123911326 - val_loss: 2.0552397328274226\n",
      "Epoch 120 - loss: 2.467073981329897 - val_loss: 2.1166955276977104\n",
      "Epoch 121 - loss: 2.46275428075863 - val_loss: 2.0970867056288554\n",
      "Epoch 122 - loss: 2.446696063292801 - val_loss: 2.0589265950045563\n",
      "Epoch 124 - loss: 2.43931710017879 - val_loss: 2.0578925431426365\n",
      "Epoch 125 - loss: 2.4381274741617496 - val_loss: 2.016735643792001\n",
      "Epoch 127 - loss: 2.420295300531501 - val_loss: 2.0048312366267234\n",
      "Epoch 129 - loss: 2.4158634968165833 - val_loss: 2.0161548646080583\n",
      "Epoch 130 - loss: 2.41253566811996 - val_loss: 2.046463381210214\n",
      "Epoch 131 - loss: 2.4103679579692887 - val_loss: 1.9628904785654864\n",
      "Epoch 132 - loss: 2.408215119146312 - val_loss: 1.95851977787918\n",
      "Epoch 134 - loss: 2.392278713050124 - val_loss: 2.0743596002403324\n",
      "Epoch 135 - loss: 2.388580457984565 - val_loss: 2.06176335137487\n",
      "Epoch 137 - loss: 2.3777945317775466 - val_loss: 2.0307252299992116\n",
      "Epoch 138 - loss: 2.3748319020864703 - val_loss: 2.020747035370546\n",
      "Epoch 139 - loss: 2.374660273231503 - val_loss: 1.980765748164302\n",
      "Epoch 140 - loss: 2.36939663263752 - val_loss: 2.0402740957178396\n",
      "Epoch 141 - loss: 2.366251544613352 - val_loss: 2.0178467489457876\n",
      "Epoch 143 - loss: 2.3407912903714005 - val_loss: 1.9676426410369428\n",
      "Epoch 147 - loss: 2.331942799023034 - val_loss: 1.8919470418375635\n",
      "Epoch 148 - loss: 2.322010418362529 - val_loss: 1.9328991910532616\n",
      "Epoch 149 - loss: 2.3208592131854493 - val_loss: 1.9414146436321267\n",
      "Epoch 151 - loss: 2.3006196772698964 - val_loss: 1.9381241787397385\n",
      "Epoch 154 - loss: 2.2930597806143274 - val_loss: 2.017956150317328\n",
      "Epoch 155 - loss: 2.2886062121637827 - val_loss: 1.9178013584789035\n",
      "Epoch 156 - loss: 2.2868200300169494 - val_loss: 1.9189780857051648\n",
      "Epoch 157 - loss: 2.2812216184728897 - val_loss: 1.9651209971980694\n",
      "Epoch 158 - loss: 2.271658330856892 - val_loss: 1.9585415415327803\n",
      "Epoch 159 - loss: 2.2694660348337408 - val_loss: 1.9483097305245498\n",
      "Epoch 160 - loss: 2.26448516880315 - val_loss: 1.975364996194599\n",
      "Epoch 161 - loss: 2.254759043950003 - val_loss: 1.9687547098404834\n",
      "Epoch 162 - loss: 2.2531528170028268 - val_loss: 1.932175320776786\n",
      "Epoch 165 - loss: 2.2458234030845787 - val_loss: 1.9351929651793944\n",
      "Epoch 166 - loss: 2.2381553670506364 - val_loss: 1.9087694342701655\n",
      "Epoch 168 - loss: 2.228086886061793 - val_loss: 1.8826825445977988\n",
      "Epoch 171 - loss: 2.2186066769978963 - val_loss: 1.8902324969733193\n",
      "Epoch 172 - loss: 2.20672524767816 - val_loss: 1.8706286135287091\n",
      "Epoch 176 - loss: 2.205365910710768 - val_loss: 1.8800563120009488\n",
      "Epoch 178 - loss: 2.1945344971065146 - val_loss: 1.8581372453417409\n",
      "Epoch 180 - loss: 2.190178943026149 - val_loss: 1.8155197541039683\n",
      "Epoch 181 - loss: 2.180527642147494 - val_loss: 1.8328462492144726\n",
      "Epoch 183 - loss: 2.1743416409976333 - val_loss: 1.9229850371362267\n",
      "Epoch 185 - loss: 2.170161877515185 - val_loss: 1.8647644512278745\n",
      "Epoch 188 - loss: 2.168569049991507 - val_loss: 1.8465193531751005\n",
      "Epoch 189 - loss: 2.1649532037962875 - val_loss: 1.84520178439322\n",
      "Epoch 190 - loss: 2.16121141685059 - val_loss: 1.803127245169324\n",
      "Epoch 191 - loss: 2.152451218142575 - val_loss: 1.8235986552979433\n",
      "Epoch 193 - loss: 2.1472783455304536 - val_loss: 1.8285720568840822\n",
      "Epoch 195 - loss: 2.143488816929084 - val_loss: 1.8042257959662955\n",
      "Epoch 196 - loss: 2.1387973618808793 - val_loss: 1.8012103889452802\n",
      "Epoch 199 - loss: 2.138246577411129 - val_loss: 1.8329602872035935\n",
      "Epoch 200 - loss: 2.130871881029953 - val_loss: 1.7923336099180556\n",
      "Epoch 206 - loss: 2.1176566320270496 - val_loss: 1.824800197403724\n",
      "Epoch 209 - loss: 2.113979535146749 - val_loss: 1.7952174581925178\n",
      "Epoch 210 - loss: 2.1107707890101444 - val_loss: 1.7134706263245427\n",
      "Epoch 213 - loss: 2.1041931601459973 - val_loss: 1.7956578993381176\n",
      "Epoch 218 - loss: 2.103455151033704 - val_loss: 1.7405538171870667\n",
      "Epoch 220 - loss: 2.0946250866605767 - val_loss: 1.7154622931491972\n",
      "Epoch 225 - loss: 2.090260464497064 - val_loss: 1.7238345685008127\n",
      "Epoch 227 - loss: 2.089897219922161 - val_loss: 1.7607350670386197\n",
      "Epoch 229 - loss: 2.0872111384303635 - val_loss: 1.7320353118452765\n",
      "Epoch 230 - loss: 2.0863453142459334 - val_loss: 1.6874057185924947\n",
      "Epoch 232 - loss: 2.078334739028095 - val_loss: 1.7246680275855177\n",
      "Epoch 235 - loss: 2.0756900827593663 - val_loss: 1.6988249660676735\n",
      "Epoch 236 - loss: 2.0754042684602894 - val_loss: 1.7160532159046213\n",
      "Epoch 237 - loss: 2.069168154769168 - val_loss: 1.6885839586722065\n",
      "Epoch 241 - loss: 2.0665035725183034 - val_loss: 1.6621629623197343\n",
      "Epoch 243 - loss: 2.0641010517746756 - val_loss: 1.7021340678527537\n",
      "Epoch 247 - loss: 2.063180241205436 - val_loss: 1.720450906415443\n",
      "Epoch 248 - loss: 2.0631448052235397 - val_loss: 1.7065005878283244\n",
      "Epoch 252 - loss: 2.0629987953287627 - val_loss: 1.7253552463365964\n",
      "Epoch 253 - loss: 2.0613910853476898 - val_loss: 1.697819367092217\n",
      "Epoch 254 - loss: 2.0571632841200187 - val_loss: 1.7508777610374622\n",
      "Epoch 257 - loss: 2.056677691938043 - val_loss: 1.6833443095297924\n",
      "Epoch 258 - loss: 2.051031453346684 - val_loss: 1.6952134836603512\n",
      "Epoch 262 - loss: 2.0461737305447643 - val_loss: 1.6949689167864057\n",
      "Epoch 265 - loss: 2.0440400904139926 - val_loss: 1.682525633378085\n",
      "Epoch 266 - loss: 2.038882599405671 - val_loss: 1.6609605152594198\n",
      "Epoch 272 - loss: 2.032913100780535 - val_loss: 1.7054408148112592\n",
      "Epoch 275 - loss: 2.031795377780231 - val_loss: 1.7082379380623969\n",
      "Epoch 276 - loss: 2.0298552455894465 - val_loss: 1.7009383508022489\n",
      "Epoch 280 - loss: 2.0215996771987306 - val_loss: 1.655080681442905\n",
      "Epoch 282 - loss: 2.0126128617746426 - val_loss: 1.6589379294081277\n",
      "Epoch 283 - loss: 2.0118152867970474 - val_loss: 1.6069124288321106\n",
      "Epoch 286 - loss: 2.010690576613796 - val_loss: 1.6802585840881004\n",
      "Epoch 288 - loss: 1.9953031848186729 - val_loss: 1.6210657518998937\n",
      "Epoch 299 - loss: 1.9791506546907003 - val_loss: 1.645383047473657\n",
      "Epoch 310 - loss: 1.9775493756194082 - val_loss: 1.6047299643689565\n",
      "Epoch 316 - loss: 1.9763382033582886 - val_loss: 1.6093383798787972\n",
      "Epoch 318 - loss: 1.9696986891182824 - val_loss: 1.6179052837588386\n",
      "Epoch 322 - loss: 1.9628177419422137 - val_loss: 1.6096718267690395\n",
      "Epoch 323 - loss: 1.9564605239274042 - val_loss: 1.5882310090762652\n",
      "Epoch 334 - loss: 1.9530993295871852 - val_loss: 1.5958575355965998\n",
      "Epoch 336 - loss: 1.9525768382873583 - val_loss: 1.5460880688332068\n",
      "Epoch 339 - loss: 1.9522615858011614 - val_loss: 1.5390269321146803\n",
      "Epoch 341 - loss: 1.9503663281734565 - val_loss: 1.5819635538371337\n",
      "Epoch 342 - loss: 1.9425157137547533 - val_loss: 1.5637034548957576\n",
      "Epoch 344 - loss: 1.939965324813301 - val_loss: 1.594578678798717\n",
      "Epoch 352 - loss: 1.9368922265392476 - val_loss: 1.599206877528761\n",
      "Epoch 357 - loss: 1.932050854906724 - val_loss: 1.5612771436770336\n",
      "Epoch 362 - loss: 1.931786718408765 - val_loss: 1.534232822907828\n",
      "Epoch 365 - loss: 1.9290312660060822 - val_loss: 1.5785668896286282\n",
      "Epoch 370 - loss: 1.9224594909102823 - val_loss: 1.5604222360658495\n",
      "Epoch 376 - loss: 1.9156794033247078 - val_loss: 1.5803352520463652\n",
      "Epoch 399 - loss: 1.910491541016119 - val_loss: 1.592359711124077\n",
      "Epoch 410 - loss: 1.9088636226925055 - val_loss: 1.5687597238631414\n",
      "Epoch 415 - loss: 1.906777921152472 - val_loss: 1.5845018253883185\n",
      "Epoch 420 - loss: 1.904994335625878 - val_loss: 1.5467437938954987\n",
      "Epoch 421 - loss: 1.9042141713038263 - val_loss: 1.5722781245559219\n",
      "Epoch 426 - loss: 1.9038222055976581 - val_loss: 1.614088201087703\n",
      "Epoch 435 - loss: 1.9014155626079372 - val_loss: 1.557289277577437\n",
      "Epoch 444 - loss: 1.8999807443449992 - val_loss: 1.5831357863260072\n",
      "Epoch 448 - loss: 1.8997405911871028 - val_loss: 1.5786314742993999\n",
      "Epoch 457 - loss: 1.8961375489556127 - val_loss: 1.5909793770065743\n",
      "Epoch 468 - loss: 1.8958985059801123 - val_loss: 1.607868618135079\n",
      "Epoch 493 - loss: 1.891877736402112 - val_loss: 1.5894526302437062\n",
      "Epoch 519 - loss: 1.8912239187128228 - val_loss: 1.5759309118371678\n",
      "Epoch 523 - loss: 1.8898383382673392 - val_loss: 1.548183007921391\n",
      "Epoch 526 - loss: 1.8876683324623278 - val_loss: 1.5798956532938344\n",
      "Epoch 527 - loss: 1.8855033707683204 - val_loss: 1.564772966888118\n",
      "Epoch 581 - loss: 1.8846398642912936 - val_loss: 1.5690165927782247\n",
      "Epoch 631 - loss: 1.8831945466002373 - val_loss: 1.5617754101804664\n",
      "Epoch 737 - loss: 1.8802651254014227 - val_loss: 1.5774625289454918\n",
      "Epoch 869 - loss: 1.8798861226790455 - val_loss: 1.5677815731459095\n",
      "Epoch 883 - loss: 1.8792230069290727 - val_loss: 1.557792088892947\n",
      "Epoch 902 - loss: 1.878593061986745 - val_loss: 1.6045722768848012\n",
      "Epoch 968 - loss: 1.8775126835546516 - val_loss: 1.5824181362920051\n",
      "Epoch 991 - loss: 1.8764231170276393 - val_loss: 1.5845675877060101\n"
     ]
    }
   ],
   "source": [
    "regressor = VectorRegressor(n = 100, hidden_layers = [4], activation = \"sigmoid\", random_state = 42)\n",
    "regressor.fit(scaled_X_train, y_train, epochs = 1000, validation_data = (scaled_X_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on test data: 1.6363038572394935\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = regressor.predict(scaled_X_test)\n",
    "print(f\"Loss on test data: {mean_absolute_error(y_test, y_pred)}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(regressor.training_loss_history)\n",
    "ax.plot(regressor.validation_loss_history)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12.540278541304668,\n",
       " 11.31345956102197,\n",
       " 9.29034471983734,\n",
       " 9.11636371228251,\n",
       " 7.640114282614082,\n",
       " 6.505026080976028,\n",
       " 6.3570673091791425,\n",
       " 5.51358221946053,\n",
       " 5.33594391669974,\n",
       " 4.986325216071821,\n",
       " 4.9410857045291685,\n",
       " 4.790030202248466,\n",
       " 4.749067443819467,\n",
       " 3.8972344058192774,\n",
       " 3.6925757484190127,\n",
       " 3.68303803091034,\n",
       " 3.5431420061352927,\n",
       " 3.366475849355694,\n",
       " 3.079655574779169,\n",
       " 2.9474978762440234,\n",
       " 2.901172241620006,\n",
       " 2.5159325174712728,\n",
       " 2.4443395138624786,\n",
       " 2.430107207574906,\n",
       " 2.418759455752082,\n",
       " 2.414501076460732,\n",
       " 2.380818836380886,\n",
       " 2.3396561542751626,\n",
       " 2.323674960444538,\n",
       " 2.3072072124522407,\n",
       " 2.3064289079221982,\n",
       " 2.2739165934330403,\n",
       " 2.218879978307752,\n",
       " 2.218539046926688,\n",
       " 2.21300746455699,\n",
       " 2.201819536484139,\n",
       " 2.1997369508970803,\n",
       " 2.182749551527073,\n",
       " 2.181427662245525,\n",
       " 2.166911377581538,\n",
       " 2.1645932766932146,\n",
       " 2.1549412618691726,\n",
       " 2.1499137750488306,\n",
       " 2.14982278419821,\n",
       " 2.1493195963487417,\n",
       " 2.149128094058345,\n",
       " 2.1337333113412806,\n",
       " 2.126550673726339,\n",
       " 2.1244119746258043,\n",
       " 2.1192398212422443,\n",
       " 2.1182093308707435,\n",
       " 2.1147810473293625,\n",
       " 2.111570095737522,\n",
       " 2.1108423165370094,\n",
       " 2.110698410780774,\n",
       " 2.1102455688949497,\n",
       " 2.099719564351566,\n",
       " 2.099410492075848,\n",
       " 2.0985132051867272,\n",
       " 2.092747017570776,\n",
       " 2.0924523966380115,\n",
       " 2.0924320215361143,\n",
       " 2.0921001093638285,\n",
       " 2.08945395105671]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.training_loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = vregressor.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.64140437, 34.21617117, 24.74893805, 38.62679494, 15.876758  ,\n",
       "       20.02510414, 13.46302104, 34.90722375, 13.29632817, 23.60215502,\n",
       "       26.86724428, 23.19228495, 35.32786318, 12.72546595, 27.18078347,\n",
       "       34.17316901, 18.61793492, 33.68498309, 11.87646859, 24.63018934,\n",
       "       18.42963081, 18.84493869, 25.39409425, 22.80014085, 34.74881644,\n",
       "       22.47990386, 21.1984451 , 27.10008261, 31.60438038, 26.05328812,\n",
       "       17.96848593, 18.22051233, 11.42318815, 20.31168452, 11.55482496,\n",
       "       37.67098392, 23.50907463, 45.18838465, 21.93169879, 14.97971544])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6244880201432412"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_function = np.vectorize(lambda x: 1 / (1 + math.exp(-x)))\n",
    "\n",
    "w = [np.array([[[0, 3],[2, 0]],[[1, 3],[9, 7]],[[5, 8],[3, 3]]]),\n",
    "    np.array([[[2],[6]],[[0],[4]],[[6],[9]]])]\n",
    "#w1 = np.array([[[0, 3],[2, 0]],[[1, 3],[9, 7]],[[5, 8],[3, 3]]])\n",
    "#w2 = np.array([[[2],[6]],[[0],[4]],[[6],[9]]])\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "B = np.array([  [8, 0],\n",
    "                [9, 2],\n",
    "                [6, 3]])\n",
    "\n",
    "pred = []\n",
    "\n",
    "for i in range(3):\n",
    "    forward_pass = B.T\n",
    "    forward_pass = activation_function(w[0][i].T @ forward_pass)\n",
    "\n",
    "    forward_pass = w[1][i].T @ forward_pass\n",
    "\n",
    "    pred.append(forward_pass.reshape(-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([7.        , 7.96402758, 7.99505466]),\n",
       " array([4., 4., 4.]),\n",
       " array([15., 15., 15.])]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "forward_pass = B.T\n",
    "forward_pass = activation_function(w[0].transpose(0, 2, 1) @ forward_pass)\n",
    "forward_pass = w[1].transpose(0, 2, 1) @ forward_pass\n",
    "pred = forward_pass.reshape(3, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.        ,  7.96402758,  7.99505466],\n",
       "       [ 4.        ,  4.        ,  4.        ],\n",
       "       [15.        , 15.        , 15.        ]])"
      ]
     },
     "execution_count": 365,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.reshape(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 1)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([48, 58, 42, 72, 93, 72]),\n",
       " array([56, 73, 57, 16, 26, 24]),\n",
       " array([32, 54, 51,  0, 10, 15])]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unsupported dtype dtype('O') for randint",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/test8.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/test8.ipynb#ch0000023?line=0'>1</a>\u001b[0m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mrandint(\u001b[39m0\u001b[39;49m, \u001b[39m10\u001b[39;49m, (\u001b[39m2\u001b[39;49m, \u001b[39m2\u001b[39;49m, (\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m)), dtype \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mobject\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32mmtrand.pyx:764\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unsupported dtype dtype('O') for randint"
     ]
    }
   ],
   "source": [
    "np.random.randint(0, 10, (2, 2, (1, 1)), dtype = \"object\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47d3b7ff548c1bae2d6b155a9b3d6f1122689b634566f833764ba5dd9fcfa2e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Deep-learning-Daniel-Petersson-bXusHwTH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
