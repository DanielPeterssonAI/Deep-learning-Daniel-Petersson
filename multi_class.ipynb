{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "[[1, 1, 1], [1, 0, 1], [1, 0, 1], [1, 0, 1], [1, 1, 1]],\n",
    "[[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]],\n",
    "[[1, 1, 1], [0, 0, 1], [1, 1, 1], [1, 0, 0], [1, 1, 1]],\n",
    "[[1, 1, 1], [0, 0, 1], [1, 1, 1], [0, 0, 1], [1, 1, 1]],\n",
    "[[1, 0, 1], [1, 0, 1], [1, 1, 1], [0, 0, 1], [0, 0, 1]],\n",
    "[[1, 0, 1], [1, 0, 1], [1, 1, 1], [0, 0, 1], [0, 0, 1]],\n",
    "[[1, 1, 1], [1, 0, 0], [1, 1, 1], [0, 0, 1], [1, 1, 1]],\n",
    "[[1, 1, 1], [1, 0, 0], [1, 1, 1], [1, 0, 1], [1, 1, 1]],\n",
    "[[1, 1, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]],\n",
    "[[1, 1, 1], [1, 0, 1], [1, 1, 1], [1, 0, 1], [1, 1, 1]],\n",
    "[[1, 1, 1], [1, 0, 1], [1, 1, 1], [0, 0, 1], [1, 1, 1]]]).reshape(11, -1)\n",
    "\n",
    "y = pd.get_dummies(pd.DataFrame({\"y\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]}), drop_first = False).values.astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=10, max_iter=5000, random_state=42)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes = (10), activation = \"relu\", random_state = 42, max_iter = 5000)\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedEvoClassifierM01:\n",
    "    def __init__(self, n = 20, hidden_layers = False, activation = \"relu\", random_state = None):\n",
    "\n",
    "        self.n = n // 2 * 2\n",
    "        self.validation_loss_history = []\n",
    "        self.training_loss_history = []\n",
    "        self.random_state = random_state\n",
    "        self.activation = activation\n",
    "        self.number_of_layers = 0\n",
    "        \n",
    "        if hidden_layers:\n",
    "            self.layers = hidden_layers + [10]\n",
    "        else:\n",
    "            self.layers = [10]\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs = 100, validation_data = False, verbose = 0):\n",
    "\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        if validation_data:\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        else:\n",
    "            activation_function = lambda x: np.maximum(0, x)\n",
    "\n",
    "        #output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        output_activation_function = lambda x: np.exp(x) / np.sum(np.exp(x), axis = 2, keepdims = True)\n",
    "\n",
    "        X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "        y_train = y_train.astype(\"int8\")\n",
    "\n",
    "        n = self.n\n",
    "        layers = [X_train.shape[1]] + self.layers\n",
    "        number_of_layers_minus_one = len(layers) - 1\n",
    "        y_preds = np.zeros((n, y_train.shape[0], y_train.shape[1]))\n",
    "        nets_loss = np.zeros(n)\n",
    "        sorted_indices = np.arange(-(n // 2), n, 1)\n",
    "        #sorted_indices = np.zeros(n)\n",
    "        best_net_index = -1\n",
    "        weights = []\n",
    "\n",
    "        for i in range(number_of_layers_minus_one):\n",
    "            weights += [np.random.normal(0, 1, (n, layers[i], layers[i + 1]))]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            forward_pass = X_train.T\n",
    "            \n",
    "            for j in range(number_of_layers_minus_one - 1):\n",
    "                forward_pass = activation_function(weights[j][sorted_indices[n // 2:]].transpose(0, 2, 1) @ forward_pass)\n",
    "            \n",
    "            forward_pass = weights[-1][sorted_indices[n // 2:]].transpose(0, 2, 1) @ forward_pass\n",
    "            \n",
    "            y_preds[sorted_indices[n // 2:]] = output_activation_function(forward_pass.transpose(0, 2, 1))\n",
    "\n",
    "            nets_loss[sorted_indices[n // 2:]] = np.mean(np.sum(-y_train * np.log10(y_preds[sorted_indices[n // 2:]]), axis = 2), axis = 1)\n",
    "\n",
    "            sorted_indices = np.argsort(nets_loss)\n",
    "\n",
    "            mutation_sigma = 0.08 + 0.5 * 1 / math.exp(epoch / ((epochs + 1) / (60 * math.log10(epochs + 1))))\n",
    "\n",
    "            for j in range(number_of_layers_minus_one):\n",
    "                weights[j][sorted_indices[n // 2::2]] = (weights[j][sorted_indices[:n // 2:2]] + weights[j][sorted_indices[1:1 + n // 2:2]]) / 2 + np.random.normal(0, mutation_sigma, (n // 4, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[1 + n // 2::2]] = (weights[j][sorted_indices[:n // 2:2]] + weights[j][sorted_indices[1:1 + n // 2:2]]) / 2 + np.random.normal(0, mutation_sigma, (n // 4, layers[j], layers[j + 1]))\n",
    "\n",
    "            if best_net_index != sorted_indices[0]:\n",
    "                best_net_index = sorted_indices[0]\n",
    "                self.training_loss_history += [nets_loss[best_net_index]]\n",
    "                \n",
    "\n",
    "                self.best_net_weights = []\n",
    "                for j in range(number_of_layers_minus_one):\n",
    "                    self.best_net_weights += [weights[j][best_net_index]]\n",
    "                \n",
    "                if validation_data:\n",
    "                    self.validation_loss_history += [np.mean(np.abs(y_val - self.predict(X_val)))]\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]}\")\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        pass\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - {mutation_sigma}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        else:\n",
    "            activation_function = lambda x: np.maximum(0, x)\n",
    "\n",
    "        #output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        output_activation_function = lambda x: np.exp(x) / np.sum(np.exp(x), axis = 1, keepdims = True)\n",
    "\n",
    "        forward_pass = X.T\n",
    "        for j in range(len(self.best_net_weights) - 1):\n",
    "            forward_pass = activation_function(self.best_net_weights[j].T @ forward_pass)\n",
    "\n",
    "        forward_pass = self.best_net_weights[-1].T @ forward_pass\n",
    "        \n",
    "        return output_activation_function(forward_pass.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 0.9998311820346312 - 1.29197000149995\n",
      "Epoch 7 - loss: 0.9493318441214512 - 1.137112388781039\n",
      "Epoch 8 - loss: 0.945449826545742 - 1.1170352695971997\n",
      "Epoch 10 - loss: 0.9084038714228551 - 1.0782959928363374\n",
      "Epoch 11 - loss: 0.7883575093520281 - 1.0596115544075175\n",
      "Epoch 18 - loss: 0.7825842323168328 - 0.9406368729628037\n",
      "Epoch 21 - loss: 0.7821526861229309 - 0.8954468027447978\n",
      "Epoch 23 - loss: 0.772074035496816 - 0.8670744764788247\n",
      "Epoch 26 - loss: 0.6475596629216347 - 0.826984420356625\n",
      "Epoch 28 - loss: 0.6458369904993683 - 0.8018137178058867\n",
      "Epoch 30 - loss: 0.5164098736104002 - 0.7778198910879496\n",
      "Epoch 39 - loss: 0.5056881722204376 - 0.6829918872536017\n",
      "Epoch 42 - loss: 0.5033540862274465 - 0.6556569221318098\n",
      "Epoch 45 - loss: 0.4994401150710219 - 0.6302147129256686\n",
      "Epoch 49 - loss: 0.497508282466169 - 0.5990101199070317\n",
      "Epoch 50 - loss: 0.47200657881450925 - 0.5916641828003322\n",
      "Epoch 61 - loss: 0.4538528599723891 - 0.5214512626607359\n",
      "Epoch 66 - loss: 0.4489289979809142 - 0.4951460338805037\n",
      "Epoch 68 - loss: 0.4424817259673976 - 0.4854717671017367\n",
      "Epoch 70 - loss: 0.43121880414178854 - 0.4762481203465925\n",
      "Epoch 73 - loss: 0.416562734221192 - 0.46321163791183306\n",
      "Epoch 85 - loss: 0.35067370023627187 - 0.41945689331143376\n",
      "Epoch 89 - loss: 0.3477902485898014 - 0.4074357791815596\n",
      "Epoch 90 - loss: 0.34055668816873674 - 0.4046048005185673\n",
      "Epoch 93 - loss: 0.3382393621474246 - 0.3965042003208088\n",
      "Epoch 94 - loss: 0.33771431316003686 - 0.39392965127748114\n",
      "Epoch 95 - loss: 0.3278251528623542 - 0.3914154547190374\n",
      "Epoch 97 - loss: 0.32216732061269615 - 0.3865624283500497\n",
      "Epoch 106 - loss: 0.3126515267664953 - 0.3673565071492032\n",
      "Epoch 110 - loss: 0.3108052378970108 - 0.3600476375365833\n",
      "Epoch 116 - loss: 0.30302180331350903 - 0.35029943502390726\n",
      "Epoch 122 - loss: 0.29268412948968403 - 0.34183483903164946\n",
      "Epoch 127 - loss: 0.2897307803062248 - 0.3356366382046472\n",
      "Epoch 128 - loss: 0.28647910014209815 - 0.33448171471288113\n",
      "Epoch 131 - loss: 0.2828326230103581 - 0.33117456240647547\n",
      "Epoch 133 - loss: 0.28200911369600745 - 0.32909499964842304\n",
      "Epoch 139 - loss: 0.27806360169637634 - 0.32340696411646064\n",
      "Epoch 140 - loss: 0.2765691423408227 - 0.3225336736950033\n",
      "Epoch 153 - loss: 0.27651324391820964 - 0.3128399464118842\n",
      "Epoch 154 - loss: 0.2756940647279143 - 0.31220747462156984\n",
      "Epoch 155 - loss: 0.2754778098897267 - 0.31158930403648777\n",
      "Epoch 161 - loss: 0.2742205887541615 - 0.3081621698545043\n",
      "Epoch 169 - loss: 0.27393997418701566 - 0.30426049616603335\n",
      "Epoch 172 - loss: 0.27359381289989265 - 0.3029689902850127\n",
      "Epoch 179 - loss: 0.26934251207692317 - 0.300270616277696\n",
      "Epoch 181 - loss: 0.26846320070758967 - 0.2995732598218599\n",
      "Epoch 184 - loss: 0.2659167571585204 - 0.29858290872936644\n",
      "Epoch 189 - loss: 0.2499538953964917 - 0.29706946663180855\n",
      "Epoch 190 - loss: 0.24099708768870176 - 0.29678591577378954\n",
      "Epoch 191 - loss: 0.23486978602152062 - 0.2965083941267387\n",
      "Epoch 192 - loss: 0.20194605652060996 - 0.29623675878016686\n",
      "Epoch 194 - loss: 0.19131537635318124 - 0.2957105922113845\n",
      "Epoch 197 - loss: 0.18847543975024433 - 0.29496210747318363\n",
      "Epoch 200 - loss: 0.17401173132386386 - 0.29425951568766684\n",
      "Epoch 201 - loss: 0.17207413197167845 - 0.29403495960465964\n",
      "Epoch 202 - loss: 0.16184801687644096 - 0.29381503446255075\n",
      "Epoch 206 - loss: 0.15666653632056524 - 0.29297948695008214\n",
      "Epoch 208 - loss: 0.15555552236628709 - 0.29258676607284995\n",
      "Epoch 209 - loss: 0.15229712414264515 - 0.2923963259939727\n",
      "Epoch 212 - loss: 0.14772795153461693 - 0.291847580295844\n",
      "Epoch 214 - loss: 0.14753812381003112 - 0.2914996827085775\n",
      "Epoch 215 - loss: 0.13797837153235215 - 0.29133086086229665\n",
      "Epoch 216 - loss: 0.13451466263162304 - 0.2911653492162162\n",
      "Epoch 219 - loss: 0.13257828163141877 - 0.29068790042915404\n",
      "Epoch 223 - loss: 0.1308840796610969 - 0.290093094013796\n",
      "Epoch 224 - loss: 0.13050046835992576 - 0.28995139127611524\n",
      "Epoch 225 - loss: 0.12972505114922453 - 0.28981235624504265\n",
      "Epoch 226 - loss: 0.12047595942969935 - 0.28967592572639833\n",
      "Epoch 230 - loss: 0.12001330629838734 - 0.28915503660356534\n",
      "Epoch 232 - loss: 0.11850516507798224 - 0.2889086829190174\n",
      "Epoch 234 - loss: 0.1045344992612603 - 0.28867113922963084\n",
      "Epoch 246 - loss: 0.10272873678061116 - 0.2874090987062806\n",
      "Epoch 264 - loss: 0.09590907250588888 - 0.2859256648832146\n",
      "Epoch 278 - loss: 0.09395082415976769 - 0.28501145594689914\n",
      "Epoch 281 - loss: 0.09377847819624584 - 0.2848360123461836\n",
      "Epoch 287 - loss: 0.09375364609697777 - 0.28450310559165704\n",
      "Epoch 292 - loss: 0.09337044416033959 - 0.2842420721586656\n",
      "Epoch 293 - loss: 0.09278818751628502 - 0.2841914889381585\n",
      "Epoch 294 - loss: 0.09255447790591628 - 0.28414141732964526\n",
      "Epoch 296 - loss: 0.09199497746833142 - 0.28404276097926495\n",
      "Epoch 300 - loss: 0.08531517350574454 - 0.2838510756048839\n",
      "Epoch 308 - loss: 0.08481630882329047 - 0.28348787059256264\n",
      "Epoch 318 - loss: 0.08339452254865942 - 0.2830656816219028\n",
      "Epoch 319 - loss: 0.0810852048048722 - 0.2830251317093292\n",
      "Epoch 331 - loss: 0.08058677646736524 - 0.28255827526860133\n",
      "Epoch 332 - loss: 0.07192682825938757 - 0.2825208435402259\n",
      "Epoch 339 - loss: 0.06764876887300363 - 0.2822642316117564\n",
      "Epoch 343 - loss: 0.06319832323522355 - 0.28212146602304466\n",
      "Epoch 349 - loss: 0.05997728125635064 - 0.2819119333557869\n",
      "Epoch 351 - loss: 0.05815085867140035 - 0.2818432090234706\n",
      "Epoch 355 - loss: 0.04533895536049003 - 0.28170728901265973\n",
      "Epoch 382 - loss: 0.03900950705205968 - 0.2808315656656267\n",
      "Epoch 384 - loss: 0.03859017334952672 - 0.2807689371090166\n",
      "Epoch 386 - loss: 0.020166927511282746 - 0.2807065492249322\n",
      "Epoch 401 - loss: 0.01648711707451175 - 0.2802453058327494\n",
      "Epoch 405 - loss: 0.008792068892168471 - 0.28012400518070146\n",
      "Epoch 407 - loss: 0.006887931044233192 - 0.280063583167096\n",
      "Epoch 410 - loss: 0.0066190071186247625 - 0.27997321716988566\n",
      "Epoch 411 - loss: 0.003987557649115078 - 0.27994316330267854\n",
      "Epoch 416 - loss: 0.002420511318634888 - 0.2797933725687788\n",
      "Epoch 422 - loss: 0.0012615694763107566 - 0.2796145897244996\n",
      "Epoch 448 - loss: 0.0011253705857586155 - 0.2788493259705622\n",
      "Epoch 451 - loss: 0.0004226458153368725 - 0.2787618038400437\n",
      "Epoch 464 - loss: 0.000311481344377273 - 0.27838394436508507\n",
      "Epoch 467 - loss: 0.00027668825776446036 - 0.2782970338342181\n",
      "Epoch 479 - loss: 0.00013634889517272604 - 0.2779503055490482\n",
      "Epoch 481 - loss: 8.96956060844415e-05 - 0.27789264663271906\n",
      "Epoch 484 - loss: 7.232293614919244e-05 - 0.2778062212515685\n",
      "Epoch 500 - loss: 5.803475067458429e-05 - 0.27734643530620195\n",
      "Epoch 504 - loss: 5.355812579601346e-05 - 0.27723175966455205\n",
      "Epoch 508 - loss: 5.166264649887683e-05 - 0.27711718113479933\n",
      "Epoch 518 - loss: 4.9151700683340875e-05 - 0.27683112968483437\n",
      "Epoch 525 - loss: 3.45479181014971e-05 - 0.27663120366433047\n",
      "Epoch 531 - loss: 3.366135477605386e-05 - 0.27646002601656705\n",
      "Epoch 533 - loss: 2.470918191263677e-05 - 0.27640300346214064\n",
      "Epoch 538 - loss: 2.2130368478013337e-05 - 0.2762605242418109\n",
      "Epoch 540 - loss: 1.2548599093776857e-05 - 0.2762035626526509\n",
      "Epoch 550 - loss: 6.5856080969289985e-06 - 0.2759190015872939\n",
      "Epoch 553 - loss: 5.494818195344873e-06 - 0.275833710674382\n",
      "Epoch 558 - loss: 3.195475679643043e-06 - 0.27569163545138387\n",
      "Epoch 572 - loss: 1.0275177020412378e-06 - 0.275294311385586\n",
      "Epoch 590 - loss: 1.6498959924541604e-07 - 0.2747844594311778\n",
      "Epoch 616 - loss: 7.471627302954734e-08 - 0.2740498477819908\n",
      "Epoch 631 - loss: 5.1865513711392926e-08 - 0.27362697596512897\n",
      "Epoch 643 - loss: 3.250349047222073e-08 - 0.27328916032977846\n",
      "Epoch 653 - loss: 1.931183942150221e-08 - 0.27300796959774676\n",
      "Epoch 661 - loss: 1.4521874184144837e-08 - 0.272783225959347\n",
      "Epoch 667 - loss: 9.191114789420833e-09 - 0.2726147893437427\n",
      "Epoch 673 - loss: 8.89875557404683e-09 - 0.27244645606801143\n",
      "Epoch 676 - loss: 8.075903839003031e-09 - 0.2723623280647893\n",
      "Epoch 688 - loss: 8.037203932780033e-09 - 0.27202607265366674\n",
      "Epoch 689 - loss: 3.782748077959326e-09 - 0.27199806985416153\n",
      "Epoch 697 - loss: 3.55758778244497e-09 - 0.2717741495261311\n",
      "Epoch 700 - loss: 1.9626541741058253e-09 - 0.2716902261118229\n",
      "Epoch 709 - loss: 1.1834931220773132e-09 - 0.2714386083730097\n",
      "Epoch 716 - loss: 1.0737783053205083e-09 - 0.2712430635226073\n",
      "Epoch 733 - loss: 5.336788578214688e-10 - 0.2707687421141589\n",
      "Epoch 741 - loss: 1.0380240022413125e-10 - 0.2705458123207293\n",
      "Epoch 787 - loss: 4.047676437409286e-11 - 0.2692674315391168\n",
      "Epoch 827 - loss: 3.5970448971078684e-11 - 0.2681605731563417\n",
      "Epoch 851 - loss: 1.3959972604663612e-11 - 0.2674985814426485\n",
      "Epoch 852 - loss: 4.698637364493006e-12 - 0.26747103293010543\n",
      "Epoch 916 - loss: 4.5240634099520215e-12 - 0.2657136469250153\n",
      "Epoch 925 - loss: 4.143057627871045e-12 - 0.2654674154087652\n",
      "Epoch 934 - loss: 1.6150600242967052e-12 - 0.2652214054118307\n",
      "Epoch 954 - loss: 1.0792884504895953e-12 - 0.26467550860993894\n",
      "Epoch 977 - loss: 1.0674842056397354e-12 - 0.2640490755662046\n",
      "Epoch 995 - loss: 4.557280107743786e-13 - 0.2635598276624995\n",
      "Epoch 1000 - loss: 3.4092920918708105e-13 - 0.2634240816831992\n",
      "Epoch 1022 - loss: 8.705685369646029e-14 - 0.262827605052952\n",
      "Epoch 1035 - loss: 4.957081508751834e-14 - 0.2624757579106366\n",
      "Epoch 1052 - loss: 2.7733181276745968e-14 - 0.2620163397328472\n",
      "Epoch 1065 - loss: 2.4436934663798514e-14 - 0.2616655465532729\n",
      "Epoch 1076 - loss: 1.5560738664837213e-15 - 0.26136907753808425\n",
      "Epoch 1099 - loss: 1.034460373211717e-15 - 0.26075024059246543\n",
      "Epoch 1104 - loss: 5.698298665996698e-17 - 0.2606158990599562\n",
      "Epoch 1119 - loss: 2.191653333075653e-17 - 0.26021327725227716\n",
      "Epoch 1140 - loss: 1.7533226664605225e-17 - 0.2596506203656993\n",
      "Epoch 1163 - loss: 1.3149919998453918e-17 - 0.25903573133175706\n",
      "Epoch 1204 - loss: 8.766613332302611e-18 - 0.2579431262038957\n",
      "Epoch 1211 - loss: 8.766613332302611e-18 - 0.25775703115637505\n",
      "Epoch 1212 - loss: 8.766613332302611e-18 - 0.2577304567819946\n",
      "Epoch 1214 - loss: 0.0 - 0.2576773160048822\n",
      "Epoch 1217 - loss: 0.0 - 0.2575976247643491\n",
      "Epoch 1225 - loss: 0.0 - 0.2573852316330819\n",
      "Epoch 1226 - loss: 0.0 - 0.2573586944367965\n",
      "Epoch 1234 - loss: 0.0 - 0.25714649237336085\n",
      "Epoch 1235 - loss: 0.0 - 0.25711997904980854\n",
      "Epoch 1236 - loss: 0.0 - 0.25709346837745617\n",
      "Epoch 1237 - loss: 0.0 - 0.2570669603560385\n",
      "Epoch 1238 - loss: 0.0 - 0.2570404549852905\n",
      "Epoch 1239 - loss: 0.0 - 0.25701395226494705\n",
      "Epoch 1240 - loss: 0.0 - 0.2569874521947433\n",
      "Epoch 1241 - loss: 0.0 - 0.25696095477441405\n",
      "Epoch 1242 - loss: 0.0 - 0.2569344600036946\n",
      "Epoch 1243 - loss: 0.0 - 0.2569079678823196\n",
      "Epoch 1244 - loss: 0.0 - 0.2568814784100245\n",
      "Epoch 1245 - loss: 0.0 - 0.2568549915865441\n",
      "Epoch 1247 - loss: 0.0 - 0.2568020258849687\n",
      "Epoch 1248 - loss: 0.0 - 0.25677554700634375\n",
      "Epoch 1249 - loss: 0.0 - 0.2567490707754745\n",
      "Epoch 1250 - loss: 0.0 - 0.2567225971920959\n",
      "Epoch 1251 - loss: 0.0 - 0.2566961262559433\n",
      "Epoch 1252 - loss: 0.0 - 0.2566696579667521\n",
      "Epoch 1253 - loss: 0.0 - 0.2566431923242575\n",
      "Epoch 1254 - loss: 0.0 - 0.2566167293281949\n",
      "Epoch 1255 - loss: 0.0 - 0.2565902689782996\n",
      "Epoch 1256 - loss: 0.0 - 0.25656381127430705\n",
      "Epoch 1257 - loss: 0.0 - 0.25653735621595264\n",
      "Epoch 1258 - loss: 0.0 - 0.2565109038029719\n",
      "Epoch 1259 - loss: 0.0 - 0.25648445403510023\n",
      "Epoch 1261 - loss: 0.0 - 0.2564315624336261\n",
      "Epoch 1262 - loss: 0.0 - 0.2564051205994947\n",
      "Epoch 1263 - loss: 0.0 - 0.2563786814094148\n",
      "Epoch 1264 - loss: 0.0 - 0.25635224486312147\n",
      "Epoch 1265 - loss: 0.0 - 0.2563258109603508\n",
      "Epoch 1266 - loss: 0.0 - 0.25629937970083827\n",
      "Epoch 1267 - loss: 0.0 - 0.25627295108431947\n",
      "Epoch 1268 - loss: 0.0 - 0.25624652511053025\n",
      "Epoch 1269 - loss: 0.0 - 0.2562201017792064\n",
      "Epoch 1270 - loss: 0.0 - 0.25619368109008356\n",
      "Epoch 1271 - loss: 0.0 - 0.2561672630428975\n",
      "Epoch 1272 - loss: 0.0 - 0.2561408476373842\n",
      "Epoch 1273 - loss: 0.0 - 0.25611443487327934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 6.12531 s\n",
      "File: /var/folders/xz/f2gwbn5n3vs4pz044n49z3cw0000gn/T/ipykernel_82011/3315824649.py\n",
      "Function: fit at line 25\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    25                                               def fit(self, X_train, y_train, epochs = 100, validation_data = False, verbose = 0):\n",
      "    26                                           \n",
      "    27         1          1.0      1.0      0.0          n = self.n\n",
      "    28         1          1.0      1.0      0.0          ndiv4 = n // 4\n",
      "    29                                           \n",
      "    30         1          1.0      1.0      0.0          if self.random_state != None:\n",
      "    31         1          9.0      9.0      0.0              np.random.seed(self.random_state)\n",
      "    32                                           \n",
      "    33         1       1852.0   1852.0      0.0          X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
      "    34         1          9.0      9.0      0.0          y_train = y_train.astype(\"int8\")\n",
      "    35                                           \n",
      "    36         1          2.0      2.0      0.0          if len(y_train.shape) == 1:\n",
      "    37                                                       self.multiclass = False\n",
      "    38         1          2.0      2.0      0.0          elif len(y_train.shape) == 2 and y_train.shape[1] == 1:\n",
      "    39                                                       self.multiclass = False\n",
      "    40                                                       y_train = y_train.ravel()\n",
      "    41                                                   else:\n",
      "    42         1          2.0      2.0      0.0              self.multiclass = True\n",
      "    43                                                       \n",
      "    44                                           \n",
      "    45         1          1.0      1.0      0.0          if validation_data:\n",
      "    46                                                       X_val, y_val = validation_data\n",
      "    47                                           \n",
      "    48         1          1.0      1.0      0.0          if self.activation == \"sigmoid\":\n",
      "    49                                                       activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
      "    50         1          1.0      1.0      0.0          elif self.activation == \"leaky_relu\":\n",
      "    51                                                       activation_function = lambda x: np.maximum(0.1 * x, x)\n",
      "    52                                                   else:\n",
      "    53         1          1.0      1.0      0.0              activation_function = lambda x: np.maximum(0, x)\n",
      "    54                                           \n",
      "    55         1          1.0      1.0      0.0          if self.multiclass == True:\n",
      "    56         1          1.0      1.0      0.0              output_activation_function = lambda x: np.exp(x) / np.sum(np.exp(x), axis = 2, keepdims = True)\n",
      "    57                                                       \n",
      "    58         1          1.0      1.0      0.0              def loss_function(y_train, y_preds, sorted_indices):\n",
      "    59                                                           return np.mean(np.sum(-y_train * np.log10(y_preds[sorted_indices[ndiv4:]]), axis = 2), axis = 1)\n",
      "    60                                           \n",
      "    61                                                   elif self.multiclass == False:\n",
      "    62                                                       output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
      "    63                                           \n",
      "    64                                                       def loss_function(y_train, y_preds, sorted_indices):\n",
      "    65                                                           return np.mean(np.abs(y_preds[sorted_indices[ndiv4:]] - y_train), axis = 1)\n",
      "    66                                           \n",
      "    67         1          1.0      1.0      0.0          lr_target = self.lr_target\n",
      "    68         1          1.0      1.0      0.0          lr_initial_decay = self.lr_initial_decay\n",
      "    69         1          1.0      1.0      0.0          lr_final_decay = self.lr_final_decay\n",
      "    70                                           \n",
      "    71         1          1.0      1.0      0.0          layers = [X_train.shape[1]]\n",
      "    72                                           \n",
      "    73         1          1.0      1.0      0.0          if self.hidden_layers:\n",
      "    74         1          1.0      1.0      0.0              layers = [X_train.shape[1]] + self.hidden_layers\n",
      "    75                                           \n",
      "    76         1          1.0      1.0      0.0          if self.multiclass == True:\n",
      "    77         1          1.0      1.0      0.0              layers = layers + [y_train.shape[1]]\n",
      "    78                                                   elif self.multiclass == False:\n",
      "    79                                                       layers = layers + [1]\n",
      "    80                                           \n",
      "    81         1          1.0      1.0      0.0          number_of_layers_minus_one = len(layers) - 1\n",
      "    82                                                   \n",
      "    83         1          1.0      1.0      0.0          if self.multiclass == True:\n",
      "    84         1          6.0      6.0      0.0              y_preds = np.zeros((n, y_train.shape[0], y_train.shape[1]))\n",
      "    85                                                   elif self.multiclass == False:\n",
      "    86                                                       y_preds = np.zeros((n, y_train.shape[0]))\n",
      "    87                                           \n",
      "    88         1          3.0      3.0      0.0          nets_loss = np.zeros(n)\n",
      "    89         1          6.0      6.0      0.0          sorted_indices = np.arange(-(ndiv4), n, 1)\n",
      "    90                                           \n",
      "    91         1          1.0      1.0      0.0          best_net_index = -1\n",
      "    92                                           \n",
      "    93         1          1.0      1.0      0.0          weights = []\n",
      "    94                                           \n",
      "    95         3          5.0      1.7      0.0          for i in range(number_of_layers_minus_one):\n",
      "    96         2        455.0    227.5      0.0              weights += [np.random.normal(0, 1, (n, layers[i], layers[i + 1]))]\n",
      "    97                                           \n",
      "    98     10001       8714.0      0.9      0.1          for epoch in range(epochs):\n",
      "    99     10000      10805.0      1.1      0.2              forward_pass = X_train.T\n",
      "   100                                                       \n",
      "   101     20000      19198.0      1.0      0.3              for j in range(number_of_layers_minus_one - 1):\n",
      "   102     10000     337837.0     33.8      5.5                  forward_pass = activation_function(weights[j][sorted_indices[ndiv4:]].transpose(0, 2, 1) @ forward_pass)\n",
      "   103                                                       \n",
      "   104     10000     208692.0     20.9      3.4              forward_pass = weights[-1][sorted_indices[ndiv4:]].transpose(0, 2, 1) @ forward_pass\n",
      "   105                                                       \n",
      "   106     10000     827210.0     82.7     13.5              y_preds[sorted_indices[ndiv4:]] = output_activation_function(forward_pass.transpose(0, 2, 1))\n",
      "   107                                           \n",
      "   108     10000     634406.0     63.4     10.4              nets_loss[sorted_indices[ndiv4:]] = loss_function(y_train, y_preds, sorted_indices)\n",
      "   109                                           \n",
      "   110     10000      50410.0      5.0      0.8              sorted_indices = np.argsort(nets_loss)\n",
      "   111     10000      15124.0      1.5      0.2              mutation_sigma = math.exp(-epoch / (epochs / (lr_initial_decay * math.log10(epochs + 1)))) + lr_final_decay * math.exp(-(epoch + 1) * (1 / (epochs))) + lr_target + (-0.036 * 10 * lr_final_decay)\n",
      "   112                                                       #mutation_sigma = math.exp(-epoch / (epochs / (lr_initial_decay * math.log10(epochs + 1)))) + 0.08 * math.exp(-(epoch + 1) * (1 / (epochs + 1))) + 0.02\n",
      "   113                                                       #mutation_sigma = math.exp(-epoch / (epochs / (lr_decay * math.log10(epochs + 1)))) + 0.02 * math.exp(-(epoch + 1) * (1 / (epochs + 1))) - 0.005\n",
      "   114                                                       #mutation_sigma = 0.08 + 0.5 * 1 / math.exp(epoch / ((epochs + 1) / (60 * math.log10(epochs + 1))))\n",
      "   115                                           \n",
      "   116     30000      27666.0      0.9      0.5              for j in range(number_of_layers_minus_one):\n",
      "   117     20000     676662.0     33.8     11.0                  weights[j][sorted_indices[0 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
      "   118     20000     661742.0     33.1     10.8                  weights[j][sorted_indices[1 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
      "   119     20000     660313.0     33.0     10.8                  weights[j][sorted_indices[2 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
      "   120     20000     657506.0     32.9     10.7                  weights[j][sorted_indices[3 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
      "   121     20000     656047.0     32.8     10.7                  weights[j][sorted_indices[4 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
      "   122     20000     655891.0     32.8     10.7                  weights[j][sorted_indices[5 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
      "   123                                           \n",
      "   124     10000       9549.0      1.0      0.2              if best_net_index != sorted_indices[0]:\n",
      "   125       204        185.0      0.9      0.0                  best_net_index = sorted_indices[0]\n",
      "   126       204        399.0      2.0      0.0                  self.training_loss_history += [nets_loss[best_net_index]]\n",
      "   127                                                           \n",
      "   128                                           \n",
      "   129       204        223.0      1.1      0.0                  self.best_net_weights = []\n",
      "   130       612        562.0      0.9      0.0                  for j in range(number_of_layers_minus_one):\n",
      "   131       408        473.0      1.2      0.0                      self.best_net_weights += [weights[j][best_net_index]]\n",
      "   132                                                           \n",
      "   133       204        179.0      0.9      0.0                  if validation_data:\n",
      "   134                                                               self.validation_loss_history += [np.mean(np.abs(y_val - self.predict(X_val)))]\n",
      "   135                                                               if verbose == 1:\n",
      "   136                                                                   print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]}\")\n",
      "   137                                                           else:\n",
      "   138       204        193.0      0.9      0.0                      if verbose == 1:\n",
      "   139                                                                   pass\n",
      "   140       204       2956.0     14.5      0.0                          print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - {mutation_sigma}\")"
     ]
    }
   ],
   "source": [
    "%reload_ext line_profiler\n",
    "\n",
    "import math\n",
    "\n",
    "class VectorizedEvoClassifierM:\n",
    "    def __init__(self, n = 24, hidden_layers = False, activation = \"relu\", lr_target = 0.04, lr_initial_decay = 60, lr_final_decay = 0.03, random_state = None):\n",
    "\n",
    "        self.n = int(round(n / 8) * 8)\n",
    "        self.validation_loss_history = []\n",
    "        self.training_loss_history = []\n",
    "        self.random_state = random_state\n",
    "        self.activation = activation\n",
    "        self.lr_target = lr_target\n",
    "        self.lr_initial_decay = lr_initial_decay\n",
    "        self.lr_final_decay = lr_final_decay\n",
    "\n",
    "        \n",
    "        if hidden_layers:\n",
    "            self.hidden_layers = hidden_layers\n",
    "        else:\n",
    "            self.hidden_layers = False\n",
    "\n",
    "        \n",
    "\n",
    "    def fit(self, X_train, y_train, epochs = 100, validation_data = False, verbose = 0):\n",
    "\n",
    "        n = self.n\n",
    "        ndiv4 = n // 4\n",
    "\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "        y_train = y_train.astype(\"int8\")\n",
    "\n",
    "        if len(y_train.shape) == 1:\n",
    "            self.multiclass = False\n",
    "        elif len(y_train.shape) == 2 and y_train.shape[1] == 1:\n",
    "            self.multiclass = False\n",
    "            y_train = y_train.ravel()\n",
    "        else:\n",
    "            self.multiclass = True\n",
    "            \n",
    "\n",
    "        if validation_data:\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        else:\n",
    "            activation_function = lambda x: np.maximum(0, x)\n",
    "\n",
    "        if self.multiclass == True:\n",
    "            output_activation_function = lambda x: np.exp(x) / np.sum(np.exp(x), axis = 2, keepdims = True)\n",
    "            \n",
    "            def loss_function(y_train, y_preds, sorted_indices):\n",
    "                return np.mean(np.sum(-y_train * np.log10(y_preds[sorted_indices[ndiv4:]]), axis = 2), axis = 1)\n",
    "\n",
    "        elif self.multiclass == False:\n",
    "            output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "            def loss_function(y_train, y_preds, sorted_indices):\n",
    "                return np.mean(np.abs(y_preds[sorted_indices[ndiv4:]] - y_train), axis = 1)\n",
    "\n",
    "        lr_target = self.lr_target\n",
    "        lr_initial_decay = self.lr_initial_decay\n",
    "        lr_final_decay = self.lr_final_decay\n",
    "\n",
    "        layers = [X_train.shape[1]]\n",
    "\n",
    "        if self.hidden_layers:\n",
    "            layers = [X_train.shape[1]] + self.hidden_layers\n",
    "\n",
    "        if self.multiclass == True:\n",
    "            layers = layers + [y_train.shape[1]]\n",
    "        elif self.multiclass == False:\n",
    "            layers = layers + [1]\n",
    "\n",
    "        number_of_layers_minus_one = len(layers) - 1\n",
    "        \n",
    "        if self.multiclass == True:\n",
    "            y_preds = np.zeros((n, y_train.shape[0], y_train.shape[1]))\n",
    "        elif self.multiclass == False:\n",
    "            y_preds = np.zeros((n, y_train.shape[0]))\n",
    "\n",
    "        nets_loss = np.zeros(n)\n",
    "        sorted_indices = np.arange(-(ndiv4), n, 1)\n",
    "\n",
    "        best_net_index = -1\n",
    "\n",
    "        weights = []\n",
    "\n",
    "        for i in range(number_of_layers_minus_one):\n",
    "            weights += [np.random.normal(0, 1, (n, layers[i], layers[i + 1]))]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            forward_pass = X_train.T\n",
    "            \n",
    "            for j in range(number_of_layers_minus_one - 1):\n",
    "                forward_pass = activation_function(weights[j][sorted_indices[ndiv4:]].transpose(0, 2, 1) @ forward_pass)\n",
    "            \n",
    "            forward_pass = weights[-1][sorted_indices[ndiv4:]].transpose(0, 2, 1) @ forward_pass\n",
    "            \n",
    "            y_preds[sorted_indices[ndiv4:]] = output_activation_function(forward_pass.transpose(0, 2, 1))\n",
    "\n",
    "            nets_loss[sorted_indices[ndiv4:]] = loss_function(y_train, y_preds, sorted_indices)\n",
    "\n",
    "            sorted_indices = np.argsort(nets_loss)\n",
    "            mutation_sigma = math.exp(-epoch / (epochs / (lr_initial_decay * math.log10(epochs + 1)))) + lr_final_decay * math.exp(-(epoch + 1) * (1 / (epochs))) + lr_target + (-0.036 * 10 * lr_final_decay)\n",
    "            #mutation_sigma = math.exp(-epoch / (epochs / (lr_initial_decay * math.log10(epochs + 1)))) + 0.08 * math.exp(-(epoch + 1) * (1 / (epochs + 1))) + 0.02\n",
    "            #mutation_sigma = math.exp(-epoch / (epochs / (lr_decay * math.log10(epochs + 1)))) + 0.02 * math.exp(-(epoch + 1) * (1 / (epochs + 1))) - 0.005\n",
    "            #mutation_sigma = 0.08 + 0.5 * 1 / math.exp(epoch / ((epochs + 1) / (60 * math.log10(epochs + 1))))\n",
    "\n",
    "            for j in range(number_of_layers_minus_one):\n",
    "                weights[j][sorted_indices[0 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[1 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[2 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[3 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[4 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[5 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "\n",
    "            if best_net_index != sorted_indices[0]:\n",
    "                best_net_index = sorted_indices[0]\n",
    "                self.training_loss_history += [nets_loss[best_net_index]]\n",
    "                \n",
    "\n",
    "                self.best_net_weights = []\n",
    "                for j in range(number_of_layers_minus_one):\n",
    "                    self.best_net_weights += [weights[j][best_net_index]]\n",
    "                \n",
    "                if validation_data:\n",
    "                    self.validation_loss_history += [np.mean(np.abs(y_val - self.predict(X_val)))]\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]}\")\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        pass\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - {mutation_sigma}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        else:\n",
    "            activation_function = lambda x: np.maximum(0, x)\n",
    "\n",
    "        if self.multiclass == True:\n",
    "            output_activation_function = lambda x: np.exp(x) / np.sum(np.exp(x), axis = 1, keepdims = True)\n",
    "        elif self.multiclass == False:\n",
    "            output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "        forward_pass = X.T\n",
    "        for j in range(len(self.best_net_weights) - 1):\n",
    "            forward_pass = activation_function(self.best_net_weights[j].T @ forward_pass)\n",
    "\n",
    "        forward_pass = self.best_net_weights[-1].T @ forward_pass\n",
    "        \n",
    "        return output_activation_function(forward_pass.T)\n",
    "\n",
    "evomodel = VectorizedEvoClassifierM(n = 24 * 4, hidden_layers = [10], lr_target = 0.1, lr_initial_decay = 60, lr_final_decay = 0.3, random_state = 42)\n",
    "%lprun -f evomodel.fit evomodel.fit(X, y, epochs = 10000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 0.9998311820346312 - 1.29197000149995\n",
      "Epoch 7 - loss: 0.9493318441214512 - 1.137112388781039\n",
      "Epoch 8 - loss: 0.945449826545742 - 1.1170352695971997\n",
      "Epoch 10 - loss: 0.9084038714228551 - 1.0782959928363374\n",
      "Epoch 11 - loss: 0.7883575093520281 - 1.0596115544075175\n",
      "Epoch 18 - loss: 0.7825842323168328 - 0.9406368729628037\n",
      "Epoch 21 - loss: 0.7821526861229309 - 0.8954468027447978\n",
      "Epoch 23 - loss: 0.772074035496816 - 0.8670744764788247\n",
      "Epoch 26 - loss: 0.6475596629216347 - 0.826984420356625\n",
      "Epoch 28 - loss: 0.6458369904993683 - 0.8018137178058867\n",
      "Epoch 30 - loss: 0.5164098736104002 - 0.7778198910879496\n",
      "Epoch 39 - loss: 0.5056881722204376 - 0.6829918872536017\n",
      "Epoch 42 - loss: 0.5033540862274465 - 0.6556569221318098\n",
      "Epoch 45 - loss: 0.4994401150710219 - 0.6302147129256686\n",
      "Epoch 49 - loss: 0.497508282466169 - 0.5990101199070317\n",
      "Epoch 50 - loss: 0.47200657881450925 - 0.5916641828003322\n",
      "Epoch 61 - loss: 0.4538528599723891 - 0.5214512626607359\n",
      "Epoch 66 - loss: 0.4489289979809142 - 0.4951460338805037\n",
      "Epoch 68 - loss: 0.4424817259673976 - 0.4854717671017367\n",
      "Epoch 70 - loss: 0.43121880414178854 - 0.4762481203465925\n",
      "Epoch 73 - loss: 0.416562734221192 - 0.46321163791183306\n",
      "Epoch 85 - loss: 0.35067370023627187 - 0.41945689331143376\n",
      "Epoch 89 - loss: 0.3477902485898014 - 0.4074357791815596\n",
      "Epoch 90 - loss: 0.34055668816873674 - 0.4046048005185673\n",
      "Epoch 93 - loss: 0.3382393621474246 - 0.3965042003208088\n",
      "Epoch 94 - loss: 0.33771431316003686 - 0.39392965127748114\n",
      "Epoch 95 - loss: 0.3278251528623542 - 0.3914154547190374\n",
      "Epoch 97 - loss: 0.32216732061269615 - 0.3865624283500497\n",
      "Epoch 106 - loss: 0.3126515267664953 - 0.3673565071492032\n",
      "Epoch 110 - loss: 0.3108052378970108 - 0.3600476375365833\n",
      "Epoch 116 - loss: 0.30302180331350903 - 0.35029943502390726\n",
      "Epoch 122 - loss: 0.29268412948968403 - 0.34183483903164946\n",
      "Epoch 127 - loss: 0.2897307803062248 - 0.3356366382046472\n",
      "Epoch 128 - loss: 0.28647910014209815 - 0.33448171471288113\n",
      "Epoch 131 - loss: 0.2828326230103581 - 0.33117456240647547\n",
      "Epoch 133 - loss: 0.28200911369600745 - 0.32909499964842304\n",
      "Epoch 139 - loss: 0.27806360169637634 - 0.32340696411646064\n",
      "Epoch 140 - loss: 0.2765691423408227 - 0.3225336736950033\n",
      "Epoch 153 - loss: 0.27651324391820964 - 0.3128399464118842\n",
      "Epoch 154 - loss: 0.2756940647279143 - 0.31220747462156984\n",
      "Epoch 155 - loss: 0.2754778098897267 - 0.31158930403648777\n",
      "Epoch 161 - loss: 0.2742205887541615 - 0.3081621698545043\n",
      "Epoch 169 - loss: 0.27393997418701566 - 0.30426049616603335\n",
      "Epoch 172 - loss: 0.27359381289989265 - 0.3029689902850127\n",
      "Epoch 179 - loss: 0.26934251207692317 - 0.300270616277696\n",
      "Epoch 181 - loss: 0.26846320070758967 - 0.2995732598218599\n",
      "Epoch 184 - loss: 0.2659167571585204 - 0.29858290872936644\n",
      "Epoch 189 - loss: 0.2499538953964917 - 0.29706946663180855\n",
      "Epoch 190 - loss: 0.24099708768870176 - 0.29678591577378954\n",
      "Epoch 191 - loss: 0.23486978602152062 - 0.2965083941267387\n",
      "Epoch 192 - loss: 0.20194605652060996 - 0.29623675878016686\n",
      "Epoch 194 - loss: 0.19131537635318124 - 0.2957105922113845\n",
      "Epoch 197 - loss: 0.18847543975024433 - 0.29496210747318363\n",
      "Epoch 200 - loss: 0.17401173132386386 - 0.29425951568766684\n",
      "Epoch 201 - loss: 0.17207413197167845 - 0.29403495960465964\n",
      "Epoch 202 - loss: 0.16184801687644096 - 0.29381503446255075\n",
      "Epoch 206 - loss: 0.15666653632056524 - 0.29297948695008214\n",
      "Epoch 208 - loss: 0.15555552236628709 - 0.29258676607284995\n",
      "Epoch 209 - loss: 0.15229712414264515 - 0.2923963259939727\n",
      "Epoch 212 - loss: 0.14772795153461693 - 0.291847580295844\n",
      "Epoch 214 - loss: 0.14753812381003112 - 0.2914996827085775\n",
      "Epoch 215 - loss: 0.13797837153235215 - 0.29133086086229665\n",
      "Epoch 216 - loss: 0.13451466263162304 - 0.2911653492162162\n",
      "Epoch 219 - loss: 0.13257828163141877 - 0.29068790042915404\n",
      "Epoch 223 - loss: 0.1308840796610969 - 0.290093094013796\n",
      "Epoch 224 - loss: 0.13050046835992576 - 0.28995139127611524\n",
      "Epoch 225 - loss: 0.12972505114922453 - 0.28981235624504265\n",
      "Epoch 226 - loss: 0.12047595942969935 - 0.28967592572639833\n",
      "Epoch 230 - loss: 0.12001330629838734 - 0.28915503660356534\n",
      "Epoch 232 - loss: 0.11850516507798224 - 0.2889086829190174\n",
      "Epoch 234 - loss: 0.1045344992612603 - 0.28867113922963084\n",
      "Epoch 246 - loss: 0.10272873678061116 - 0.2874090987062806\n",
      "Epoch 264 - loss: 0.09590907250588888 - 0.2859256648832146\n",
      "Epoch 278 - loss: 0.09395082415976769 - 0.28501145594689914\n",
      "Epoch 281 - loss: 0.09377847819624584 - 0.2848360123461836\n",
      "Epoch 287 - loss: 0.09375364609697777 - 0.28450310559165704\n",
      "Epoch 292 - loss: 0.09337044416033959 - 0.2842420721586656\n",
      "Epoch 293 - loss: 0.09278818751628502 - 0.2841914889381585\n",
      "Epoch 294 - loss: 0.09255447790591628 - 0.28414141732964526\n",
      "Epoch 296 - loss: 0.09199497746833142 - 0.28404276097926495\n",
      "Epoch 300 - loss: 0.08531517350574454 - 0.2838510756048839\n",
      "Epoch 308 - loss: 0.08481630882329047 - 0.28348787059256264\n",
      "Epoch 318 - loss: 0.08339452254865942 - 0.2830656816219028\n",
      "Epoch 319 - loss: 0.0810852048048722 - 0.2830251317093292\n",
      "Epoch 331 - loss: 0.08058677646736524 - 0.28255827526860133\n",
      "Epoch 332 - loss: 0.07192682825938757 - 0.2825208435402259\n",
      "Epoch 339 - loss: 0.06764876887300363 - 0.2822642316117564\n",
      "Epoch 343 - loss: 0.06319832323522355 - 0.28212146602304466\n",
      "Epoch 349 - loss: 0.05997728125635064 - 0.2819119333557869\n",
      "Epoch 351 - loss: 0.05815085867140035 - 0.2818432090234706\n",
      "Epoch 355 - loss: 0.04533895536049003 - 0.28170728901265973\n",
      "Epoch 382 - loss: 0.03900950705205968 - 0.2808315656656267\n",
      "Epoch 384 - loss: 0.03859017334952672 - 0.2807689371090166\n",
      "Epoch 386 - loss: 0.020166927511282746 - 0.2807065492249322\n",
      "Epoch 401 - loss: 0.01648711707451175 - 0.2802453058327494\n",
      "Epoch 405 - loss: 0.008792068892168471 - 0.28012400518070146\n",
      "Epoch 407 - loss: 0.006887931044233192 - 0.280063583167096\n",
      "Epoch 410 - loss: 0.0066190071186247625 - 0.27997321716988566\n",
      "Epoch 411 - loss: 0.003987557649115078 - 0.27994316330267854\n",
      "Epoch 416 - loss: 0.002420511318634888 - 0.2797933725687788\n",
      "Epoch 422 - loss: 0.0012615694763107566 - 0.2796145897244996\n",
      "Epoch 448 - loss: 0.0011253705857586155 - 0.2788493259705622\n",
      "Epoch 451 - loss: 0.0004226458153368725 - 0.2787618038400437\n",
      "Epoch 464 - loss: 0.000311481344377273 - 0.27838394436508507\n",
      "Epoch 467 - loss: 0.00027668825776446036 - 0.2782970338342181\n",
      "Epoch 479 - loss: 0.00013634889517272604 - 0.2779503055490482\n",
      "Epoch 481 - loss: 8.96956060844415e-05 - 0.27789264663271906\n",
      "Epoch 484 - loss: 7.232293614919244e-05 - 0.2778062212515685\n",
      "Epoch 500 - loss: 5.803475067458429e-05 - 0.27734643530620195\n",
      "Epoch 504 - loss: 5.355812579601346e-05 - 0.27723175966455205\n",
      "Epoch 508 - loss: 5.166264649887683e-05 - 0.27711718113479933\n",
      "Epoch 518 - loss: 4.9151700683340875e-05 - 0.27683112968483437\n",
      "Epoch 525 - loss: 3.45479181014971e-05 - 0.27663120366433047\n",
      "Epoch 531 - loss: 3.366135477605386e-05 - 0.27646002601656705\n",
      "Epoch 533 - loss: 2.470918191263677e-05 - 0.27640300346214064\n",
      "Epoch 538 - loss: 2.2130368478013337e-05 - 0.2762605242418109\n",
      "Epoch 540 - loss: 1.2548599093776857e-05 - 0.2762035626526509\n",
      "Epoch 550 - loss: 6.5856080969289985e-06 - 0.2759190015872939\n",
      "Epoch 553 - loss: 5.494818195344873e-06 - 0.275833710674382\n",
      "Epoch 558 - loss: 3.195475679643043e-06 - 0.27569163545138387\n",
      "Epoch 572 - loss: 1.0275177020412378e-06 - 0.275294311385586\n",
      "Epoch 590 - loss: 1.6498959924541604e-07 - 0.2747844594311778\n",
      "Epoch 616 - loss: 7.471627302954734e-08 - 0.2740498477819908\n",
      "Epoch 631 - loss: 5.1865513711392926e-08 - 0.27362697596512897\n",
      "Epoch 643 - loss: 3.250349047222073e-08 - 0.27328916032977846\n",
      "Epoch 653 - loss: 1.931183942150221e-08 - 0.27300796959774676\n",
      "Epoch 661 - loss: 1.4521874184144837e-08 - 0.272783225959347\n",
      "Epoch 667 - loss: 9.191114789420833e-09 - 0.2726147893437427\n",
      "Epoch 673 - loss: 8.89875557404683e-09 - 0.27244645606801143\n",
      "Epoch 676 - loss: 8.075903839003031e-09 - 0.2723623280647893\n",
      "Epoch 688 - loss: 8.037203932780033e-09 - 0.27202607265366674\n",
      "Epoch 689 - loss: 3.782748077959326e-09 - 0.27199806985416153\n",
      "Epoch 697 - loss: 3.55758778244497e-09 - 0.2717741495261311\n",
      "Epoch 700 - loss: 1.9626541741058253e-09 - 0.2716902261118229\n",
      "Epoch 709 - loss: 1.1834931220773132e-09 - 0.2714386083730097\n",
      "Epoch 716 - loss: 1.0737783053205083e-09 - 0.2712430635226073\n",
      "Epoch 733 - loss: 5.336788578214688e-10 - 0.2707687421141589\n",
      "Epoch 741 - loss: 1.0380240022413125e-10 - 0.2705458123207293\n",
      "Epoch 787 - loss: 4.047676437409286e-11 - 0.2692674315391168\n",
      "Epoch 827 - loss: 3.5970448971078684e-11 - 0.2681605731563417\n",
      "Epoch 851 - loss: 1.3959972604663612e-11 - 0.2674985814426485\n",
      "Epoch 852 - loss: 4.698637364493006e-12 - 0.26747103293010543\n",
      "Epoch 916 - loss: 4.5240634099520215e-12 - 0.2657136469250153\n",
      "Epoch 925 - loss: 4.143057627871045e-12 - 0.2654674154087652\n",
      "Epoch 934 - loss: 1.6150600242967052e-12 - 0.2652214054118307\n",
      "Epoch 954 - loss: 1.0792884504895953e-12 - 0.26467550860993894\n",
      "Epoch 977 - loss: 1.0674842056397354e-12 - 0.2640490755662046\n",
      "Epoch 995 - loss: 4.557280107743786e-13 - 0.2635598276624995\n",
      "Epoch 1000 - loss: 3.4092920918708105e-13 - 0.2634240816831992\n",
      "Epoch 1022 - loss: 8.705685369646029e-14 - 0.262827605052952\n",
      "Epoch 1035 - loss: 4.957081508751834e-14 - 0.2624757579106366\n",
      "Epoch 1052 - loss: 2.7733181276745968e-14 - 0.2620163397328472\n",
      "Epoch 1065 - loss: 2.4436934663798514e-14 - 0.2616655465532729\n",
      "Epoch 1076 - loss: 1.5560738664837213e-15 - 0.26136907753808425\n",
      "Epoch 1099 - loss: 1.034460373211717e-15 - 0.26075024059246543\n",
      "Epoch 1104 - loss: 5.698298665996698e-17 - 0.2606158990599562\n",
      "Epoch 1119 - loss: 2.191653333075653e-17 - 0.26021327725227716\n",
      "Epoch 1140 - loss: 1.7533226664605225e-17 - 0.2596506203656993\n",
      "Epoch 1163 - loss: 1.3149919998453918e-17 - 0.25903573133175706\n",
      "Epoch 1204 - loss: 8.766613332302611e-18 - 0.2579431262038957\n",
      "Epoch 1211 - loss: 8.766613332302611e-18 - 0.25775703115637505\n",
      "Epoch 1212 - loss: 8.766613332302611e-18 - 0.2577304567819946\n",
      "Epoch 1214 - loss: 0.0 - 0.2576773160048822\n",
      "Epoch 1217 - loss: 0.0 - 0.2575976247643491\n",
      "Epoch 1225 - loss: 0.0 - 0.2573852316330819\n",
      "Epoch 1226 - loss: 0.0 - 0.2573586944367965\n",
      "Epoch 1234 - loss: 0.0 - 0.25714649237336085\n",
      "Epoch 1235 - loss: 0.0 - 0.25711997904980854\n",
      "Epoch 1236 - loss: 0.0 - 0.25709346837745617\n",
      "Epoch 1237 - loss: 0.0 - 0.2570669603560385\n",
      "Epoch 1238 - loss: 0.0 - 0.2570404549852905\n",
      "Epoch 1239 - loss: 0.0 - 0.25701395226494705\n",
      "Epoch 1240 - loss: 0.0 - 0.2569874521947433\n",
      "Epoch 1241 - loss: 0.0 - 0.25696095477441405\n",
      "Epoch 1242 - loss: 0.0 - 0.2569344600036946\n",
      "Epoch 1243 - loss: 0.0 - 0.2569079678823196\n",
      "Epoch 1244 - loss: 0.0 - 0.2568814784100245\n",
      "Epoch 1245 - loss: 0.0 - 0.2568549915865441\n",
      "Epoch 1247 - loss: 0.0 - 0.2568020258849687\n",
      "Epoch 1248 - loss: 0.0 - 0.25677554700634375\n",
      "Epoch 1249 - loss: 0.0 - 0.2567490707754745\n",
      "Epoch 1250 - loss: 0.0 - 0.2567225971920959\n",
      "Epoch 1251 - loss: 0.0 - 0.2566961262559433\n",
      "Epoch 1252 - loss: 0.0 - 0.2566696579667521\n",
      "Epoch 1253 - loss: 0.0 - 0.2566431923242575\n",
      "Epoch 1254 - loss: 0.0 - 0.2566167293281949\n",
      "Epoch 1255 - loss: 0.0 - 0.2565902689782996\n",
      "Epoch 1256 - loss: 0.0 - 0.25656381127430705\n",
      "Epoch 1257 - loss: 0.0 - 0.25653735621595264\n",
      "Epoch 1258 - loss: 0.0 - 0.2565109038029719\n",
      "Epoch 1259 - loss: 0.0 - 0.25648445403510023\n",
      "Epoch 1261 - loss: 0.0 - 0.2564315624336261\n",
      "Epoch 1262 - loss: 0.0 - 0.2564051205994947\n",
      "Epoch 1263 - loss: 0.0 - 0.2563786814094148\n",
      "Epoch 1264 - loss: 0.0 - 0.25635224486312147\n",
      "Epoch 1265 - loss: 0.0 - 0.2563258109603508\n",
      "Epoch 1266 - loss: 0.0 - 0.25629937970083827\n",
      "Epoch 1267 - loss: 0.0 - 0.25627295108431947\n",
      "Epoch 1268 - loss: 0.0 - 0.25624652511053025\n",
      "Epoch 1269 - loss: 0.0 - 0.2562201017792064\n",
      "Epoch 1270 - loss: 0.0 - 0.25619368109008356\n",
      "Epoch 1271 - loss: 0.0 - 0.2561672630428975\n",
      "Epoch 1272 - loss: 0.0 - 0.2561408476373842\n",
      "Epoch 1273 - loss: 0.0 - 0.25611443487327934\n"
     ]
    }
   ],
   "source": [
    "evomodel = VectorizedEvoClassifierM(n = 24 * 4, hidden_layers = [10], lr_target = 0.1, lr_initial_decay = 60, lr_final_decay = 0.3, random_state = 42)\n",
    "evomodel.fit(X, y, epochs = 10000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(evomodel.training_loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 1.1082605459524975 - 0.58\n",
      "Epoch 2 - loss: 0.9543084041914337 - 0.12621184586265957\n",
      "Epoch 3 - loss: 0.9359153862388222 - 0.09404898574986237\n",
      "Epoch 4 - loss: 0.9268551659953826 - 0.08427106939606842\n",
      "Epoch 5 - loss: 0.9114848113842181 - 0.08129845912799868\n",
      "Epoch 6 - loss: 0.909553585999841 - 0.08039474800119967\n",
      "Epoch 7 - loss: 0.8952747188709296 - 0.08012000838616408\n",
      "Epoch 8 - loss: 0.8915235571537253 - 0.08003648406757206\n",
      "Epoch 9 - loss: 0.8545270712003362 - 0.08001109161808728\n",
      "Epoch 10 - loss: 0.8381145037108438 - 0.08000337199221416\n",
      "Epoch 12 - loss: 0.8112831889965421 - 0.0800003116519689\n",
      "Epoch 13 - loss: 0.7953168669221099 - 0.08000009474614113\n",
      "Epoch 14 - loss: 0.7923423801548026 - 0.0800000288040255\n",
      "Epoch 15 - loss: 0.7862474367635817 - 0.08000000875678814\n",
      "Epoch 17 - loss: 0.7696366769355939 - 0.08000000080933468\n",
      "Epoch 18 - loss: 0.7672391522298984 - 0.08000000024604799\n",
      "Epoch 20 - loss: 0.7523209175194037 - 0.08000000002274066\n",
      "Epoch 21 - loss: 0.7466652546095311 - 0.08000000000691346\n",
      "Epoch 23 - loss: 0.7255198958173092 - 0.08000000000063896\n",
      "Epoch 25 - loss: 0.7058590080259738 - 0.08000000000005905\n",
      "Epoch 27 - loss: 0.6990665619423027 - 0.08000000000000546\n",
      "Epoch 28 - loss: 0.6909209930196192 - 0.08000000000000167\n",
      "Epoch 29 - loss: 0.6846982419927229 - 0.0800000000000005\n",
      "Epoch 30 - loss: 0.674919506517508 - 0.08000000000000015\n",
      "Epoch 32 - loss: 0.6479022902974371 - 0.08000000000000002\n",
      "Epoch 35 - loss: 0.631369541177049 - 0.08\n",
      "Epoch 36 - loss: 0.6293241691401025 - 0.08\n",
      "Epoch 38 - loss: 0.6196026702235303 - 0.08\n",
      "Epoch 39 - loss: 0.6105273285010863 - 0.08\n",
      "Epoch 41 - loss: 0.6024983494061142 - 0.08\n",
      "Epoch 43 - loss: 0.5942257296681462 - 0.08\n",
      "Epoch 47 - loss: 0.5781253191164887 - 0.08\n",
      "Epoch 49 - loss: 0.5692478664922944 - 0.08\n",
      "Epoch 50 - loss: 0.56542633174807 - 0.08\n",
      "Epoch 53 - loss: 0.5617568170956522 - 0.08\n",
      "Epoch 54 - loss: 0.5503299209081821 - 0.08\n",
      "Epoch 55 - loss: 0.5496500443638271 - 0.08\n",
      "Epoch 57 - loss: 0.5475617828626654 - 0.08\n",
      "Epoch 58 - loss: 0.5312255604422151 - 0.08\n",
      "Epoch 62 - loss: 0.5297412325314602 - 0.08\n",
      "Epoch 65 - loss: 0.5219499648331954 - 0.08\n",
      "Epoch 67 - loss: 0.5184394642983349 - 0.08\n",
      "Epoch 68 - loss: 0.5066343521476693 - 0.08\n",
      "Epoch 72 - loss: 0.5066191279926724 - 0.08\n",
      "Epoch 73 - loss: 0.5037985618795499 - 0.08\n",
      "Epoch 74 - loss: 0.49585442281713754 - 0.08\n",
      "Epoch 75 - loss: 0.48832142343879464 - 0.08\n",
      "Epoch 76 - loss: 0.48163768517974453 - 0.08\n",
      "Epoch 77 - loss: 0.4784238699194795 - 0.08\n",
      "Epoch 79 - loss: 0.47766020371593876 - 0.08\n",
      "Epoch 81 - loss: 0.46361843667876185 - 0.08\n",
      "Epoch 85 - loss: 0.44457238455768305 - 0.08\n",
      "Epoch 89 - loss: 0.43580101119360626 - 0.08\n",
      "Epoch 91 - loss: 0.43537983367190997 - 0.08\n",
      "Epoch 92 - loss: 0.4336667785074875 - 0.08\n",
      "Epoch 94 - loss: 0.4316411029423816 - 0.08\n",
      "Epoch 96 - loss: 0.429176255682667 - 0.08\n",
      "Epoch 97 - loss: 0.4254703051829105 - 0.08\n",
      "Epoch 98 - loss: 0.4247034871900297 - 0.08\n",
      "Epoch 99 - loss: 0.423748811424556 - 0.08\n"
     ]
    }
   ],
   "source": [
    "evomodel = VectorizedEvoClassifierM01(n = 48, hidden_layers = [10], random_state = 42)\n",
    "evomodel.fit(X, y, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = evomodel.predict(X)\n",
    "\n",
    "(y_pred > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 10)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "a = np.array([\n",
    "    [[15.48941891, -2.09846987, -8.58431883, 0.92524371, 4.67316101, 15.6263859, 12.23011818, 10.22337356, -5.97467029, 0.7186], \n",
    "    [-1.67894923, 4.82557838, -4.92448585, 1.07249996, -4.52925044, 20.74225489, 6.74903597, 15.98542702, -3.82235716, 1.74474188]],\n",
    "    \n",
    "    [[15.48941891, -2.09846987, -8.58431883, 0.92524371, 4.67316101, 15.6263859, 12.23011818, 10.22337356, -5.97467029, 0.7186], \n",
    "    [-1.67894923, 4.82557838, -4.92448585, 1.07249996, -4.52925044, 20.74225489, 6.74903597, 15.98542702, -3.82235716, 1.74474188]]\n",
    "])\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_function = lambda x: np.exp(x) / np.sum(np.exp(x), axis = 2, keepdims = True)\n",
    "\n",
    "test = lambda x: np.sum(np.exp(x), axis = 2)\n",
    "\n",
    "b = activation_function(a)\n",
    "b\n",
    "\n",
    "bt = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.04082153, 5.04082153])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(-bt * np.log10(b), axis = 2), axis = 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47d3b7ff548c1bae2d6b155a9b3d6f1122689b634566f833764ba5dd9fcfa2e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Deep-learning-Daniel-Petersson-bXusHwTH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
