{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "[[1, 1, 1], [1, 0, 1], [1, 0, 1], [1, 0, 1], [1, 1, 1]],\n",
    "[[0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]],\n",
    "[[1, 1, 1], [0, 0, 1], [1, 1, 1], [1, 0, 0], [1, 1, 1]],\n",
    "[[1, 1, 1], [0, 0, 1], [1, 1, 1], [0, 0, 1], [1, 1, 1]],\n",
    "[[1, 0, 1], [1, 0, 1], [1, 1, 1], [0, 0, 1], [0, 0, 1]],\n",
    "[[1, 0, 1], [1, 0, 1], [1, 1, 1], [0, 0, 1], [0, 0, 1]],\n",
    "[[1, 1, 1], [1, 0, 0], [1, 1, 1], [0, 0, 1], [1, 1, 1]],\n",
    "[[1, 1, 1], [1, 0, 0], [1, 1, 1], [1, 0, 1], [1, 1, 1]],\n",
    "[[1, 1, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]],\n",
    "[[1, 1, 1], [1, 0, 1], [1, 1, 1], [1, 0, 1], [1, 1, 1]],\n",
    "[[1, 1, 1], [1, 0, 1], [1, 1, 1], [0, 0, 1], [1, 1, 1]]]).reshape(11, -1)\n",
    "\n",
    "y = pd.get_dummies(pd.DataFrame({\"y\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]}), drop_first = False).values.astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 1.4620975333273363 - 1.0591700149950014\n",
      "Epoch 2 - loss: 1.0660570925007433 - 0.7567501204128965\n",
      "Epoch 3 - loss: 0.9693921706104199 - 0.6417829614124327\n",
      "Epoch 4 - loss: 0.9106338716995542 - 0.5457519238109084\n",
      "Epoch 6 - loss: 0.8858587018520923 - 0.3985331952020093\n",
      "Epoch 7 - loss: 0.8685268723315486 - 0.34256327497219746\n",
      "Epoch 8 - loss: 0.8528331500141249 - 0.2958096095857409\n",
      "Epoch 9 - loss: 0.8340046745336416 - 0.25675381171339234\n",
      "Epoch 10 - loss: 0.8114845851378024 - 0.22412765067582877\n",
      "Epoch 11 - loss: 0.7792348585162082 - 0.19687183874968364\n",
      "Epoch 12 - loss: 0.7601730280382892 - 0.1741016074932371\n",
      "Epoch 14 - loss: 0.7588270718099785 - 0.13918363269482578\n",
      "Epoch 15 - loss: 0.7302012687859982 - 0.12590308230967884\n",
      "Epoch 17 - loss: 0.7024409327418527 - 0.10553177083548385\n",
      "Epoch 18 - loss: 0.6953571227810953 - 0.09778092009120423\n",
      "Epoch 19 - loss: 0.6802478311186408 - 0.09130221144910564\n",
      "Epoch 20 - loss: 0.6753461677383283 - 0.08588606235552002\n",
      "Epoch 21 - loss: 0.6534418897871102 - 0.0813574193009857\n",
      "Epoch 22 - loss: 0.6461227118626268 - 0.07757006910695377\n",
      "Epoch 23 - loss: 0.6257415992854672 - 0.0744018874367853\n",
      "Epoch 24 - loss: 0.6216663142284393 - 0.07175087012189132\n",
      "Epoch 25 - loss: 0.6147924282767884 - 0.06953181833301364\n",
      "Epoch 26 - loss: 0.6022343084293107 - 0.06767356987465045\n",
      "Epoch 28 - loss: 0.593177557784308 - 0.06481152299122671\n",
      "Epoch 29 - loss: 0.5758178189337003 - 0.0637166125472213\n",
      "Epoch 30 - loss: 0.5693694908057597 - 0.06279732053027398\n",
      "Epoch 31 - loss: 0.5646861121644247 - 0.06202471830098714\n",
      "Epoch 32 - loss: 0.5550203932807904 - 0.061374643253164975\n",
      "Epoch 33 - loss: 0.5526734309862034 - 0.06082691360249604\n",
      "Epoch 34 - loss: 0.5457481115011117 - 0.06036467254001166\n",
      "Epoch 35 - loss: 0.5434061501034114 - 0.05997384043727378\n",
      "Epoch 36 - loss: 0.529788944412139 - 0.05964265730160307\n",
      "Epoch 37 - loss: 0.5239116073396819 - 0.05936130061251422\n",
      "Epoch 38 - loss: 0.5153300743296426 - 0.059121566120187784\n",
      "Epoch 39 - loss: 0.5095950744095813 - 0.058916601232886466\n",
      "Epoch 40 - loss: 0.5085280843587958 - 0.058740682329205575\n",
      "Epoch 41 - loss: 0.5046746941526247 - 0.058589028758474014\n",
      "Epoch 42 - loss: 0.4925199522955673 - 0.058457647484875856\n",
      "Epoch 43 - loss: 0.4920995376403676 - 0.05834320332669277\n",
      "Epoch 44 - loss: 0.48490671624401643 - 0.05824291057383133\n",
      "Epoch 45 - loss: 0.4762254987130037 - 0.058154442461529794\n",
      "Epoch 46 - loss: 0.4754524441641861 - 0.058075855558411556\n",
      "Epoch 47 - loss: 0.46812348562905853 - 0.05800552661172321\n",
      "Epoch 50 - loss: 0.46211608490376005 - 0.05783160931640379\n",
      "Epoch 51 - loss: 0.461595496828459 - 0.057782809709819395\n",
      "Epoch 52 - loss: 0.460868068187474 - 0.057737383944876396\n",
      "Epoch 53 - loss: 0.45192976714632616 - 0.05769478083959639\n",
      "Epoch 54 - loss: 0.45054251943119233 - 0.05765454001542091\n",
      "Epoch 55 - loss: 0.44734940803768614 - 0.05761627693721256\n",
      "Epoch 56 - loss: 0.44663156773106305 - 0.05757967041793897\n",
      "Epoch 57 - loss: 0.4450730317400312 - 0.057544452181979545\n",
      "Epoch 58 - loss: 0.44408786751183377 - 0.05751039814789329\n",
      "Epoch 59 - loss: 0.43846846152493024 - 0.057477321147364265\n",
      "Epoch 60 - loss: 0.4382985554126951 - 0.05744506484371186\n",
      "Epoch 61 - loss: 0.4294873248326478 - 0.057413498652336005\n",
      "Epoch 64 - loss: 0.4246135351982012 - 0.057321936868345504\n",
      "Epoch 66 - loss: 0.4242984793505791 - 0.05726277172227424\n",
      "Epoch 67 - loss: 0.4183905092881767 - 0.05723359052018138\n",
      "Epoch 69 - loss: 0.41704553254848326 - 0.05717584438400209\n",
      "Epoch 70 - loss: 0.40935329144804755 - 0.05714722263705576\n",
      "Epoch 72 - loss: 0.4049727995433527 - 0.057090373068523916\n",
      "Epoch 73 - loss: 0.4042087646687404 - 0.05706211211872959\n",
      "Epoch 74 - loss: 0.4036558847418021 - 0.05703394276557185\n",
      "Epoch 75 - loss: 0.40236702428598237 - 0.057005854480520556\n",
      "Epoch 76 - loss: 0.39014398794440813 - 0.05697783846507718\n",
      "Epoch 77 - loss: 0.37318256876540035 - 0.05694988736575415\n",
      "Epoch 79 - loss: 0.35686955342702925 - 0.05689415633741495\n",
      "Epoch 81 - loss: 0.34831836114154857 - 0.05683862335131983\n",
      "Epoch 82 - loss: 0.3377950945844348 - 0.05681092246504319\n",
      "Epoch 83 - loss: 0.3375001620329507 - 0.056783261799714164\n",
      "Epoch 85 - loss: 0.33748046033142715 - 0.05672805305348636\n",
      "Epoch 86 - loss: 0.3303451438191811 - 0.05670050173277131\n",
      "Epoch 87 - loss: 0.3290217542184313 - 0.05667298404987794\n",
      "Epoch 88 - loss: 0.3244905716937174 - 0.05664549896615604\n",
      "Epoch 89 - loss: 0.31147428131621124 - 0.056618045609573585\n",
      "Epoch 90 - loss: 0.3086004601905576 - 0.056590623247270745\n",
      "Epoch 91 - loss: 0.30791114798617464 - 0.05656323126263544\n",
      "Epoch 92 - loss: 0.3052123442378807 - 0.05653586913615594\n",
      "Epoch 93 - loss: 0.2969326543644739 - 0.05650853642942779\n",
      "Epoch 95 - loss: 0.2920768228465551 - 0.056453957849196955\n",
      "Epoch 96 - loss: 0.29160606245759246 - 0.056426711394840685\n",
      "Epoch 97 - loss: 0.28480802948964945 - 0.05639949318142568\n",
      "Epoch 98 - loss: 0.27672718601067725 - 0.05637230301463817\n",
      "Epoch 100 - loss: 0.274253690176058 - 0.05631800617694133\n",
      "Epoch 101 - loss: 0.2662110718898017 - 0.05629089923779557\n",
      "Epoch 102 - loss: 0.2610521260094815 - 0.05626381980185181\n",
      "Epoch 104 - loss: 0.2531648529174033 - 0.056209743070762354\n",
      "Epoch 105 - loss: 0.25291412320418244 - 0.056182745617415215\n",
      "Epoch 106 - loss: 0.24538055535898434 - 0.056155775347483235\n",
      "Epoch 107 - loss: 0.23980167809906716 - 0.056128832200935876\n",
      "Epoch 108 - loss: 0.2311655302271565 - 0.05610191612321649\n",
      "Epoch 111 - loss: 0.22241247250704313 - 0.0560213298217321\n",
      "Epoch 112 - loss: 0.22066743704654648 - 0.055994521554763775\n",
      "Epoch 113 - loss: 0.21614618710485478 - 0.05596774013922893\n",
      "Epoch 114 - loss: 0.21435521297680785 - 0.05594098553897021\n",
      "Epoch 115 - loss: 0.2092390635071661 - 0.055914257719401735\n",
      "Epoch 117 - loss: 0.20579017249877385 - 0.05586088229036446\n",
      "Epoch 118 - loss: 0.20289748790602838 - 0.055834234617493546\n",
      "Epoch 119 - loss: 0.20087607057838744 - 0.05580761359818274\n",
      "Epoch 120 - loss: 0.1962524499197316 - 0.05578101920262745\n",
      "Epoch 121 - loss: 0.19355775920937834 - 0.05575445140157428\n",
      "Epoch 122 - loss: 0.1931366219497335 - 0.05572791016623443\n",
      "Epoch 123 - loss: 0.1916630211976834 - 0.055701395468211616\n",
      "Epoch 124 - loss: 0.1868739102901362 - 0.05567490727944172\n",
      "Epoch 126 - loss: 0.1860215106026166 - 0.055622010318770905\n",
      "Epoch 127 - loss: 0.1843126642458538 - 0.05559560149198924\n",
      "Epoch 128 - loss: 0.1761910963041182 - 0.05556921906463442\n",
      "Epoch 129 - loss: 0.17547455465284054 - 0.05554286300969414\n",
      "Epoch 130 - loss: 0.1718034304230572 - 0.05551653330028624\n",
      "Epoch 131 - loss: 0.16794075116667997 - 0.05549022990964159\n",
      "Epoch 133 - loss: 0.16706240917375406 - 0.05543770197804717\n",
      "Epoch 134 - loss: 0.16263159943391375 - 0.05541147738400682\n",
      "Epoch 136 - loss: 0.16231903944821532 - 0.05535910680724059\n",
      "Epoch 137 - loss: 0.16153869130596668 - 0.055332960771816285\n",
      "Epoch 138 - loss: 0.1565644495227964 - 0.055306840869986724\n",
      "Epoch 139 - loss: 0.1526783659756227 - 0.05528074707552793\n",
      "Epoch 140 - loss: 0.14928254318266423 - 0.05525467936225915\n",
      "Epoch 141 - loss: 0.14880311327683346 - 0.05522863770404004\n",
      "Epoch 142 - loss: 0.14794595739806837 - 0.0552026220747683\n",
      "Epoch 143 - loss: 0.14735482118456533 - 0.05517663244837765\n",
      "Epoch 144 - loss: 0.14582752152886944 - 0.05515066879883613\n",
      "Epoch 146 - loss: 0.1432095587187656 - 0.05509881932633629\n",
      "Epoch 147 - loss: 0.14152335058220047 - 0.05507293345147433\n",
      "Epoch 149 - loss: 0.14036378821177908 - 0.05502123929499323\n",
      "Epoch 150 - loss: 0.13822796508577959 - 0.05499543096164836\n",
      "Epoch 151 - loss: 0.1361863358822199 - 0.05496964842379746\n",
      "Epoch 154 - loss: 0.13274512594704332 - 0.05489245532541961\n",
      "Epoch 157 - loss: 0.13221501258270965 - 0.05481549345958541\n",
      "Epoch 158 - loss: 0.1304602396233223 - 0.05478989076951744\n",
      "Epoch 159 - loss: 0.12953445019465915 - 0.054764313669356766\n",
      "Epoch 163 - loss: 0.12841130907123274 - 0.05466226065658649\n",
      "Epoch 164 - loss: 0.12508520639980727 - 0.05463681112278806\n",
      "Epoch 165 - loss: 0.12350157095697117 - 0.05461138702580776\n",
      "Epoch 166 - loss: 0.1229972773999961 - 0.054585988340220694\n",
      "Epoch 168 - loss: 0.12146322920445 - 0.05453526710165433\n",
      "Epoch 170 - loss: 0.12103392102890137 - 0.05448464720419973\n",
      "Epoch 173 - loss: 0.1199730817934212 - 0.05440890692978274\n",
      "Epoch 174 - loss: 0.11854718064927919 - 0.05438371062310111\n",
      "Epoch 175 - loss: 0.11712470296454015 - 0.05435853950013301\n",
      "Epoch 176 - loss: 0.11471795716166698 - 0.05433339353570719\n",
      "Epoch 182 - loss: 0.11430549494928641 - 0.05418304467211863\n",
      "Epoch 183 - loss: 0.11312190417270483 - 0.054158074114805076\n",
      "Epoch 185 - loss: 0.11306157076464354 - 0.054108207849461416\n",
      "Epoch 187 - loss: 0.1130347671436478 - 0.05405844121698281\n",
      "Epoch 189 - loss: 0.1110970378359699 - 0.05400877401830254\n",
      "Epoch 192 - loss: 0.11020242136938006 - 0.05393445922417422\n",
      "Epoch 193 - loss: 0.10919527954456335 - 0.053909737128058126\n",
      "Epoch 194 - loss: 0.10911728071389533 - 0.05388503974168123\n",
      "Epoch 195 - loss: 0.10809074325393837 - 0.05386036704034617\n",
      "Epoch 197 - loss: 0.10715545018600779 - 0.053811095594135334\n",
      "Epoch 200 - loss: 0.10583489860903827 - 0.05373737294661602\n",
      "Epoch 201 - loss: 0.10571864921324839 - 0.05371284783826729\n",
      "Epoch 202 - loss: 0.10506019886711218 - 0.05368834724276846\n",
      "Epoch 203 - loss: 0.10390360788208927 - 0.053663871135618914\n",
      "Epoch 204 - loss: 0.10236952371304463 - 0.053639419492342556\n",
      "Epoch 207 - loss: 0.10208873353691125 - 0.05356621110135824\n",
      "Epoch 208 - loss: 0.10170333225141173 - 0.0535418570693024\n",
      "Epoch 209 - loss: 0.10017447725719802 - 0.05351752737910565\n",
      "Epoch 210 - loss: 0.09913313619227547 - 0.053493222006438336\n",
      "Epoch 211 - loss: 0.09909105271635169 - 0.053468940926995036\n",
      "Epoch 212 - loss: 0.0985153192063469 - 0.053444684116494674\n",
      "Epoch 216 - loss: 0.09784216930107577 - 0.05334789907915006\n",
      "Epoch 217 - loss: 0.09739373866355115 - 0.053323763249996804\n",
      "Epoch 218 - loss: 0.09694435127290899 - 0.05329965154460881\n",
      "Epoch 219 - loss: 0.09619463235807348 - 0.05327556393887437\n",
      "Epoch 220 - loss: 0.09526753692925326 - 0.05325150040870586\n",
      "Epoch 222 - loss: 0.09478058413441696 - 0.05320344547883663\n",
      "Epoch 223 - loss: 0.09439613853886462 - 0.053179454031080965\n",
      "Epoch 224 - loss: 0.093227665177048 - 0.053155486562781316\n",
      "Epoch 225 - loss: 0.09292371510802128 - 0.05313154304997023\n",
      "Epoch 226 - loss: 0.09154921375531214 - 0.0531076234687042\n",
      "Epoch 227 - loss: 0.09070965365158008 - 0.053083727795063616\n",
      "Epoch 228 - loss: 0.08868766933141753 - 0.05305985600515282\n",
      "Epoch 232 - loss: 0.08594144516477419 - 0.05296460720572942\n",
      "Epoch 233 - loss: 0.08534161818490994 - 0.05294085447686752\n",
      "Epoch 234 - loss: 0.08411256077917639 - 0.05291712548886208\n",
      "Epoch 236 - loss: 0.0810345425061341 - 0.05286973864052829\n",
      "Epoch 239 - loss: 0.0808924315080562 - 0.052798835831996606\n",
      "Epoch 240 - loss: 0.07893402028381648 - 0.052775248791650374\n",
      "Epoch 242 - loss: 0.07632151460953959 - 0.052728145413146695\n",
      "Epoch 244 - loss: 0.07440550702739762 - 0.05268113614725605\n",
      "Epoch 245 - loss: 0.07402781113998314 - 0.05265766674776432\n",
      "Epoch 246 - loss: 0.07239628225104217 - 0.0526342208059413\n",
      "Epoch 247 - loss: 0.0687959476670917 - 0.052610798298341026\n",
      "Epoch 249 - loss: 0.06682599434840074 - 0.052564023492142155\n",
      "Epoch 250 - loss: 0.06601268518647452 - 0.052540671146768726\n",
      "Epoch 251 - loss: 0.0648353369043927 - 0.052517342142068386\n",
      "Epoch 252 - loss: 0.06379940240130637 - 0.05249403645471213\n",
      "Epoch 254 - loss: 0.06168292838418951 - 0.05244749493883243\n",
      "Epoch 256 - loss: 0.058967262218247836 - 0.05240104641296348\n",
      "Epoch 257 - loss: 0.055530290216336486 - 0.05237785696320784\n",
      "Epoch 259 - loss: 0.05531825819806418 - 0.052331547574107\n",
      "Epoch 260 - loss: 0.05529628027312234 - 0.05230842758845238\n",
      "Epoch 261 - loss: 0.0506821659303969 - 0.05228533071122728\n",
      "Epoch 263 - loss: 0.05034295765382783 - 0.05223920618970119\n",
      "Epoch 264 - loss: 0.047057163467264095 - 0.05221617849927567\n",
      "Epoch 266 - loss: 0.0456788752593644 - 0.052170192143961215\n",
      "Epoch 267 - loss: 0.0456262951906129 - 0.05214723343308592\n",
      "Epoch 268 - loss: 0.04441574056517135 - 0.052124297669445964\n",
      "Epoch 269 - loss: 0.043362046114626566 - 0.0521013848301056\n",
      "Epoch 270 - loss: 0.040840671392080564 - 0.05207849489215196\n",
      "Epoch 272 - loss: 0.03726468449165085 - 0.05203278362886803\n",
      "Epoch 274 - loss: 0.03489574952228849 - 0.05198716369674906\n",
      "Epoch 275 - loss: 0.03485875574676424 - 0.051964387922837245\n",
      "Epoch 276 - loss: 0.03358478592922486 - 0.05194163491331525\n",
      "Epoch 277 - loss: 0.033364689430899645 - 0.051918904645430065\n",
      "Epoch 278 - loss: 0.030358464425389496 - 0.05189619709645143\n",
      "Epoch 279 - loss: 0.029924842663667916 - 0.05187351224367177\n",
      "Epoch 280 - loss: 0.02936492108007046 - 0.05185085006440624\n",
      "Epoch 281 - loss: 0.027748104771367955 - 0.05182821053599267\n",
      "Epoch 282 - loss: 0.027632812686276906 - 0.05180559363579152\n",
      "Epoch 283 - loss: 0.02656319223410875 - 0.05178299934118589\n",
      "Epoch 284 - loss: 0.021548236882624083 - 0.051760427629581475\n",
      "Epoch 286 - loss: 0.02075728126834277 - 0.05171535186511204\n",
      "Epoch 287 - loss: 0.019204681423871255 - 0.051692847767171236\n",
      "Epoch 289 - loss: 0.018653836340262724 - 0.051647907027356964\n",
      "Epoch 290 - loss: 0.018241999507267213 - 0.05162547034054273\n",
      "Epoch 291 - loss: 0.01566808110634116 - 0.05160305607920072\n",
      "Epoch 294 - loss: 0.014039496777414784 - 0.051535947623977285\n",
      "Epoch 296 - loss: 0.013834212447590893 - 0.0514913203708582\n",
      "Epoch 297 - loss: 0.012230483126886866 - 0.05146904019243324\n",
      "Epoch 300 - loss: 0.011832303008935053 - 0.051402333182401234\n",
      "Epoch 301 - loss: 0.010918398951518436 - 0.05138014194668596\n",
      "Epoch 303 - loss: 0.010864952587959043 - 0.051335825993517735\n",
      "Epoch 304 - loss: 0.0103716762540828 - 0.05131370123174884\n",
      "Epoch 305 - loss: 0.009194298960168696 - 0.051291598583683005\n",
      "Epoch 307 - loss: 0.009082075339802652 - 0.051247459540272065\n",
      "Epoch 308 - loss: 0.008811858578062997 - 0.051225423100787905\n",
      "Epoch 309 - loss: 0.007753537776927792 - 0.05120340868672868\n",
      "Epoch 310 - loss: 0.00759469185325436 - 0.05118141627607998\n",
      "Epoch 312 - loss: 0.0072488197359812 - 0.051137497377066465\n",
      "Epoch 314 - loss: 0.006788336999739658 - 0.0510936662280717\n",
      "Epoch 317 - loss: 0.006100123514334294 - 0.05102808365243786\n",
      "Epoch 319 - loss: 0.005965710753058256 - 0.05098447111221073\n",
      "Epoch 320 - loss: 0.00596011900584468 - 0.05096269752970423\n",
      "Epoch 321 - loss: 0.004926482577282952 - 0.050940945709897094\n",
      "Epoch 323 - loss: 0.004824264829362512 - 0.05089750727139527\n",
      "Epoch 326 - loss: 0.00467366281916889 - 0.050832512290798215\n",
      "Epoch 328 - loss: 0.004440450747649813 - 0.050789290502412265\n",
      "Epoch 330 - loss: 0.004246940316986554 - 0.05074615507121711\n",
      "Epoch 333 - loss: 0.004234138491303215 - 0.05068161346681626\n",
      "Epoch 334 - loss: 0.003941080899261206 - 0.0506601425905768\n",
      "Epoch 335 - loss: 0.003777659246925377 - 0.05063869317448172\n",
      "Epoch 337 - loss: 0.003746924731951452 - 0.05059585863694847\n",
      "Epoch 338 - loss: 0.0036265120794301275 - 0.05057447347267576\n",
      "Epoch 340 - loss: 0.0035305340943581924 - 0.050531767246192294\n",
      "Epoch 341 - loss: 0.0033564491401386343 - 0.05051044614127532\n",
      "Epoch 342 - loss: 0.0031908083578343484 - 0.05048914634680626\n",
      "Epoch 344 - loss: 0.003023616901515118 - 0.050446610604034\n",
      "Epoch 346 - loss: 0.0029554399406412093 - 0.050404159847732485\n",
      "Epoch 347 - loss: 0.0028457535186348663 - 0.05038296628643154\n",
      "Epoch 349 - loss: 0.0028154012809491337 - 0.0503406426915614\n",
      "Epoch 350 - loss: 0.002803677995243287 - 0.05031951261566863\n",
      "Epoch 351 - loss: 0.002693908308376722 - 0.05029840365929023\n",
      "Epoch 353 - loss: 0.0026753365611388484 - 0.05025624902066182\n",
      "Epoch 354 - loss: 0.0026520000385156428 - 0.05023520329625718\n",
      "Epoch 355 - loss: 0.0024544310746335094 - 0.050214178607057576\n",
      "Epoch 356 - loss: 0.002438434304265783 - 0.050193174932038336\n",
      "Epoch 357 - loss: 0.002391999772255711 - 0.05017219225019578\n",
      "Epoch 358 - loss: 0.002169502133420005 - 0.05015123054054721\n",
      "Epoch 360 - loss: 0.0020094660092165833 - 0.05010936995400619\n",
      "Epoch 362 - loss: 0.001973305959827008 - 0.050067593004972856\n",
      "Epoch 363 - loss: 0.001871579309509856 - 0.050046735842287325\n",
      "Epoch 366 - loss: 0.0018277301016715235 - 0.04998428935133175\n",
      "Epoch 368 - loss: 0.0016785091292500565 - 0.049942762313509255\n",
      "Epoch 369 - loss: 0.0016417846935159986 - 0.04992202991912064\n",
      "Epoch 370 - loss: 0.0016208041278021666 - 0.04990131824676367\n",
      "Epoch 371 - loss: 0.0014864463040522526 - 0.049880627275726674\n",
      "Epoch 372 - loss: 0.0014634086028590175 - 0.04985995698531868\n",
      "Epoch 375 - loss: 0.0013678565128338036 - 0.04979806999126903\n",
      "Epoch 376 - loss: 0.0013478561239571138 - 0.04977748221688061\n",
      "Epoch 378 - loss: 0.0012529037300688503 - 0.04973636837998835\n",
      "Epoch 381 - loss: 0.0012153092760445402 - 0.0496748515961617\n",
      "Epoch 382 - loss: 0.001112831254009453 - 0.04965438697857972\n",
      "Epoch 383 - loss: 0.0010559817321066011 - 0.04963394281538642\n",
      "Epoch 384 - loss: 0.0010505053988555523 - 0.049613519086137634\n",
      "Epoch 385 - loss: 0.0010031415066480956 - 0.04959311577040964\n",
      "Epoch 388 - loss: 0.0009854480378188733 - 0.04953202810041914\n",
      "Epoch 390 - loss: 0.0009646827874955935 - 0.04949140468117868\n",
      "Epoch 393 - loss: 0.000885831024185981 - 0.04943062168721333\n",
      "Epoch 396 - loss: 0.0008712610224531641 - 0.04937002076897972\n",
      "Epoch 397 - loss: 0.0008589425553315971 - 0.049349860829860294\n",
      "Epoch 399 - loss: 0.0008108786994130486 - 0.04930960138106918\n",
      "Epoch 400 - loss: 0.0007801924890271438 - 0.04928950183113804\n",
      "Epoch 401 - loss: 0.0007523232992624237 - 0.04926942237071041\n",
      "Epoch 402 - loss: 0.0007152675619339623 - 0.049249362979706814\n",
      "Epoch 403 - loss: 0.000704688206399226 - 0.049229323638067875\n",
      "Epoch 404 - loss: 0.0006582624794857712 - 0.04920930432575423\n",
      "Epoch 405 - loss: 0.0006396600981429201 - 0.04918930502274659\n",
      "Epoch 406 - loss: 0.0006250704820652881 - 0.049169325709045635\n",
      "Epoch 407 - loss: 0.000609424553008229 - 0.04914936636467206\n",
      "Epoch 411 - loss: 0.0005673133562808459 - 0.04906972828156367\n",
      "Epoch 413 - loss: 0.0005536566060655767 - 0.049030028537977374\n",
      "Epoch 414 - loss: 0.0005128739990515336 - 0.04901020842114949\n",
      "Epoch 415 - loss: 0.0004866498686821658 - 0.048990408114531675\n",
      "Epoch 420 - loss: 0.00047769449022266267 - 0.04889170304227514\n",
      "Epoch 421 - loss: 0.00047708633909945635 - 0.048872021181803255\n",
      "Epoch 422 - loss: 0.00042568157878902427 - 0.04885235899335419\n",
      "Epoch 423 - loss: 0.00040401957313397115 - 0.04883271645726576\n",
      "Epoch 427 - loss: 0.00040287037436142234 - 0.048754342443961964\n",
      "Epoch 429 - loss: 0.00037466619413499476 - 0.0487152728416995\n",
      "Epoch 434 - loss: 0.0003654633645021777 - 0.04861794001234104\n",
      "Epoch 435 - loss: 0.0003562764145284675 - 0.04859853177806319\n",
      "Epoch 436 - loss: 0.0003344818654205569 - 0.048579142942318736\n",
      "Epoch 437 - loss: 0.0003103925467627668 - 0.04855977348571884\n",
      "Epoch 439 - loss: 0.0003010236499235208 - 0.048521092632494246\n",
      "Epoch 440 - loss: 0.0002881219079435475 - 0.04850178119718869\n",
      "Epoch 442 - loss: 0.00026377670842871637 - 0.04846321621263386\n",
      "Epoch 443 - loss: 0.00025473704421386526 - 0.0484439626248196\n",
      "Epoch 444 - loss: 0.00022642098385044116 - 0.04842472828096956\n",
      "Epoch 449 - loss: 0.0002110285342414037 - 0.0483288445486532\n",
      "Epoch 454 - loss: 0.0001991836024520202 - 0.04823343903844685\n",
      "Epoch 456 - loss: 0.00017855254656951445 - 0.048195410201882796\n",
      "Epoch 457 - loss: 0.00016556051148689638 - 0.048176424286220904\n",
      "Epoch 460 - loss: 0.00015751162754371745 - 0.04811958032194163\n",
      "Epoch 461 - loss: 0.00014284273439420732 - 0.048100670198257374\n",
      "Epoch 462 - loss: 0.00014193737518087567 - 0.04808177897524489\n",
      "Epoch 464 - loss: 0.00013842281088118002 - 0.04804405315568922\n",
      "Epoch 465 - loss: 0.0001305782805368285 - 0.04802521852142023\n",
      "Epoch 467 - loss: 0.00012942518254118127 - 0.047987605709726676\n",
      "Epoch 468 - loss: 0.0001247472790973784 - 0.04796882749468933\n",
      "Epoch 469 - loss: 0.00011474547923886277 - 0.04795006804848103\n",
      "Epoch 473 - loss: 0.0001056614855014289 - 0.04787521757703061\n",
      "Epoch 475 - loss: 0.00010016927720841537 - 0.04783790446742386\n",
      "Epoch 478 - loss: 9.12894860337129e-05 - 0.04778207454078398\n",
      "Epoch 480 - loss: 7.865683707788002e-05 - 0.047744947531087774\n",
      "Epoch 484 - loss: 7.615220758922512e-05 - 0.04767091590292855\n",
      "Epoch 485 - loss: 7.526914966931782e-05 - 0.04765245421940586\n",
      "Epoch 486 - loss: 7.154001860519588e-05 - 0.04763401098833892\n",
      "Epoch 490 - loss: 6.200940745918298e-05 - 0.0475604222200405\n",
      "Epoch 492 - loss: 5.832094195230895e-05 - 0.047523738071976525\n",
      "Epoch 504 - loss: 5.612851631490443e-05 - 0.04730516726281122\n",
      "Epoch 507 - loss: 5.181531343874816e-05 - 0.04725093315286328\n",
      "Epoch 516 - loss: 5.0593145673604786e-05 - 0.04708920362901763\n",
      "Epoch 517 - loss: 4.9557349004403895e-05 - 0.047071323367009636\n",
      "Epoch 520 - loss: 4.7861432493812636e-05 - 0.047017789737503084\n",
      "Epoch 521 - loss: 4.702955094884769e-05 - 0.046999980853691566\n",
      "Epoch 523 - loss: 4.4430840925779694e-05 - 0.04696441646822444\n",
      "Epoch 524 - loss: 3.931403451969819e-05 - 0.046946660931004455\n",
      "Epoch 526 - loss: 3.8839109396517864e-05 - 0.04691120307881392\n",
      "Epoch 527 - loss: 3.510840494406722e-05 - 0.046893500728385516\n",
      "Epoch 529 - loss: 3.318804175521377e-05 - 0.04685814909035066\n",
      "Epoch 530 - loss: 3.167057880057315e-05 - 0.04684049976739257\n",
      "Epoch 531 - loss: 2.9405730618843668e-05 - 0.04682286808493571\n",
      "Epoch 534 - loss: 2.747934905425216e-05 - 0.04677007870434382\n",
      "Epoch 535 - loss: 2.7460606331634865e-05 - 0.04675251740775121\n",
      "Epoch 537 - loss: 2.3917675093330252e-05 - 0.04671744745457886\n",
      "Epoch 538 - loss: 2.3019825771654575e-05 - 0.04669993876292917\n",
      "Epoch 543 - loss: 2.1937757372089696e-05 - 0.04661265745422227\n",
      "Epoch 549 - loss: 2.033366602016832e-05 - 0.04650849431141461\n",
      "Epoch 554 - loss: 1.986244369922964e-05 - 0.046422167835893084\n",
      "Epoch 555 - loss: 1.9080617907589716e-05 - 0.04640495427627146\n",
      "Epoch 556 - loss: 1.6545056415241224e-05 - 0.04638775792160555\n",
      "Epoch 560 - loss: 1.6507105943533026e-05 - 0.046319144208829606\n",
      "Epoch 561 - loss: 1.482772142724894e-05 - 0.04630203362134041\n",
      "Epoch 563 - loss: 1.4774616667551761e-05 - 0.04626786373537366\n",
      "Epoch 564 - loss: 1.4112233262439547e-05 - 0.04625080440272621\n",
      "Epoch 565 - loss: 1.387970269300337e-05 - 0.0462337621208846\n",
      "Epoch 566 - loss: 1.3489188948126799e-05 - 0.04621673687280652\n",
      "Epoch 567 - loss: 1.2609484713147128e-05 - 0.04619972864146674\n",
      "Epoch 568 - loss: 1.2415774822905176e-05 - 0.04618273740985702\n",
      "Epoch 571 - loss: 1.2168602463969516e-05 - 0.04613186554358075\n",
      "Epoch 572 - loss: 1.2010940256890048e-05 - 0.046114942141148675\n",
      "Epoch 574 - loss: 1.116798570155604e-05 - 0.046081146064208676\n",
      "Epoch 575 - loss: 1.02523879370514e-05 - 0.04606427335590468\n",
      "Epoch 577 - loss: 9.387196562569202e-06 - 0.04603057851526512\n",
      "Epoch 580 - loss: 9.249609894375582e-06 - 0.045980162441641804\n",
      "Epoch 581 - loss: 8.38741305673134e-06 - 0.045963390666485394\n",
      "Epoch 584 - loss: 7.774202271219761e-06 - 0.045913175854365225\n",
      "Epoch 585 - loss: 7.358328715527498e-06 - 0.04589647103231395\n",
      "Epoch 591 - loss: 7.092378991649921e-06 - 0.04579659214242622\n",
      "Epoch 592 - loss: 7.02651756834533e-06 - 0.04578000384581445\n",
      "Epoch 594 - loss: 6.458652464025444e-06 - 0.0457468769760349\n",
      "Epoch 597 - loss: 6.237492844180965e-06 - 0.045697310731648046\n",
      "Epoch 598 - loss: 6.167715468961892e-06 - 0.045680821666822906\n",
      "Epoch 599 - loss: 5.748440054351482e-06 - 0.04566434908282079\n",
      "Epoch 600 - loss: 5.539757511661353e-06 - 0.045647892963169144\n",
      "Epoch 601 - loss: 5.430550042719252e-06 - 0.04563145329141183\n",
      "Epoch 603 - loss: 5.0005285475581305e-06 - 0.04559862322583793\n",
      "Epoch 604 - loss: 4.747218245549291e-06 - 0.04558223279919129\n",
      "Epoch 609 - loss: 4.73153972372158e-06 - 0.045500526072235\n",
      "Epoch 610 - loss: 4.646215803450308e-06 - 0.04548423369370972\n",
      "Epoch 612 - loss: 4.309762371008919e-06 - 0.04545169777308823\n",
      "Epoch 617 - loss: 4.126877559949108e-06 - 0.04537064209229071\n",
      "Epoch 618 - loss: 4.093897646345823e-06 - 0.04535447953282504\n",
      "Epoch 619 - loss: 3.944810027300744e-06 - 0.04533833312784024\n",
      "Epoch 621 - loss: 3.6727231512934844e-06 - 0.04530608871674379\n",
      "Epoch 624 - loss: 3.5718647408837595e-06 - 0.04525784285556971\n",
      "Epoch 628 - loss: 3.5500469338666246e-06 - 0.045193739775777765\n",
      "Epoch 629 - loss: 3.507284744509964e-06 - 0.045177754030206914\n",
      "Epoch 631 - loss: 3.2902998412709788e-06 - 0.04514583045636154\n",
      "Epoch 632 - loss: 3.1923038130235662e-06 - 0.045129892596163436\n",
      "Epoch 634 - loss: 2.9612754322794033e-06 - 0.04509806464952706\n",
      "Epoch 638 - loss: 2.7053545141860215e-06 - 0.045034599406036234\n",
      "Epoch 639 - loss: 2.512263034667975e-06 - 0.04501877272129146\n",
      "Epoch 641 - loss: 2.3088701784835187e-06 - 0.04498716679231316\n",
      "Epoch 642 - loss: 2.079957482000639e-06 - 0.04497138751647371\n",
      "Epoch 646 - loss: 1.9485299868570802e-06 - 0.044908427969447905\n",
      "Epoch 650 - loss: 1.8927137460683748e-06 - 0.04484571975760473\n",
      "Epoch 651 - loss: 1.7408179084930993e-06 - 0.044830081858100036\n",
      "Epoch 657 - loss: 1.7359036710543205e-06 - 0.04473658214658494\n",
      "Epoch 658 - loss: 1.5340038631821068e-06 - 0.044721053330140655\n",
      "Epoch 660 - loss: 1.5031104976054502e-06 - 0.04469004224490263\n",
      "Epoch 662 - loss: 1.4226148451283957e-06 - 0.044659093119854255\n",
      "Epoch 663 - loss: 1.4168680005044046e-06 - 0.04464364175370509\n",
      "Epoch 667 - loss: 1.3400764442917808e-06 - 0.04458199057125672\n",
      "Epoch 669 - loss: 1.2733649534927504e-06 - 0.04455125733359628\n",
      "Epoch 671 - loss: 1.2410055620870912e-06 - 0.044520585500985636\n",
      "Epoch 673 - loss: 1.1944999532735538e-06 - 0.04448997495073743\n",
      "Epoch 674 - loss: 1.0436438342408234e-06 - 0.04447469261822648\n",
      "Epoch 678 - loss: 8.995074397976831e-07 - 0.04441371588252726\n",
      "Epoch 680 - loss: 8.728165349891401e-07 - 0.04438331885791916\n",
      "Epoch 682 - loss: 7.837235421972325e-07 - 0.044352982566606726\n",
      "Epoch 685 - loss: 6.464188530275767e-07 - 0.044307591739191146\n",
      "Epoch 691 - loss: 6.362150325194726e-07 - 0.04421721758234883\n",
      "Epoch 692 - loss: 6.269278880586844e-07 - 0.04420220787087303\n",
      "Epoch 694 - loss: 6.26568419553068e-07 - 0.044172233439554086\n",
      "Epoch 695 - loss: 6.204932418265866e-07 - 0.0441572686897365\n",
      "Epoch 697 - loss: 5.909006672080412e-07 - 0.04412738404696135\n",
      "Epoch 698 - loss: 5.544272517884276e-07 - 0.044112464124119136\n",
      "Epoch 699 - loss: 5.474974496839774e-07 - 0.04409755911374229\n",
      "Epoch 700 - loss: 5.414012348409212e-07 - 0.0440826690009258\n",
      "Epoch 701 - loss: 5.149041238697649e-07 - 0.044067793770779545\n",
      "Epoch 702 - loss: 4.811503204587143e-07 - 0.04405293340842831\n",
      "Epoch 703 - loss: 4.003590581714906e-07 - 0.044038087899011716\n",
      "Epoch 711 - loss: 3.6504700462688526e-07 - 0.04391985675097721\n",
      "Epoch 713 - loss: 3.484321914629153e-07 - 0.04389044645757209\n",
      "Epoch 717 - loss: 3.414013919233431e-07 - 0.04383180203877194\n",
      "Epoch 718 - loss: 3.1541779638726405e-07 - 0.04381717755019616\n",
      "Epoch 722 - loss: 3.0985004356845703e-07 - 0.04375882562165501\n",
      "Epoch 724 - loss: 2.9770300248104326e-07 - 0.04372973706866088\n",
      "Epoch 726 - loss: 2.499140390206709e-07 - 0.04370070663463439\n",
      "Epoch 728 - loss: 2.488587015234393e-07 - 0.04367173420345378\n",
      "Epoch 729 - loss: 2.3280594288767428e-07 - 0.04365726970270608\n",
      "Epoch 731 - loss: 2.2687636851860112e-07 - 0.04362838405857334\n",
      "Epoch 734 - loss: 2.112948483061958e-07 - 0.04358516376924682\n",
      "Epoch 736 - loss: 2.0633051844446595e-07 - 0.04355642219286524\n",
      "Epoch 737 - loss: 1.9434125131071537e-07 - 0.04354207294649133\n",
      "Epoch 739 - loss: 1.9326934692625085e-07 - 0.04351341746563103\n",
      "Epoch 740 - loss: 1.7215649438459734e-07 - 0.043499111202489164\n",
      "Epoch 743 - loss: 1.6476913172597175e-07 - 0.04345627815058434\n",
      "Epoch 745 - loss: 1.2578027572420816e-07 - 0.0434277940878406\n",
      "Epoch 750 - loss: 1.2114197993763187e-07 - 0.04335683266878526\n",
      "Epoch 752 - loss: 1.1495890639472853e-07 - 0.043328547298246686\n",
      "Epoch 756 - loss: 1.0992737478654948e-07 - 0.04327214598687817\n",
      "Epoch 758 - loss: 1.040845932611501e-07 - 0.0432440298204429\n",
      "Epoch 763 - loss: 9.975029628102516e-08 - 0.04317398492949485\n",
      "Epoch 768 - loss: 9.907194927055917e-08 - 0.04310428938889785\n",
      "Epoch 771 - loss: 9.4578385827388e-08 - 0.043062639027511\n",
      "Epoch 772 - loss: 9.152586494833562e-08 - 0.04304878331749314\n",
      "Epoch 774 - loss: 8.902946450071347e-08 - 0.04302111342996898\n",
      "Epoch 776 - loss: 8.765232414197165e-08 - 0.04299349882691696\n",
      "Epoch 778 - loss: 8.496183161849927e-08 - 0.042965939397878634\n",
      "Epoch 779 - loss: 8.467664017724597e-08 - 0.04295218033915671\n",
      "Epoch 780 - loss: 6.670237816312206e-08 - 0.04293843503261626\n",
      "Epoch 781 - loss: 5.776719931219276e-08 - 0.042924703464512\n",
      "Epoch 786 - loss: 5.670374313470527e-08 - 0.04285625122040848\n",
      "Epoch 789 - loss: 5.641446895685857e-08 - 0.04281534385847068\n",
      "Epoch 791 - loss: 5.072645561477822e-08 - 0.04278814038329674\n",
      "Epoch 795 - loss: 4.90264412322399e-08 - 0.042733896382091276\n",
      "Epoch 796 - loss: 4.505801788391875e-08 - 0.04272036925040228\n",
      "Epoch 801 - loss: 4.492811183871997e-08 - 0.04265293612744329\n",
      "Epoch 802 - loss: 4.232336614177037e-08 - 0.04263948991554231\n",
      "Epoch 805 - loss: 3.852190792187335e-08 - 0.04259923186306793\n",
      "Epoch 806 - loss: 3.494863839527562e-08 - 0.04258583932858815\n",
      "Epoch 808 - loss: 3.116830922380448e-08 - 0.042559094403770766\n",
      "Epoch 810 - loss: 2.907511823802882e-08 - 0.04253240291534881\n",
      "Epoch 813 - loss: 2.7534312703502795e-08 - 0.04249246564246504\n",
      "Epoch 815 - loss: 2.457457385607614e-08 - 0.04246590727839696\n",
      "Epoch 820 - loss: 2.3453028481871006e-08 - 0.042399743289818013\n",
      "Epoch 824 - loss: 2.103158875583932e-08 - 0.04234704977394848\n",
      "Epoch 826 - loss: 2.011680665830005e-08 - 0.04232078195097949\n",
      "Epoch 828 - loss: 1.9927577565869723e-08 - 0.042294566611155805\n",
      "Epoch 829 - loss: 1.9054355496266863e-08 - 0.04228147858964607\n",
      "Epoch 830 - loss: 1.564048375738685e-08 - 0.04226840364961602\n",
      "Epoch 836 - loss: 1.56395243385911e-08 - 0.04219022798922633\n",
      "Epoch 837 - loss: 1.555462164452766e-08 - 0.042177244254186605\n",
      "Epoch 839 - loss: 1.3951603745246124e-08 - 0.042151315702872394\n",
      "Epoch 846 - loss: 1.3614890905378942e-08 - 0.042060973061097316\n",
      "Epoch 847 - loss: 1.1782711407525945e-08 - 0.042048118516379795\n",
      "Epoch 849 - loss: 1.1657666417604e-08 - 0.04202244795846181\n",
      "Epoch 852 - loss: 1.0751414501059448e-08 - 0.041984038257944466\n",
      "Epoch 853 - loss: 1.0683315058236222e-08 - 0.041971260609575504\n",
      "Epoch 855 - loss: 8.780379742922463e-09 - 0.04194574361385774\n",
      "Epoch 861 - loss: 8.362354146669374e-09 - 0.04186949811740032\n",
      "Epoch 867 - loss: 7.273808608956952e-09 - 0.041793708724243435\n",
      "Epoch 869 - loss: 7.119469805779876e-09 - 0.041768546477429176\n",
      "Epoch 871 - loss: 6.776324015410464e-09 - 0.04174343450481758\n",
      "Epoch 874 - loss: 5.707752527467455e-09 - 0.041705860590355255\n",
      "Epoch 877 - loss: 5.680522479706577e-09 - 0.041668399228722654\n",
      "Epoch 878 - loss: 5.610894080964519e-09 - 0.041655937061616\n",
      "Epoch 879 - loss: 5.258776354093605e-09 - 0.04164348735044745\n",
      "Epoch 882 - loss: 4.52693492575739e-09 - 0.04160621282813546\n",
      "Epoch 884 - loss: 4.307728719312493e-09 - 0.04158142519837149\n",
      "Epoch 889 - loss: 3.422715012221017e-09 - 0.04151967258257037\n",
      "Epoch 890 - loss: 3.1683062620272092e-09 - 0.041507359067771324\n",
      "Epoch 892 - loss: 2.8306180199637614e-09 - 0.0414827689479523\n",
      "Epoch 895 - loss: 2.7046427376050785e-09 - 0.04144597585833768\n",
      "Epoch 896 - loss: 2.5565504632497894e-09 - 0.04143373600342679\n",
      "Epoch 898 - loss: 2.3126228502435256e-09 - 0.04140929298258845\n",
      "Epoch 903 - loss: 2.2131971571378264e-09 - 0.04134839887979515\n",
      "Epoch 904 - loss: 2.0505334137629236e-09 - 0.04133625655309057\n",
      "Epoch 905 - loss: 1.915842581629468e-09 - 0.04132412636264355\n",
      "Epoch 906 - loss: 1.894278753282091e-09 - 0.0413120082963239\n",
      "Epoch 909 - loss: 1.869713325388482e-09 - 0.041275726721009084\n",
      "Epoch 911 - loss: 1.8235862236629719e-09 - 0.04125159940292759\n",
      "Epoch 914 - loss: 1.6125413080087423e-09 - 0.04121549878272457\n",
      "Epoch 916 - loss: 1.4032004531116361e-09 - 0.041191491800144024\n",
      "Epoch 918 - loss: 1.321472708605266e-09 - 0.04116753278354667\n",
      "Epoch 921 - loss: 1.0657993302562824e-09 - 0.041131683985280026\n",
      "Epoch 923 - loss: 1.053353671356789e-09 - 0.04110784446477648\n",
      "Epoch 926 - loss: 9.811079681798718e-10 - 0.04107217446313711\n",
      "Epoch 928 - loss: 8.814788033997907e-10 - 0.0410484538427381\n",
      "Epoch 929 - loss: 8.048805499184496e-10 - 0.04103661131114803\n",
      "Epoch 931 - loss: 7.082862433989728e-10 - 0.0410129617459741\n",
      "Epoch 934 - loss: 6.922560742814682e-10 - 0.040977575965945555\n",
      "Epoch 935 - loss: 6.880714802633994e-10 - 0.04096580427680515\n",
      "Epoch 936 - loss: 6.744242654389931e-10 - 0.040954044353470005\n",
      "Epoch 937 - loss: 6.625434573273819e-10 - 0.040942296184180195\n",
      "Epoch 938 - loss: 6.020610751893125e-10 - 0.04093055975718755\n",
      "Epoch 939 - loss: 5.862664474528717e-10 - 0.04091883506075564\n",
      "Epoch 943 - loss: 5.439117999422657e-10 - 0.04087205334631709\n",
      "Epoch 944 - loss: 5.079311027052523e-10 - 0.040860387127052596\n",
      "Epoch 947 - loss: 5.070572906160924e-10 - 0.04082545838498109\n",
      "Epoch 948 - loss: 4.962918806554105e-10 - 0.04081383873738822\n",
      "Epoch 951 - loss: 4.848896988982796e-10 - 0.04077904943122727\n",
      "Epoch 952 - loss: 4.235664978053555e-10 - 0.040767476169391394\n",
      "Epoch 956 - loss: 4.086877490679663e-10 - 0.04072129868126006\n",
      "Epoch 957 - loss: 3.898877467021273e-10 - 0.0407097831413084\n",
      "Epoch 958 - loss: 3.789822945079347e-10 - 0.04069827911114085\n",
      "Epoch 959 - loss: 3.636646934343325e-10 - 0.04068678657925336\n",
      "Epoch 963 - loss: 3.408524489825655e-10 - 0.04064093120482569\n",
      "Epoch 970 - loss: 3.353099812320668e-10 - 0.0405611243363095\n",
      "Epoch 972 - loss: 3.006340716819783e-10 - 0.040538424794744954\n",
      "Epoch 974 - loss: 2.6893773240031355e-10 - 0.040515770606894715\n",
      "Epoch 978 - loss: 2.419215242479283e-10 - 0.04047059793005104\n",
      "Epoch 983 - loss: 2.4116054269220737e-10 - 0.040414385588364\n",
      "Epoch 984 - loss: 2.068229105267779e-10 - 0.04040317680809983\n",
      "Epoch 989 - loss: 1.842371952822521e-10 - 0.040347300730661374\n",
      "Epoch 990 - loss: 1.749465883163646e-10 - 0.04033615900172366\n",
      "Epoch 992 - loss: 1.487154522464662e-10 - 0.04031390894119742\n",
      "Epoch 997 - loss: 1.4293148623847972e-10 - 0.040258478089102566\n"
     ]
    }
   ],
   "source": [
    "from evolutionary_algos import EvoMLPClassifier\n",
    "\n",
    "classifier = EvoMLPClassifier(n = 48, hidden_layers = [10])\n",
    "classifier.fit(X, y, epochs = 1000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X)\n",
    "\n",
    "(y_pred > 0.5) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7.05389268\n",
      "Iteration 2, loss = 7.02946800\n",
      "Iteration 3, loss = 7.00504514\n",
      "Iteration 4, loss = 6.98072087\n",
      "Iteration 5, loss = 6.95640106\n",
      "Iteration 6, loss = 6.93208857\n",
      "Iteration 7, loss = 6.90778417\n",
      "Iteration 8, loss = 6.88348810\n",
      "Iteration 9, loss = 6.85924536\n",
      "Iteration 10, loss = 6.83378350\n",
      "Iteration 11, loss = 6.80751711\n",
      "Iteration 12, loss = 6.78128655\n",
      "Iteration 13, loss = 6.75498104\n",
      "Iteration 14, loss = 6.72872448\n",
      "Iteration 15, loss = 6.70239401\n",
      "Iteration 16, loss = 6.67599111\n",
      "Iteration 17, loss = 6.64951607\n",
      "Iteration 18, loss = 6.62296857\n",
      "Iteration 19, loss = 6.59636327\n",
      "Iteration 20, loss = 6.56973084\n",
      "Iteration 21, loss = 6.54130061\n",
      "Iteration 22, loss = 6.51271377\n",
      "Iteration 23, loss = 6.48399200\n",
      "Iteration 24, loss = 6.45541091\n",
      "Iteration 25, loss = 6.42681750\n",
      "Iteration 26, loss = 6.39787491\n",
      "Iteration 27, loss = 6.36882020\n",
      "Iteration 28, loss = 6.33966092\n",
      "Iteration 29, loss = 6.31016515\n",
      "Iteration 30, loss = 6.27913819\n",
      "Iteration 31, loss = 6.24794183\n",
      "Iteration 32, loss = 6.21660128\n",
      "Iteration 33, loss = 6.18513849\n",
      "Iteration 34, loss = 6.15355736\n",
      "Iteration 35, loss = 6.12169779\n",
      "Iteration 36, loss = 6.08975013\n",
      "Iteration 37, loss = 6.05719493\n",
      "Iteration 38, loss = 6.02330505\n",
      "Iteration 39, loss = 5.98920045\n",
      "Iteration 40, loss = 5.95466274\n",
      "Iteration 41, loss = 5.91958043\n",
      "Iteration 42, loss = 5.88432861\n",
      "Iteration 43, loss = 5.84893465\n",
      "Iteration 44, loss = 5.81312723\n",
      "Iteration 45, loss = 5.77707038\n",
      "Iteration 46, loss = 5.74092253\n",
      "Iteration 47, loss = 5.70457581\n",
      "Iteration 48, loss = 5.66639396\n",
      "Iteration 49, loss = 5.62821024\n",
      "Iteration 50, loss = 5.58986555\n",
      "Iteration 51, loss = 5.55136258\n",
      "Iteration 52, loss = 5.51280720\n",
      "Iteration 53, loss = 5.47396416\n",
      "Iteration 54, loss = 5.43468440\n",
      "Iteration 55, loss = 5.39552870\n",
      "Iteration 56, loss = 5.35657789\n",
      "Iteration 57, loss = 5.31759171\n",
      "Iteration 58, loss = 5.27856936\n",
      "Iteration 59, loss = 5.23960545\n",
      "Iteration 60, loss = 5.20093894\n",
      "Iteration 61, loss = 5.16247079\n",
      "Iteration 62, loss = 5.12446081\n",
      "Iteration 63, loss = 5.08660541\n",
      "Iteration 64, loss = 5.04865612\n",
      "Iteration 65, loss = 5.01092869\n",
      "Iteration 66, loss = 4.97344681\n",
      "Iteration 67, loss = 4.93623276\n",
      "Iteration 68, loss = 4.89930761\n",
      "Iteration 69, loss = 4.86269130\n",
      "Iteration 70, loss = 4.82640264\n",
      "Iteration 71, loss = 4.79045949\n",
      "Iteration 72, loss = 4.75469911\n",
      "Iteration 73, loss = 4.71911680\n",
      "Iteration 74, loss = 4.68392710\n",
      "Iteration 75, loss = 4.64872985\n",
      "Iteration 76, loss = 4.61370806\n",
      "Iteration 77, loss = 4.57894731\n",
      "Iteration 78, loss = 4.54440035\n",
      "Iteration 79, loss = 4.50988428\n",
      "Iteration 80, loss = 4.47573043\n",
      "Iteration 81, loss = 4.44197487\n",
      "Iteration 82, loss = 4.40864815\n",
      "Iteration 83, loss = 4.37577642\n",
      "Iteration 84, loss = 4.34338226\n",
      "Iteration 85, loss = 4.31148522\n",
      "Iteration 86, loss = 4.28007121\n",
      "Iteration 87, loss = 4.24858796\n",
      "Iteration 88, loss = 4.21766493\n",
      "Iteration 89, loss = 4.18729523\n",
      "Iteration 90, loss = 4.15749572\n",
      "Iteration 91, loss = 4.12791273\n",
      "Iteration 92, loss = 4.09828628\n",
      "Iteration 93, loss = 4.06921807\n",
      "Iteration 94, loss = 4.04073011\n",
      "Iteration 95, loss = 4.01283980\n",
      "Iteration 96, loss = 3.98556059\n",
      "Iteration 97, loss = 3.95890249\n",
      "Iteration 98, loss = 3.93287254\n",
      "Iteration 99, loss = 3.90747511\n",
      "Iteration 100, loss = 3.88271223\n",
      "Iteration 101, loss = 3.85858385\n",
      "Iteration 102, loss = 3.83508803\n",
      "Iteration 103, loss = 3.81222116\n",
      "Iteration 104, loss = 3.78997810\n",
      "Iteration 105, loss = 3.76835235\n",
      "Iteration 106, loss = 3.74728383\n",
      "Iteration 107, loss = 3.72580192\n",
      "Iteration 108, loss = 3.70484033\n",
      "Iteration 109, loss = 3.68441026\n",
      "Iteration 110, loss = 3.66451914\n",
      "Iteration 111, loss = 3.64521625\n",
      "Iteration 112, loss = 3.62647339\n",
      "Iteration 113, loss = 3.60827238\n",
      "Iteration 114, loss = 3.59060975\n",
      "Iteration 115, loss = 3.57347995\n",
      "Iteration 116, loss = 3.55687557\n",
      "Iteration 117, loss = 3.54068834\n",
      "Iteration 118, loss = 3.52475653\n",
      "Iteration 119, loss = 3.50931938\n",
      "Iteration 120, loss = 3.49436760\n",
      "Iteration 121, loss = 3.47985692\n",
      "Iteration 122, loss = 3.46578112\n",
      "Iteration 123, loss = 3.45213806\n",
      "Iteration 124, loss = 3.43893490\n",
      "Iteration 125, loss = 3.42615639\n",
      "Iteration 126, loss = 3.41379060\n",
      "Iteration 127, loss = 3.40182495\n",
      "Iteration 128, loss = 3.39024639\n",
      "Iteration 129, loss = 3.37904153\n",
      "Iteration 130, loss = 3.36819677\n",
      "Iteration 131, loss = 3.35769842\n",
      "Iteration 132, loss = 3.34753277\n",
      "Iteration 133, loss = 3.33768622\n",
      "Iteration 134, loss = 3.32814534\n",
      "Iteration 135, loss = 3.31889691\n",
      "Iteration 136, loss = 3.30992805\n",
      "Iteration 137, loss = 3.30122619\n",
      "Iteration 138, loss = 3.29277914\n",
      "Iteration 139, loss = 3.28457513\n",
      "Iteration 140, loss = 3.27660284\n",
      "Iteration 141, loss = 3.26888173\n",
      "Iteration 142, loss = 3.26135195\n",
      "Iteration 143, loss = 3.25399809\n",
      "Iteration 144, loss = 3.24685738\n",
      "Iteration 145, loss = 3.23990674\n",
      "Iteration 146, loss = 3.23312814\n",
      "Iteration 147, loss = 3.22651347\n",
      "Iteration 148, loss = 3.22005501\n",
      "Iteration 149, loss = 3.21374544\n",
      "Iteration 150, loss = 3.20757779\n",
      "Iteration 151, loss = 3.20154545\n",
      "Iteration 152, loss = 3.19564216\n",
      "Iteration 153, loss = 3.18986196\n",
      "Iteration 154, loss = 3.18419922\n",
      "Iteration 155, loss = 3.17863402\n",
      "Iteration 156, loss = 3.17316643\n",
      "Iteration 157, loss = 3.16779374\n",
      "Iteration 158, loss = 3.16249485\n",
      "Iteration 159, loss = 3.15731416\n",
      "Iteration 160, loss = 3.15222240\n",
      "Iteration 161, loss = 3.14721629\n",
      "Iteration 162, loss = 3.14229263\n",
      "Iteration 163, loss = 3.13746878\n",
      "Iteration 164, loss = 3.13269796\n",
      "Iteration 165, loss = 3.12801412\n",
      "Iteration 166, loss = 3.12340726\n",
      "Iteration 167, loss = 3.11886677\n",
      "Iteration 168, loss = 3.11439013\n",
      "Iteration 169, loss = 3.10997485\n",
      "Iteration 170, loss = 3.10561856\n",
      "Iteration 171, loss = 3.10131895\n",
      "Iteration 172, loss = 3.09707378\n",
      "Iteration 173, loss = 3.09291674\n",
      "Iteration 174, loss = 3.08878890\n",
      "Iteration 175, loss = 3.08468604\n",
      "Iteration 176, loss = 3.08065895\n",
      "Iteration 177, loss = 3.07668353\n",
      "Iteration 178, loss = 3.07274833\n",
      "Iteration 179, loss = 3.06885182\n",
      "Iteration 180, loss = 3.06499252\n",
      "Iteration 181, loss = 3.06116902\n",
      "Iteration 182, loss = 3.05737990\n",
      "Iteration 183, loss = 3.05362385\n",
      "Iteration 184, loss = 3.04989954\n",
      "Iteration 185, loss = 3.04624132\n",
      "Iteration 186, loss = 3.04260237\n",
      "Iteration 187, loss = 3.03896843\n",
      "Iteration 188, loss = 3.03536881\n",
      "Iteration 189, loss = 3.03181806\n",
      "Iteration 190, loss = 3.02828931\n",
      "Iteration 191, loss = 3.02478181\n",
      "Iteration 192, loss = 3.02129479\n",
      "Iteration 193, loss = 3.01782750\n",
      "Iteration 194, loss = 3.01437921\n",
      "Iteration 195, loss = 3.01098503\n",
      "Iteration 196, loss = 3.00759534\n",
      "Iteration 197, loss = 3.00420139\n",
      "Iteration 198, loss = 3.00084477\n",
      "Iteration 199, loss = 2.99751801\n",
      "Iteration 200, loss = 2.99420302\n",
      "Iteration 201, loss = 2.99089946\n",
      "Iteration 202, loss = 2.98760700\n",
      "Iteration 203, loss = 2.98426138\n",
      "Iteration 204, loss = 2.98084271\n",
      "Iteration 205, loss = 2.97746105\n",
      "Iteration 206, loss = 2.97407175\n",
      "Iteration 207, loss = 2.97067845\n",
      "Iteration 208, loss = 2.96728367\n",
      "Iteration 209, loss = 2.96388926\n",
      "Iteration 210, loss = 2.96049665\n",
      "Iteration 211, loss = 2.95710690\n",
      "Iteration 212, loss = 2.95372086\n",
      "Iteration 213, loss = 2.95033914\n",
      "Iteration 214, loss = 2.94696219\n",
      "Iteration 215, loss = 2.94359034\n",
      "Iteration 216, loss = 2.94022379\n",
      "Iteration 217, loss = 2.93686265\n",
      "Iteration 218, loss = 2.93350695\n",
      "Iteration 219, loss = 2.93015664\n",
      "Iteration 220, loss = 2.92681165\n",
      "Iteration 221, loss = 2.92347182\n",
      "Iteration 222, loss = 2.92013697\n",
      "Iteration 223, loss = 2.91680690\n",
      "Iteration 224, loss = 2.91348136\n",
      "Iteration 225, loss = 2.91016010\n",
      "Iteration 226, loss = 2.90684284\n",
      "Iteration 227, loss = 2.90352929\n",
      "Iteration 228, loss = 2.90021914\n",
      "Iteration 229, loss = 2.89691209\n",
      "Iteration 230, loss = 2.89360783\n",
      "Iteration 231, loss = 2.89030604\n",
      "Iteration 232, loss = 2.88700639\n",
      "Iteration 233, loss = 2.88370858\n",
      "Iteration 234, loss = 2.88041227\n",
      "Iteration 235, loss = 2.87700961\n",
      "Iteration 236, loss = 2.87343075\n",
      "Iteration 237, loss = 2.86979127\n",
      "Iteration 238, loss = 2.86610039\n",
      "Iteration 239, loss = 2.86264837\n",
      "Iteration 240, loss = 2.85918528\n",
      "Iteration 241, loss = 2.85571305\n",
      "Iteration 242, loss = 2.85223335\n",
      "Iteration 243, loss = 2.84874754\n",
      "Iteration 244, loss = 2.84525670\n",
      "Iteration 245, loss = 2.84176172\n",
      "Iteration 246, loss = 2.83826330\n",
      "Iteration 247, loss = 2.83482998\n",
      "Iteration 248, loss = 2.83140974\n",
      "Iteration 249, loss = 2.82799110\n",
      "Iteration 250, loss = 2.82457399\n",
      "Iteration 251, loss = 2.82115838\n",
      "Iteration 252, loss = 2.81774427\n",
      "Iteration 253, loss = 2.81433168\n",
      "Iteration 254, loss = 2.81092066\n",
      "Iteration 255, loss = 2.80751126\n",
      "Iteration 256, loss = 2.80410354\n",
      "Iteration 257, loss = 2.80069755\n",
      "Iteration 258, loss = 2.79729333\n",
      "Iteration 259, loss = 2.79389093\n",
      "Iteration 260, loss = 2.79049036\n",
      "Iteration 261, loss = 2.78709162\n",
      "Iteration 262, loss = 2.78369469\n",
      "Iteration 263, loss = 2.78029953\n",
      "Iteration 264, loss = 2.77705409\n",
      "Iteration 265, loss = 2.77379709\n",
      "Iteration 266, loss = 2.77049657\n",
      "Iteration 267, loss = 2.76715995\n",
      "Iteration 268, loss = 2.76379418\n",
      "Iteration 269, loss = 2.76040566\n",
      "Iteration 270, loss = 2.75700921\n",
      "Iteration 271, loss = 2.75375915\n",
      "Iteration 272, loss = 2.75052240\n",
      "Iteration 273, loss = 2.74727977\n",
      "Iteration 274, loss = 2.74403052\n",
      "Iteration 275, loss = 2.74077417\n",
      "Iteration 276, loss = 2.73751048\n",
      "Iteration 277, loss = 2.73423941\n",
      "Iteration 278, loss = 2.73096110\n",
      "Iteration 279, loss = 2.72767581\n",
      "Iteration 280, loss = 2.72438393\n",
      "Iteration 281, loss = 2.72108590\n",
      "Iteration 282, loss = 2.71787275\n",
      "Iteration 283, loss = 2.71468172\n",
      "Iteration 284, loss = 2.71146393\n",
      "Iteration 285, loss = 2.70822327\n",
      "Iteration 286, loss = 2.70496346\n",
      "Iteration 287, loss = 2.70168802\n",
      "Iteration 288, loss = 2.69846742\n",
      "Iteration 289, loss = 2.69528387\n",
      "Iteration 290, loss = 2.69208280\n",
      "Iteration 291, loss = 2.68886512\n",
      "Iteration 292, loss = 2.68563189\n",
      "Iteration 293, loss = 2.68238430\n",
      "Iteration 294, loss = 2.67918481\n",
      "Iteration 295, loss = 2.67600758\n",
      "Iteration 296, loss = 2.67280945\n",
      "Iteration 297, loss = 2.66959333\n",
      "Iteration 298, loss = 2.66636206\n",
      "Iteration 299, loss = 2.66318316\n",
      "Iteration 300, loss = 2.66001418\n",
      "Iteration 301, loss = 2.65682290\n",
      "Iteration 302, loss = 2.65361095\n",
      "Iteration 303, loss = 2.65038508\n",
      "Iteration 304, loss = 2.64721312\n",
      "Iteration 305, loss = 2.64402369\n",
      "Iteration 306, loss = 2.64083457\n",
      "Iteration 307, loss = 2.63765057\n",
      "Iteration 308, loss = 2.63447939\n",
      "Iteration 309, loss = 2.63130259\n",
      "Iteration 310, loss = 2.62811063\n",
      "Iteration 311, loss = 2.62496280\n",
      "Iteration 312, loss = 2.62179568\n",
      "Iteration 313, loss = 2.61860245\n",
      "Iteration 314, loss = 2.61543008\n",
      "Iteration 315, loss = 2.61227419\n",
      "Iteration 316, loss = 2.60910244\n",
      "Iteration 317, loss = 2.60591702\n",
      "Iteration 318, loss = 2.60275859\n",
      "Iteration 319, loss = 2.59960241\n",
      "Iteration 320, loss = 2.59641693\n",
      "Iteration 321, loss = 2.59326905\n",
      "Iteration 322, loss = 2.59012260\n",
      "Iteration 323, loss = 2.58696115\n",
      "Iteration 324, loss = 2.58378674\n",
      "Iteration 325, loss = 2.58060137\n",
      "Iteration 326, loss = 2.57750085\n",
      "Iteration 327, loss = 2.57436437\n",
      "Iteration 328, loss = 2.57119386\n",
      "Iteration 329, loss = 2.56799239\n",
      "Iteration 330, loss = 2.56486163\n",
      "Iteration 331, loss = 2.56173845\n",
      "Iteration 332, loss = 2.55859987\n",
      "Iteration 333, loss = 2.55544769\n",
      "Iteration 334, loss = 2.55228375\n",
      "Iteration 335, loss = 2.54910991\n",
      "Iteration 336, loss = 2.54592796\n",
      "Iteration 337, loss = 2.54285290\n",
      "Iteration 338, loss = 2.53972324\n",
      "Iteration 339, loss = 2.53651926\n",
      "Iteration 340, loss = 2.53337682\n",
      "Iteration 341, loss = 2.53026441\n",
      "Iteration 342, loss = 2.52713792\n",
      "Iteration 343, loss = 2.52399889\n",
      "Iteration 344, loss = 2.52084892\n",
      "Iteration 345, loss = 2.51768966\n",
      "Iteration 346, loss = 2.51452271\n",
      "Iteration 347, loss = 2.51141928\n",
      "Iteration 348, loss = 2.50830805\n",
      "Iteration 349, loss = 2.50515439\n",
      "Iteration 350, loss = 2.50197346\n",
      "Iteration 351, loss = 2.49885694\n",
      "Iteration 352, loss = 2.49572928\n",
      "Iteration 353, loss = 2.49259187\n",
      "Iteration 354, loss = 2.48944614\n",
      "Iteration 355, loss = 2.48636716\n",
      "Iteration 356, loss = 2.48324232\n",
      "Iteration 357, loss = 2.48007450\n",
      "Iteration 358, loss = 2.47697182\n",
      "Iteration 359, loss = 2.47387248\n",
      "Iteration 360, loss = 2.47076158\n",
      "Iteration 361, loss = 2.46764039\n",
      "Iteration 362, loss = 2.46451029\n",
      "Iteration 363, loss = 2.46137265\n",
      "Iteration 364, loss = 2.45822886\n",
      "Iteration 365, loss = 2.45512454\n",
      "Iteration 366, loss = 2.45202592\n",
      "Iteration 367, loss = 2.44887934\n",
      "Iteration 368, loss = 2.44577543\n",
      "Iteration 369, loss = 2.44268397\n",
      "Iteration 370, loss = 2.43958245\n",
      "Iteration 371, loss = 2.43647415\n",
      "Iteration 372, loss = 2.43339135\n",
      "Iteration 373, loss = 2.43030937\n",
      "Iteration 374, loss = 2.42722825\n",
      "Iteration 375, loss = 2.42414802\n",
      "Iteration 376, loss = 2.42106876\n",
      "Iteration 377, loss = 2.41799054\n",
      "Iteration 378, loss = 2.41491342\n",
      "Iteration 379, loss = 2.41183748\n",
      "Iteration 380, loss = 2.40876280\n",
      "Iteration 381, loss = 2.40569285\n",
      "Iteration 382, loss = 2.40262370\n",
      "Iteration 383, loss = 2.39955952\n",
      "Iteration 384, loss = 2.39649794\n",
      "Iteration 385, loss = 2.39343709\n",
      "Iteration 386, loss = 2.39037706\n",
      "Iteration 387, loss = 2.38731791\n",
      "Iteration 388, loss = 2.38425976\n",
      "Iteration 389, loss = 2.38120269\n",
      "Iteration 390, loss = 2.37814680\n",
      "Iteration 391, loss = 2.37510660\n",
      "Iteration 392, loss = 2.37205549\n",
      "Iteration 393, loss = 2.36900105\n",
      "Iteration 394, loss = 2.36595905\n",
      "Iteration 395, loss = 2.36291777\n",
      "Iteration 396, loss = 2.35987729\n",
      "Iteration 397, loss = 2.35683773\n",
      "Iteration 398, loss = 2.35379919\n",
      "Iteration 399, loss = 2.35076179\n",
      "Iteration 400, loss = 2.34772974\n",
      "Iteration 401, loss = 2.34469615\n",
      "Iteration 402, loss = 2.34166759\n",
      "Iteration 403, loss = 2.33864455\n",
      "Iteration 404, loss = 2.33561924\n",
      "Iteration 405, loss = 2.33259918\n",
      "Iteration 406, loss = 2.32958001\n",
      "Iteration 407, loss = 2.32656187\n",
      "Iteration 408, loss = 2.32354489\n",
      "Iteration 409, loss = 2.32053360\n",
      "Iteration 410, loss = 2.31752067\n",
      "Iteration 411, loss = 2.31451311\n",
      "Iteration 412, loss = 2.31150667\n",
      "Iteration 413, loss = 2.30850727\n",
      "Iteration 414, loss = 2.30550360\n",
      "Iteration 415, loss = 2.30250667\n",
      "Iteration 416, loss = 2.29951083\n",
      "Iteration 417, loss = 2.29651623\n",
      "Iteration 418, loss = 2.29352694\n",
      "Iteration 419, loss = 2.29053747\n",
      "Iteration 420, loss = 2.28755298\n",
      "Iteration 421, loss = 2.28456971\n",
      "Iteration 422, loss = 2.28158783\n",
      "Iteration 423, loss = 2.27861765\n",
      "Iteration 424, loss = 2.27563511\n",
      "Iteration 425, loss = 2.27266392\n",
      "Iteration 426, loss = 2.26969410\n",
      "Iteration 427, loss = 2.26673278\n",
      "Iteration 428, loss = 2.26376561\n",
      "Iteration 429, loss = 2.26080660\n",
      "Iteration 430, loss = 2.25784896\n",
      "Iteration 431, loss = 2.25489285\n",
      "Iteration 432, loss = 2.25193920\n",
      "Iteration 433, loss = 2.24899245\n",
      "Iteration 434, loss = 2.24604781\n",
      "Iteration 435, loss = 2.24310469\n",
      "Iteration 436, loss = 2.24016329\n",
      "Iteration 437, loss = 2.23722731\n",
      "Iteration 438, loss = 2.23429293\n",
      "Iteration 439, loss = 2.23136360\n",
      "Iteration 440, loss = 2.22843596\n",
      "Iteration 441, loss = 2.22551021\n",
      "Iteration 442, loss = 2.22259653\n",
      "Iteration 443, loss = 2.21967171\n",
      "Iteration 444, loss = 2.21675856\n",
      "Iteration 445, loss = 2.21384728\n",
      "Iteration 446, loss = 2.21094052\n",
      "Iteration 447, loss = 2.20803771\n",
      "Iteration 448, loss = 2.20513906\n",
      "Iteration 449, loss = 2.20224225\n",
      "Iteration 450, loss = 2.19934746\n",
      "Iteration 451, loss = 2.19645490\n",
      "Iteration 452, loss = 2.19356743\n",
      "Iteration 453, loss = 2.19068387\n",
      "Iteration 454, loss = 2.18780497\n",
      "Iteration 455, loss = 2.18492825\n",
      "Iteration 456, loss = 2.18205389\n",
      "Iteration 457, loss = 2.17919377\n",
      "Iteration 458, loss = 2.17631961\n",
      "Iteration 459, loss = 2.17345926\n",
      "Iteration 460, loss = 2.17060121\n",
      "Iteration 461, loss = 2.16774565\n",
      "Iteration 462, loss = 2.16490387\n",
      "Iteration 463, loss = 2.16204927\n",
      "Iteration 464, loss = 2.15920804\n",
      "Iteration 465, loss = 2.15636922\n",
      "Iteration 466, loss = 2.15353298\n",
      "Iteration 467, loss = 2.15070099\n",
      "Iteration 468, loss = 2.14787555\n",
      "Iteration 469, loss = 2.14505394\n",
      "Iteration 470, loss = 2.14223483\n",
      "Iteration 471, loss = 2.13941839\n",
      "Iteration 472, loss = 2.13660480\n",
      "Iteration 473, loss = 2.13379426\n",
      "Iteration 474, loss = 2.13100573\n",
      "Iteration 475, loss = 2.12819651\n",
      "Iteration 476, loss = 2.12540102\n",
      "Iteration 477, loss = 2.12261499\n",
      "Iteration 478, loss = 2.11983132\n",
      "Iteration 479, loss = 2.11705011\n",
      "Iteration 480, loss = 2.11427152\n",
      "Iteration 481, loss = 2.11149574\n",
      "Iteration 482, loss = 2.10872294\n",
      "Iteration 483, loss = 2.10595332\n",
      "Iteration 484, loss = 2.10318709\n",
      "Iteration 485, loss = 2.10042443\n",
      "Iteration 486, loss = 2.09766553\n",
      "Iteration 487, loss = 2.09493949\n",
      "Iteration 488, loss = 2.09219029\n",
      "Iteration 489, loss = 2.08942952\n",
      "Iteration 490, loss = 2.08669650\n",
      "Iteration 491, loss = 2.08396639\n",
      "Iteration 492, loss = 2.08123928\n",
      "Iteration 493, loss = 2.07851531\n",
      "Iteration 494, loss = 2.07579461\n",
      "Iteration 495, loss = 2.07307734\n",
      "Iteration 496, loss = 2.07036366\n",
      "Iteration 497, loss = 2.06765373\n",
      "Iteration 498, loss = 2.06494770\n",
      "Iteration 499, loss = 2.06226998\n",
      "Iteration 500, loss = 2.05956683\n",
      "Iteration 501, loss = 2.05687000\n",
      "Iteration 502, loss = 2.05418983\n",
      "Iteration 503, loss = 2.05151276\n",
      "Iteration 504, loss = 2.04883886\n",
      "Iteration 505, loss = 2.04616822\n",
      "Iteration 506, loss = 2.04350095\n",
      "Iteration 507, loss = 2.04083719\n",
      "Iteration 508, loss = 2.03817707\n",
      "Iteration 509, loss = 2.03552076\n",
      "Iteration 510, loss = 2.03286839\n",
      "Iteration 511, loss = 2.03022009\n",
      "Iteration 512, loss = 2.02757600\n",
      "Iteration 513, loss = 2.02493762\n",
      "Iteration 514, loss = 2.02230573\n",
      "Iteration 515, loss = 2.01967898\n",
      "Iteration 516, loss = 2.01705607\n",
      "Iteration 517, loss = 2.01445335\n",
      "Iteration 518, loss = 2.01185283\n",
      "Iteration 519, loss = 2.00926229\n",
      "Iteration 520, loss = 2.00665826\n",
      "Iteration 521, loss = 2.00406456\n",
      "Iteration 522, loss = 2.00147212\n",
      "Iteration 523, loss = 1.99888136\n",
      "Iteration 524, loss = 1.99629269\n",
      "Iteration 525, loss = 1.99370693\n",
      "Iteration 526, loss = 1.99113780\n",
      "Iteration 527, loss = 1.98857171\n",
      "Iteration 528, loss = 1.98600871\n",
      "Iteration 529, loss = 1.98344885\n",
      "Iteration 530, loss = 1.98090176\n",
      "Iteration 531, loss = 1.97834262\n",
      "Iteration 532, loss = 1.97579598\n",
      "Iteration 533, loss = 1.97325245\n",
      "Iteration 534, loss = 1.97071254\n",
      "Iteration 535, loss = 1.96818200\n",
      "Iteration 536, loss = 1.96565284\n",
      "Iteration 537, loss = 1.96312534\n",
      "Iteration 538, loss = 1.96060598\n",
      "Iteration 539, loss = 1.95809036\n",
      "Iteration 540, loss = 1.95557778\n",
      "Iteration 541, loss = 1.95306840\n",
      "Iteration 542, loss = 1.95057306\n",
      "Iteration 543, loss = 1.94806299\n",
      "Iteration 544, loss = 1.94557227\n",
      "Iteration 545, loss = 1.94308424\n",
      "Iteration 546, loss = 1.94059740\n",
      "Iteration 547, loss = 1.93811198\n",
      "Iteration 548, loss = 1.93563640\n",
      "Iteration 549, loss = 1.93316436\n",
      "Iteration 550, loss = 1.93069531\n",
      "Iteration 551, loss = 1.92822941\n",
      "Iteration 552, loss = 1.92576680\n",
      "Iteration 553, loss = 1.92330764\n",
      "Iteration 554, loss = 1.92085208\n",
      "Iteration 555, loss = 1.91840024\n",
      "Iteration 556, loss = 1.91596003\n",
      "Iteration 557, loss = 1.91353246\n",
      "Iteration 558, loss = 1.91108548\n",
      "Iteration 559, loss = 1.90865098\n",
      "Iteration 560, loss = 1.90622327\n",
      "Iteration 561, loss = 1.90380239\n",
      "Iteration 562, loss = 1.90138462\n",
      "Iteration 563, loss = 1.89897007\n",
      "Iteration 564, loss = 1.89655885\n",
      "Iteration 565, loss = 1.89415106\n",
      "Iteration 566, loss = 1.89174683\n",
      "Iteration 567, loss = 1.88934625\n",
      "Iteration 568, loss = 1.88694943\n",
      "Iteration 569, loss = 1.88456335\n",
      "Iteration 570, loss = 1.88217785\n",
      "Iteration 571, loss = 1.87979285\n",
      "Iteration 572, loss = 1.87740981\n",
      "Iteration 573, loss = 1.87503632\n",
      "Iteration 574, loss = 1.87266640\n",
      "Iteration 575, loss = 1.87030011\n",
      "Iteration 576, loss = 1.86793752\n",
      "Iteration 577, loss = 1.86558417\n",
      "Iteration 578, loss = 1.86322492\n",
      "Iteration 579, loss = 1.86087615\n",
      "Iteration 580, loss = 1.85853068\n",
      "Iteration 581, loss = 1.85619028\n",
      "Iteration 582, loss = 1.85385386\n",
      "Iteration 583, loss = 1.85152092\n",
      "Iteration 584, loss = 1.84919151\n",
      "Iteration 585, loss = 1.84686567\n",
      "Iteration 586, loss = 1.84454347\n",
      "Iteration 587, loss = 1.84222495\n",
      "Iteration 588, loss = 1.83991062\n",
      "Iteration 589, loss = 1.83760032\n",
      "Iteration 590, loss = 1.83529411\n",
      "Iteration 591, loss = 1.83299205\n",
      "Iteration 592, loss = 1.83069392\n",
      "Iteration 593, loss = 1.82839988\n",
      "Iteration 594, loss = 1.82610950\n",
      "Iteration 595, loss = 1.82382282\n",
      "Iteration 596, loss = 1.82153989\n",
      "Iteration 597, loss = 1.81926073\n",
      "Iteration 598, loss = 1.81698537\n",
      "Iteration 599, loss = 1.81471617\n",
      "Iteration 600, loss = 1.81244734\n",
      "Iteration 601, loss = 1.81018482\n",
      "Iteration 602, loss = 1.80792621\n",
      "Iteration 603, loss = 1.80567121\n",
      "Iteration 604, loss = 1.80341984\n",
      "Iteration 605, loss = 1.80117210\n",
      "Iteration 606, loss = 1.79892802\n",
      "Iteration 607, loss = 1.79668761\n",
      "Iteration 608, loss = 1.79445088\n",
      "Iteration 609, loss = 1.79221786\n",
      "Iteration 610, loss = 1.78998856\n",
      "Iteration 611, loss = 1.78776299\n",
      "Iteration 612, loss = 1.78554117\n",
      "Iteration 613, loss = 1.78332309\n",
      "Iteration 614, loss = 1.78110876\n",
      "Iteration 615, loss = 1.77890126\n",
      "Iteration 616, loss = 1.77669293\n",
      "Iteration 617, loss = 1.77448948\n",
      "Iteration 618, loss = 1.77229071\n",
      "Iteration 619, loss = 1.77009551\n",
      "Iteration 620, loss = 1.76790385\n",
      "Iteration 621, loss = 1.76571575\n",
      "Iteration 622, loss = 1.76353121\n",
      "Iteration 623, loss = 1.76135023\n",
      "Iteration 624, loss = 1.75917281\n",
      "Iteration 625, loss = 1.75699895\n",
      "Iteration 626, loss = 1.75482865\n",
      "Iteration 627, loss = 1.75266191\n",
      "Iteration 628, loss = 1.75049873\n",
      "Iteration 629, loss = 1.74833912\n",
      "Iteration 630, loss = 1.74618305\n",
      "Iteration 631, loss = 1.74403053\n",
      "Iteration 632, loss = 1.74188155\n",
      "Iteration 633, loss = 1.73973611\n",
      "Iteration 634, loss = 1.73759418\n",
      "Iteration 635, loss = 1.73545577\n",
      "Iteration 636, loss = 1.73332086\n",
      "Iteration 637, loss = 1.73118943\n",
      "Iteration 638, loss = 1.72906149\n",
      "Iteration 639, loss = 1.72693700\n",
      "Iteration 640, loss = 1.72481597\n",
      "Iteration 641, loss = 1.72269838\n",
      "Iteration 642, loss = 1.72058421\n",
      "Iteration 643, loss = 1.71847345\n",
      "Iteration 644, loss = 1.71636609\n",
      "Iteration 645, loss = 1.71426212\n",
      "Iteration 646, loss = 1.71216152\n",
      "Iteration 647, loss = 1.71006428\n",
      "Iteration 648, loss = 1.70797039\n",
      "Iteration 649, loss = 1.70587983\n",
      "Iteration 650, loss = 1.70379260\n",
      "Iteration 651, loss = 1.70170868\n",
      "Iteration 652, loss = 1.69962805\n",
      "Iteration 653, loss = 1.69755071\n",
      "Iteration 654, loss = 1.69547665\n",
      "Iteration 655, loss = 1.69340585\n",
      "Iteration 656, loss = 1.69133829\n",
      "Iteration 657, loss = 1.68927397\n",
      "Iteration 658, loss = 1.68721288\n",
      "Iteration 659, loss = 1.68515499\n",
      "Iteration 660, loss = 1.68310031\n",
      "Iteration 661, loss = 1.68104882\n",
      "Iteration 662, loss = 1.67900050\n",
      "Iteration 663, loss = 1.67695534\n",
      "Iteration 664, loss = 1.67491334\n",
      "Iteration 665, loss = 1.67287447\n",
      "Iteration 666, loss = 1.67083874\n",
      "Iteration 667, loss = 1.66880611\n",
      "Iteration 668, loss = 1.66677660\n",
      "Iteration 669, loss = 1.66475017\n",
      "Iteration 670, loss = 1.66272682\n",
      "Iteration 671, loss = 1.66070655\n",
      "Iteration 672, loss = 1.65868933\n",
      "Iteration 673, loss = 1.65667516\n",
      "Iteration 674, loss = 1.65466402\n",
      "Iteration 675, loss = 1.65265590\n",
      "Iteration 676, loss = 1.65065080\n",
      "Iteration 677, loss = 1.64864870\n",
      "Iteration 678, loss = 1.64664959\n",
      "Iteration 679, loss = 1.64465345\n",
      "Iteration 680, loss = 1.64266029\n",
      "Iteration 681, loss = 1.64067008\n",
      "Iteration 682, loss = 1.63868282\n",
      "Iteration 683, loss = 1.63669849\n",
      "Iteration 684, loss = 1.63471708\n",
      "Iteration 685, loss = 1.63273860\n",
      "Iteration 686, loss = 1.63076301\n",
      "Iteration 687, loss = 1.62879031\n",
      "Iteration 688, loss = 1.62682050\n",
      "Iteration 689, loss = 1.62485356\n",
      "Iteration 690, loss = 1.62288948\n",
      "Iteration 691, loss = 1.62092825\n",
      "Iteration 692, loss = 1.61896986\n",
      "Iteration 693, loss = 1.61701430\n",
      "Iteration 694, loss = 1.61506156\n",
      "Iteration 695, loss = 1.61311163\n",
      "Iteration 696, loss = 1.61116449\n",
      "Iteration 697, loss = 1.60922015\n",
      "Iteration 698, loss = 1.60727858\n",
      "Iteration 699, loss = 1.60533979\n",
      "Iteration 700, loss = 1.60340375\n",
      "Iteration 701, loss = 1.60147046\n",
      "Iteration 702, loss = 1.59953991\n",
      "Iteration 703, loss = 1.59761209\n",
      "Iteration 704, loss = 1.59568699\n",
      "Iteration 705, loss = 1.59376489\n",
      "Iteration 706, loss = 1.59184807\n",
      "Iteration 707, loss = 1.58993424\n",
      "Iteration 708, loss = 1.58802301\n",
      "Iteration 709, loss = 1.58611430\n",
      "Iteration 710, loss = 1.58420805\n",
      "Iteration 711, loss = 1.58230419\n",
      "Iteration 712, loss = 1.58040270\n",
      "Iteration 713, loss = 1.57851289\n",
      "Iteration 714, loss = 1.57661182\n",
      "Iteration 715, loss = 1.57472224\n",
      "Iteration 716, loss = 1.57283469\n",
      "Iteration 717, loss = 1.57094907\n",
      "Iteration 718, loss = 1.56906532\n",
      "Iteration 719, loss = 1.56718343\n",
      "Iteration 720, loss = 1.56530342\n",
      "Iteration 721, loss = 1.56343020\n",
      "Iteration 722, loss = 1.56155574\n",
      "Iteration 723, loss = 1.55968765\n",
      "Iteration 724, loss = 1.55782103\n",
      "Iteration 725, loss = 1.55595589\n",
      "Iteration 726, loss = 1.55409227\n",
      "Iteration 727, loss = 1.55224074\n",
      "Iteration 728, loss = 1.55037714\n",
      "Iteration 729, loss = 1.54852516\n",
      "Iteration 730, loss = 1.54667429\n",
      "Iteration 731, loss = 1.54482458\n",
      "Iteration 732, loss = 1.54297908\n",
      "Iteration 733, loss = 1.54113689\n",
      "Iteration 734, loss = 1.53929842\n",
      "Iteration 735, loss = 1.53746071\n",
      "Iteration 736, loss = 1.53562383\n",
      "Iteration 737, loss = 1.53378791\n",
      "Iteration 738, loss = 1.53196587\n",
      "Iteration 739, loss = 1.53012790\n",
      "Iteration 740, loss = 1.52830325\n",
      "Iteration 741, loss = 1.52648050\n",
      "Iteration 742, loss = 1.52466475\n",
      "Iteration 743, loss = 1.52285041\n",
      "Iteration 744, loss = 1.52103619\n",
      "Iteration 745, loss = 1.51922221\n",
      "Iteration 746, loss = 1.51740865\n",
      "Iteration 747, loss = 1.51559572\n",
      "Iteration 748, loss = 1.51380642\n",
      "Iteration 749, loss = 1.51199898\n",
      "Iteration 750, loss = 1.51018933\n",
      "Iteration 751, loss = 1.50839657\n",
      "Iteration 752, loss = 1.50660340\n",
      "Iteration 753, loss = 1.50480993\n",
      "Iteration 754, loss = 1.50301633\n",
      "Iteration 755, loss = 1.50122288\n",
      "Iteration 756, loss = 1.49943481\n",
      "Iteration 757, loss = 1.49764725\n",
      "Iteration 758, loss = 1.49586847\n",
      "Iteration 759, loss = 1.49409152\n",
      "Iteration 760, loss = 1.49231788\n",
      "Iteration 761, loss = 1.49054360\n",
      "Iteration 762, loss = 1.48876887\n",
      "Iteration 763, loss = 1.48699399\n",
      "Iteration 764, loss = 1.48523889\n",
      "Iteration 765, loss = 1.48346987\n",
      "Iteration 766, loss = 1.48170058\n",
      "Iteration 767, loss = 1.47994497\n",
      "Iteration 768, loss = 1.47818822\n",
      "Iteration 769, loss = 1.47643053\n",
      "Iteration 770, loss = 1.47467218\n",
      "Iteration 771, loss = 1.47291449\n",
      "Iteration 772, loss = 1.47116549\n",
      "Iteration 773, loss = 1.46941703\n",
      "Iteration 774, loss = 1.46767775\n",
      "Iteration 775, loss = 1.46593751\n",
      "Iteration 776, loss = 1.46419598\n",
      "Iteration 777, loss = 1.46245345\n",
      "Iteration 778, loss = 1.46071785\n",
      "Iteration 779, loss = 1.45897916\n",
      "Iteration 780, loss = 1.45725526\n",
      "Iteration 781, loss = 1.45553074\n",
      "Iteration 782, loss = 1.45380435\n",
      "Iteration 783, loss = 1.45207633\n",
      "Iteration 784, loss = 1.45034707\n",
      "Iteration 785, loss = 1.44861904\n",
      "Iteration 786, loss = 1.44689931\n",
      "Iteration 787, loss = 1.44518892\n",
      "Iteration 788, loss = 1.44347770\n",
      "Iteration 789, loss = 1.44176447\n",
      "Iteration 790, loss = 1.44004955\n",
      "Iteration 791, loss = 1.43833341\n",
      "Iteration 792, loss = 1.43664853\n",
      "Iteration 793, loss = 1.43494629\n",
      "Iteration 794, loss = 1.43322858\n",
      "Iteration 795, loss = 1.43152854\n",
      "Iteration 796, loss = 1.42983942\n",
      "Iteration 797, loss = 1.42814736\n",
      "Iteration 798, loss = 1.42645255\n",
      "Iteration 799, loss = 1.42475543\n",
      "Iteration 800, loss = 1.42305651\n",
      "Iteration 801, loss = 1.42135939\n",
      "Iteration 802, loss = 1.41967756\n",
      "Iteration 803, loss = 1.41798899\n",
      "Iteration 804, loss = 1.41630772\n",
      "Iteration 805, loss = 1.41462410\n",
      "Iteration 806, loss = 1.41295649\n",
      "Iteration 807, loss = 1.41127629\n",
      "Iteration 808, loss = 1.40959875\n",
      "Iteration 809, loss = 1.40793056\n",
      "Iteration 810, loss = 1.40625936\n",
      "Iteration 811, loss = 1.40458558\n",
      "Iteration 812, loss = 1.40292128\n",
      "Iteration 813, loss = 1.40125594\n",
      "Iteration 814, loss = 1.39958987\n",
      "Iteration 815, loss = 1.39793151\n",
      "Iteration 816, loss = 1.39627006\n",
      "Iteration 817, loss = 1.39461005\n",
      "Iteration 818, loss = 1.39295258\n",
      "Iteration 819, loss = 1.39130161\n",
      "Iteration 820, loss = 1.38964986\n",
      "Iteration 821, loss = 1.38800012\n",
      "Iteration 822, loss = 1.38634934\n",
      "Iteration 823, loss = 1.38470491\n",
      "Iteration 824, loss = 1.38305902\n",
      "Iteration 825, loss = 1.38142191\n",
      "Iteration 826, loss = 1.37977707\n",
      "Iteration 827, loss = 1.37814311\n",
      "Iteration 828, loss = 1.37651019\n",
      "Iteration 829, loss = 1.37487332\n",
      "Iteration 830, loss = 1.37323305\n",
      "Iteration 831, loss = 1.37160724\n",
      "Iteration 832, loss = 1.36998028\n",
      "Iteration 833, loss = 1.36834241\n",
      "Iteration 834, loss = 1.36672334\n",
      "Iteration 835, loss = 1.36510581\n",
      "Iteration 836, loss = 1.36348363\n",
      "Iteration 837, loss = 1.36185735\n",
      "Iteration 838, loss = 1.36022767\n",
      "Iteration 839, loss = 1.35862185\n",
      "Iteration 840, loss = 1.35701332\n",
      "Iteration 841, loss = 1.35539402\n",
      "Iteration 842, loss = 1.35376556\n",
      "Iteration 843, loss = 1.35219890\n",
      "Iteration 844, loss = 1.35056780\n",
      "Iteration 845, loss = 1.34896311\n",
      "Iteration 846, loss = 1.34738315\n",
      "Iteration 847, loss = 1.34579080\n",
      "Iteration 848, loss = 1.34418720\n",
      "Iteration 849, loss = 1.34257380\n",
      "Iteration 850, loss = 1.34095227\n",
      "Iteration 851, loss = 1.33935000\n",
      "Iteration 852, loss = 1.33775360\n",
      "Iteration 853, loss = 1.33615447\n",
      "Iteration 854, loss = 1.33457598\n",
      "Iteration 855, loss = 1.33299092\n",
      "Iteration 856, loss = 1.33139624\n",
      "Iteration 857, loss = 1.32980379\n",
      "Iteration 858, loss = 1.32822476\n",
      "Iteration 859, loss = 1.32664088\n",
      "Iteration 860, loss = 1.32505279\n",
      "Iteration 861, loss = 1.32348266\n",
      "Iteration 862, loss = 1.32190798\n",
      "Iteration 863, loss = 1.32032431\n",
      "Iteration 864, loss = 1.31874663\n",
      "Iteration 865, loss = 1.31717518\n",
      "Iteration 866, loss = 1.31560747\n",
      "Iteration 867, loss = 1.31403108\n",
      "Iteration 868, loss = 1.31247577\n",
      "Iteration 869, loss = 1.31089642\n",
      "Iteration 870, loss = 1.30933591\n",
      "Iteration 871, loss = 1.30776690\n",
      "Iteration 872, loss = 1.30621553\n",
      "Iteration 873, loss = 1.30464668\n",
      "Iteration 874, loss = 1.30309328\n",
      "Iteration 875, loss = 1.30153146\n",
      "Iteration 876, loss = 1.29998219\n",
      "Iteration 877, loss = 1.29843244\n",
      "Iteration 878, loss = 1.29687753\n",
      "Iteration 879, loss = 1.29531818\n",
      "Iteration 880, loss = 1.29377952\n",
      "Iteration 881, loss = 1.29223678\n",
      "Iteration 882, loss = 1.29068578\n",
      "Iteration 883, loss = 1.28912782\n",
      "Iteration 884, loss = 1.28761324\n",
      "Iteration 885, loss = 1.28604487\n",
      "Iteration 886, loss = 1.28450805\n",
      "Iteration 887, loss = 1.28297498\n",
      "Iteration 888, loss = 1.28143412\n",
      "Iteration 889, loss = 1.27988662\n",
      "Iteration 890, loss = 1.27839082\n",
      "Iteration 891, loss = 1.27682315\n",
      "Iteration 892, loss = 1.27529698\n",
      "Iteration 893, loss = 1.27377384\n",
      "Iteration 894, loss = 1.27224312\n",
      "Iteration 895, loss = 1.27070592\n",
      "Iteration 896, loss = 1.26919348\n",
      "Iteration 897, loss = 1.26765097\n",
      "Iteration 898, loss = 1.26613097\n",
      "Iteration 899, loss = 1.26460732\n",
      "Iteration 900, loss = 1.26308645\n",
      "Iteration 901, loss = 1.26157127\n",
      "Iteration 902, loss = 1.26005169\n",
      "Iteration 903, loss = 1.25853892\n",
      "Iteration 904, loss = 1.25702433\n",
      "Iteration 905, loss = 1.25550395\n",
      "Iteration 906, loss = 1.25399205\n",
      "Iteration 907, loss = 1.25250236\n",
      "Iteration 908, loss = 1.25098496\n",
      "Iteration 909, loss = 1.24948766\n",
      "Iteration 910, loss = 1.24798339\n",
      "Iteration 911, loss = 1.24647301\n",
      "Iteration 912, loss = 1.24495758\n",
      "Iteration 913, loss = 1.24348296\n",
      "Iteration 914, loss = 1.24195062\n",
      "Iteration 915, loss = 1.24046801\n",
      "Iteration 916, loss = 1.23898165\n",
      "Iteration 917, loss = 1.23748841\n",
      "Iteration 918, loss = 1.23598906\n",
      "Iteration 919, loss = 1.23448456\n",
      "Iteration 920, loss = 1.23297599\n",
      "Iteration 921, loss = 1.23152607\n",
      "Iteration 922, loss = 1.23001066\n",
      "Iteration 923, loss = 1.22852174\n",
      "Iteration 924, loss = 1.22705539\n",
      "Iteration 925, loss = 1.22558159\n",
      "Iteration 926, loss = 1.22410084\n",
      "Iteration 927, loss = 1.22261390\n",
      "Iteration 928, loss = 1.22112174\n",
      "Iteration 929, loss = 1.21962546\n",
      "Iteration 930, loss = 1.21812616\n",
      "Iteration 931, loss = 1.21666777\n",
      "Iteration 932, loss = 1.21518347\n",
      "Iteration 933, loss = 1.21369833\n",
      "Iteration 934, loss = 1.21222312\n",
      "Iteration 935, loss = 1.21075672\n",
      "Iteration 936, loss = 1.20928503\n",
      "Iteration 937, loss = 1.20780881\n",
      "Iteration 938, loss = 1.20632950\n",
      "Iteration 939, loss = 1.20486356\n",
      "Iteration 940, loss = 1.20339344\n",
      "Iteration 941, loss = 1.20192647\n",
      "Iteration 942, loss = 1.20046417\n",
      "Iteration 943, loss = 1.19899892\n",
      "Iteration 944, loss = 1.19753580\n",
      "Iteration 945, loss = 1.19607252\n",
      "Iteration 946, loss = 1.19461508\n",
      "Iteration 947, loss = 1.19315683\n",
      "Iteration 948, loss = 1.19169842\n",
      "Iteration 949, loss = 1.19024553\n",
      "Iteration 950, loss = 1.18879186\n",
      "Iteration 951, loss = 1.18733458\n",
      "Iteration 952, loss = 1.18589224\n",
      "Iteration 953, loss = 1.18443091\n",
      "Iteration 954, loss = 1.18299049\n",
      "Iteration 955, loss = 1.18154957\n",
      "Iteration 956, loss = 1.18010427\n",
      "Iteration 957, loss = 1.17865510\n",
      "Iteration 958, loss = 1.17720270\n",
      "Iteration 959, loss = 1.17574781\n",
      "Iteration 960, loss = 1.17432767\n",
      "Iteration 961, loss = 1.17285722\n",
      "Iteration 962, loss = 1.17141949\n",
      "Iteration 963, loss = 1.16997844\n",
      "Iteration 964, loss = 1.16854492\n",
      "Iteration 965, loss = 1.16711237\n",
      "Iteration 966, loss = 1.16568575\n",
      "Iteration 967, loss = 1.16425501\n",
      "Iteration 968, loss = 1.16282059\n",
      "Iteration 969, loss = 1.16138308\n",
      "Iteration 970, loss = 1.15994314\n",
      "Iteration 971, loss = 1.15850894\n",
      "Iteration 972, loss = 1.15708585\n",
      "Iteration 973, loss = 1.15565500\n",
      "Iteration 974, loss = 1.15421957\n",
      "Iteration 975, loss = 1.15279552\n",
      "Iteration 976, loss = 1.15136877\n",
      "Iteration 977, loss = 1.14994854\n",
      "Iteration 978, loss = 1.14853103\n",
      "Iteration 979, loss = 1.14711848\n",
      "Iteration 980, loss = 1.14570247\n",
      "Iteration 981, loss = 1.14428338\n",
      "Iteration 982, loss = 1.14286170\n",
      "Iteration 983, loss = 1.14143803\n",
      "Iteration 984, loss = 1.14001297\n",
      "Iteration 985, loss = 1.13862927\n",
      "Iteration 986, loss = 1.13718699\n",
      "Iteration 987, loss = 1.13577507\n",
      "Iteration 988, loss = 1.13436572\n",
      "Iteration 989, loss = 1.13295429\n",
      "Iteration 990, loss = 1.13156005\n",
      "Iteration 991, loss = 1.13014641\n",
      "Iteration 992, loss = 1.12874851\n",
      "Iteration 993, loss = 1.12734767\n",
      "Iteration 994, loss = 1.12594419\n",
      "Iteration 995, loss = 1.12453850\n",
      "Iteration 996, loss = 1.12313111\n",
      "Iteration 997, loss = 1.12172256\n",
      "Iteration 998, loss = 1.12033953\n",
      "Iteration 999, loss = 1.11892204\n",
      "Iteration 1000, loss = 1.11752837\n",
      "Iteration 1001, loss = 1.11613710\n",
      "Iteration 1002, loss = 1.11473721\n",
      "Iteration 1003, loss = 1.11333996\n",
      "Iteration 1004, loss = 1.11195618\n",
      "Iteration 1005, loss = 1.11055470\n",
      "Iteration 1006, loss = 1.10916514\n",
      "Iteration 1007, loss = 1.10777494\n",
      "Iteration 1008, loss = 1.10639231\n",
      "Iteration 1009, loss = 1.10500845\n",
      "Iteration 1010, loss = 1.10362265\n",
      "Iteration 1011, loss = 1.10223520\n",
      "Iteration 1012, loss = 1.10084650\n",
      "Iteration 1013, loss = 1.09945695\n",
      "Iteration 1014, loss = 1.09810809\n",
      "Iteration 1015, loss = 1.09669288\n",
      "Iteration 1016, loss = 1.09531692\n",
      "Iteration 1017, loss = 1.09393924\n",
      "Iteration 1018, loss = 1.09256007\n",
      "Iteration 1019, loss = 1.09118456\n",
      "Iteration 1020, loss = 1.08980221\n",
      "Iteration 1021, loss = 1.08842973\n",
      "Iteration 1022, loss = 1.08705720\n",
      "Iteration 1023, loss = 1.08567717\n",
      "Iteration 1024, loss = 1.08430646\n",
      "Iteration 1025, loss = 1.08293460\n",
      "Iteration 1026, loss = 1.08156395\n",
      "Iteration 1027, loss = 1.08019400\n",
      "Iteration 1028, loss = 1.07882508\n",
      "Iteration 1029, loss = 1.07746172\n",
      "Iteration 1030, loss = 1.07609412\n",
      "Iteration 1031, loss = 1.07473150\n",
      "Iteration 1032, loss = 1.07336774\n",
      "Iteration 1033, loss = 1.07200304\n",
      "Iteration 1034, loss = 1.07063768\n",
      "Iteration 1035, loss = 1.06927420\n",
      "Iteration 1036, loss = 1.06791768\n",
      "Iteration 1037, loss = 1.06655110\n",
      "Iteration 1038, loss = 1.06519386\n",
      "Iteration 1039, loss = 1.06383590\n",
      "Iteration 1040, loss = 1.06247969\n",
      "Iteration 1041, loss = 1.06112986\n",
      "Iteration 1042, loss = 1.05977027\n",
      "Iteration 1043, loss = 1.05841959\n",
      "Iteration 1044, loss = 1.05706806\n",
      "Iteration 1045, loss = 1.05571586\n",
      "Iteration 1046, loss = 1.05436320\n",
      "Iteration 1047, loss = 1.05301033\n",
      "Iteration 1048, loss = 1.05167711\n",
      "Iteration 1049, loss = 1.05031669\n",
      "Iteration 1050, loss = 1.04897616\n",
      "Iteration 1051, loss = 1.04763358\n",
      "Iteration 1052, loss = 1.04629116\n",
      "Iteration 1053, loss = 1.04494786\n",
      "Iteration 1054, loss = 1.04360388\n",
      "Iteration 1055, loss = 1.04225948\n",
      "Iteration 1056, loss = 1.04091492\n",
      "Iteration 1057, loss = 1.03957121\n",
      "Iteration 1058, loss = 1.03822770\n",
      "Iteration 1059, loss = 1.03689780\n",
      "Iteration 1060, loss = 1.03555382\n",
      "Iteration 1061, loss = 1.03422164\n",
      "Iteration 1062, loss = 1.03288887\n",
      "Iteration 1063, loss = 1.03155558\n",
      "Iteration 1064, loss = 1.03022190\n",
      "Iteration 1065, loss = 1.02888802\n",
      "Iteration 1066, loss = 1.02755595\n",
      "Iteration 1067, loss = 1.02622165\n",
      "Iteration 1068, loss = 1.02488942\n",
      "Iteration 1069, loss = 1.02355763\n",
      "Iteration 1070, loss = 1.02224926\n",
      "Iteration 1071, loss = 1.02090450\n",
      "Iteration 1072, loss = 1.01958232\n",
      "Iteration 1073, loss = 1.01825986\n",
      "Iteration 1074, loss = 1.01694142\n",
      "Iteration 1075, loss = 1.01561547\n",
      "Iteration 1076, loss = 1.01429359\n",
      "Iteration 1077, loss = 1.01297161\n",
      "Iteration 1078, loss = 1.01164971\n",
      "Iteration 1079, loss = 1.01032806\n",
      "Iteration 1080, loss = 1.00900684\n",
      "Iteration 1081, loss = 1.00768760\n",
      "Iteration 1082, loss = 1.00636754\n",
      "Iteration 1083, loss = 1.00507444\n",
      "Iteration 1084, loss = 1.00374309\n",
      "Iteration 1085, loss = 1.00242887\n",
      "Iteration 1086, loss = 1.00112108\n",
      "Iteration 1087, loss = 0.99981315\n",
      "Iteration 1088, loss = 0.99850495\n",
      "Iteration 1089, loss = 0.99719645\n",
      "Iteration 1090, loss = 0.99589056\n",
      "Iteration 1091, loss = 0.99457996\n",
      "Iteration 1092, loss = 0.99327207\n",
      "Iteration 1093, loss = 0.99196415\n",
      "Iteration 1094, loss = 0.99065641\n",
      "Iteration 1095, loss = 0.98934902\n",
      "Iteration 1096, loss = 0.98804218\n",
      "Iteration 1097, loss = 0.98673910\n",
      "Iteration 1098, loss = 0.98543192\n",
      "Iteration 1099, loss = 0.98412862\n",
      "Iteration 1100, loss = 0.98282621\n",
      "Iteration 1101, loss = 0.98152476\n",
      "Iteration 1102, loss = 0.98024203\n",
      "Iteration 1103, loss = 0.97892753\n",
      "Iteration 1104, loss = 0.97763125\n",
      "Iteration 1105, loss = 0.97633633\n",
      "Iteration 1106, loss = 0.97504149\n",
      "Iteration 1107, loss = 0.97374796\n",
      "Iteration 1108, loss = 0.97245494\n",
      "Iteration 1109, loss = 0.97116246\n",
      "Iteration 1110, loss = 0.96987057\n",
      "Iteration 1111, loss = 0.96857933\n",
      "Iteration 1112, loss = 0.96728878\n",
      "Iteration 1113, loss = 0.96601073\n",
      "Iteration 1114, loss = 0.96471369\n",
      "Iteration 1115, loss = 0.96342891\n",
      "Iteration 1116, loss = 0.96214457\n",
      "Iteration 1117, loss = 0.96086063\n",
      "Iteration 1118, loss = 0.95957706\n",
      "Iteration 1119, loss = 0.95829389\n",
      "Iteration 1120, loss = 0.95701127\n",
      "Iteration 1121, loss = 0.95573018\n",
      "Iteration 1122, loss = 0.95444961\n",
      "Iteration 1123, loss = 0.95316954\n",
      "Iteration 1124, loss = 0.95189004\n",
      "Iteration 1125, loss = 0.95061119\n",
      "Iteration 1126, loss = 0.94933306\n",
      "Iteration 1127, loss = 0.94805571\n",
      "Iteration 1128, loss = 0.94677946\n",
      "Iteration 1129, loss = 0.94550472\n",
      "Iteration 1130, loss = 0.94423100\n",
      "Iteration 1131, loss = 0.94296060\n",
      "Iteration 1132, loss = 0.94168735\n",
      "Iteration 1133, loss = 0.94041732\n",
      "Iteration 1134, loss = 0.93914793\n",
      "Iteration 1135, loss = 0.93787915\n",
      "Iteration 1136, loss = 0.93661341\n",
      "Iteration 1137, loss = 0.93534461\n",
      "Iteration 1138, loss = 0.93407873\n",
      "Iteration 1139, loss = 0.93281336\n",
      "Iteration 1140, loss = 0.93154853\n",
      "Iteration 1141, loss = 0.93028429\n",
      "Iteration 1142, loss = 0.92902070\n",
      "Iteration 1143, loss = 0.92776013\n",
      "Iteration 1144, loss = 0.92649682\n",
      "Iteration 1145, loss = 0.92523650\n",
      "Iteration 1146, loss = 0.92397687\n",
      "Iteration 1147, loss = 0.92271795\n",
      "Iteration 1148, loss = 0.92145980\n",
      "Iteration 1149, loss = 0.92020243\n",
      "Iteration 1150, loss = 0.91894586\n",
      "Iteration 1151, loss = 0.91769350\n",
      "Iteration 1152, loss = 0.91643635\n",
      "Iteration 1153, loss = 0.91518330\n",
      "Iteration 1154, loss = 0.91393097\n",
      "Iteration 1155, loss = 0.91267938\n",
      "Iteration 1156, loss = 0.91142853\n",
      "Iteration 1157, loss = 0.91017844\n",
      "Iteration 1158, loss = 0.90893163\n",
      "Iteration 1159, loss = 0.90768173\n",
      "Iteration 1160, loss = 0.90643502\n",
      "Iteration 1161, loss = 0.90518899\n",
      "Iteration 1162, loss = 0.90394366\n",
      "Iteration 1163, loss = 0.90269950\n",
      "Iteration 1164, loss = 0.90145495\n",
      "Iteration 1165, loss = 0.90021201\n",
      "Iteration 1166, loss = 0.89897339\n",
      "Iteration 1167, loss = 0.89773029\n",
      "Iteration 1168, loss = 0.89649117\n",
      "Iteration 1169, loss = 0.89525264\n",
      "Iteration 1170, loss = 0.89401466\n",
      "Iteration 1171, loss = 0.89277721\n",
      "Iteration 1172, loss = 0.89154150\n",
      "Iteration 1173, loss = 0.89030515\n",
      "Iteration 1174, loss = 0.88907046\n",
      "Iteration 1175, loss = 0.88783630\n",
      "Iteration 1176, loss = 0.88660273\n",
      "Iteration 1177, loss = 0.88536979\n",
      "Iteration 1178, loss = 0.88413757\n",
      "Iteration 1179, loss = 0.88290609\n",
      "Iteration 1180, loss = 0.88167579\n",
      "Iteration 1181, loss = 0.88044667\n",
      "Iteration 1182, loss = 0.87921864\n",
      "Iteration 1183, loss = 0.87799136\n",
      "Iteration 1184, loss = 0.87676485\n",
      "Iteration 1185, loss = 0.87553911\n",
      "Iteration 1186, loss = 0.87431415\n",
      "Iteration 1187, loss = 0.87308999\n",
      "Iteration 1188, loss = 0.87187021\n",
      "Iteration 1189, loss = 0.87064515\n",
      "Iteration 1190, loss = 0.86942438\n",
      "Iteration 1191, loss = 0.86820431\n",
      "Iteration 1192, loss = 0.86698495\n",
      "Iteration 1193, loss = 0.86576631\n",
      "Iteration 1194, loss = 0.86454947\n",
      "Iteration 1195, loss = 0.86333240\n",
      "Iteration 1196, loss = 0.86211704\n",
      "Iteration 1197, loss = 0.86090236\n",
      "Iteration 1198, loss = 0.85968835\n",
      "Iteration 1199, loss = 0.85847506\n",
      "Iteration 1200, loss = 0.85726249\n",
      "Iteration 1201, loss = 0.85605068\n",
      "Iteration 1202, loss = 0.85483964\n",
      "Iteration 1203, loss = 0.85363365\n",
      "Iteration 1204, loss = 0.85242232\n",
      "Iteration 1205, loss = 0.85121453\n",
      "Iteration 1206, loss = 0.85000863\n",
      "Iteration 1207, loss = 0.84880335\n",
      "Iteration 1208, loss = 0.84759871\n",
      "Iteration 1209, loss = 0.84639472\n",
      "Iteration 1210, loss = 0.84519141\n",
      "Iteration 1211, loss = 0.84398880\n",
      "Iteration 1212, loss = 0.84278690\n",
      "Iteration 1213, loss = 0.84158576\n",
      "Iteration 1214, loss = 0.84038537\n",
      "Iteration 1215, loss = 0.83918577\n",
      "Iteration 1216, loss = 0.83798697\n",
      "Iteration 1217, loss = 0.83678898\n",
      "Iteration 1218, loss = 0.83559199\n",
      "Iteration 1219, loss = 0.83439655\n",
      "Iteration 1220, loss = 0.83320201\n",
      "Iteration 1221, loss = 0.83200821\n",
      "Iteration 1222, loss = 0.83081658\n",
      "Iteration 1223, loss = 0.82962399\n",
      "Iteration 1224, loss = 0.82843350\n",
      "Iteration 1225, loss = 0.82724371\n",
      "Iteration 1226, loss = 0.82605464\n",
      "Iteration 1227, loss = 0.82486630\n",
      "Iteration 1228, loss = 0.82367869\n",
      "Iteration 1229, loss = 0.82249185\n",
      "Iteration 1230, loss = 0.82130579\n",
      "Iteration 1231, loss = 0.82012318\n",
      "Iteration 1232, loss = 0.81893716\n",
      "Iteration 1233, loss = 0.81775451\n",
      "Iteration 1234, loss = 0.81657259\n",
      "Iteration 1235, loss = 0.81539142\n",
      "Iteration 1236, loss = 0.81421196\n",
      "Iteration 1237, loss = 0.81303249\n",
      "Iteration 1238, loss = 0.81185465\n",
      "Iteration 1239, loss = 0.81067752\n",
      "Iteration 1240, loss = 0.80950112\n",
      "Iteration 1241, loss = 0.80832546\n",
      "Iteration 1242, loss = 0.80715058\n",
      "Iteration 1243, loss = 0.80597649\n",
      "Iteration 1244, loss = 0.80480320\n",
      "Iteration 1245, loss = 0.80363356\n",
      "Iteration 1246, loss = 0.80246023\n",
      "Iteration 1247, loss = 0.80129139\n",
      "Iteration 1248, loss = 0.80012329\n",
      "Iteration 1249, loss = 0.79895586\n",
      "Iteration 1250, loss = 0.79778912\n",
      "Iteration 1251, loss = 0.79662309\n",
      "Iteration 1252, loss = 0.79545778\n",
      "Iteration 1253, loss = 0.79429323\n",
      "Iteration 1254, loss = 0.79312944\n",
      "Iteration 1255, loss = 0.79196646\n",
      "Iteration 1256, loss = 0.79080429\n",
      "Iteration 1257, loss = 0.78964296\n",
      "Iteration 1258, loss = 0.78848249\n",
      "Iteration 1259, loss = 0.78732289\n",
      "Iteration 1260, loss = 0.78616551\n",
      "Iteration 1261, loss = 0.78500765\n",
      "Iteration 1262, loss = 0.78385233\n",
      "Iteration 1263, loss = 0.78269804\n",
      "Iteration 1264, loss = 0.78154449\n",
      "Iteration 1265, loss = 0.78039167\n",
      "Iteration 1266, loss = 0.77923961\n",
      "Iteration 1267, loss = 0.77808833\n",
      "Iteration 1268, loss = 0.77693784\n",
      "Iteration 1269, loss = 0.77578815\n",
      "Iteration 1270, loss = 0.77463930\n",
      "Iteration 1271, loss = 0.77349130\n",
      "Iteration 1272, loss = 0.77234417\n",
      "Iteration 1273, loss = 0.77119845\n",
      "Iteration 1274, loss = 0.77005360\n",
      "Iteration 1275, loss = 0.76891007\n",
      "Iteration 1276, loss = 0.76776735\n",
      "Iteration 1277, loss = 0.76662641\n",
      "Iteration 1278, loss = 0.76548545\n",
      "Iteration 1279, loss = 0.76434620\n",
      "Iteration 1280, loss = 0.76320772\n",
      "Iteration 1281, loss = 0.76207004\n",
      "Iteration 1282, loss = 0.76093317\n",
      "Iteration 1283, loss = 0.75979713\n",
      "Iteration 1284, loss = 0.75866194\n",
      "Iteration 1285, loss = 0.75752762\n",
      "Iteration 1286, loss = 0.75639734\n",
      "Iteration 1287, loss = 0.75526348\n",
      "Iteration 1288, loss = 0.75413297\n",
      "Iteration 1289, loss = 0.75300402\n",
      "Iteration 1290, loss = 0.75187582\n",
      "Iteration 1291, loss = 0.75074837\n",
      "Iteration 1292, loss = 0.74962170\n",
      "Iteration 1293, loss = 0.74849582\n",
      "Iteration 1294, loss = 0.74737077\n",
      "Iteration 1295, loss = 0.74624656\n",
      "Iteration 1296, loss = 0.74512323\n",
      "Iteration 1297, loss = 0.74400079\n",
      "Iteration 1298, loss = 0.74287926\n",
      "Iteration 1299, loss = 0.74175866\n",
      "Iteration 1300, loss = 0.74063928\n",
      "Iteration 1301, loss = 0.73952126\n",
      "Iteration 1302, loss = 0.73840436\n",
      "Iteration 1303, loss = 0.73728856\n",
      "Iteration 1304, loss = 0.73617418\n",
      "Iteration 1305, loss = 0.73506085\n",
      "Iteration 1306, loss = 0.73394833\n",
      "Iteration 1307, loss = 0.73283665\n",
      "Iteration 1308, loss = 0.73172582\n",
      "Iteration 1309, loss = 0.73061585\n",
      "Iteration 1310, loss = 0.72950677\n",
      "Iteration 1311, loss = 0.72839859\n",
      "Iteration 1312, loss = 0.72729154\n",
      "Iteration 1313, loss = 0.72618601\n",
      "Iteration 1314, loss = 0.72508154\n",
      "Iteration 1315, loss = 0.72397793\n",
      "Iteration 1316, loss = 0.72287520\n",
      "Iteration 1317, loss = 0.72177377\n",
      "Iteration 1318, loss = 0.72067346\n",
      "Iteration 1319, loss = 0.71957439\n",
      "Iteration 1320, loss = 0.71847616\n",
      "Iteration 1321, loss = 0.71737879\n",
      "Iteration 1322, loss = 0.71628232\n",
      "Iteration 1323, loss = 0.71518675\n",
      "Iteration 1324, loss = 0.71409211\n",
      "Iteration 1325, loss = 0.71300111\n",
      "Iteration 1326, loss = 0.71190704\n",
      "Iteration 1327, loss = 0.71081676\n",
      "Iteration 1328, loss = 0.70972768\n",
      "Iteration 1329, loss = 0.70863940\n",
      "Iteration 1330, loss = 0.70755195\n",
      "Iteration 1331, loss = 0.70646534\n",
      "Iteration 1332, loss = 0.70537959\n",
      "Iteration 1333, loss = 0.70429473\n",
      "Iteration 1334, loss = 0.70321078\n",
      "Iteration 1335, loss = 0.70212777\n",
      "Iteration 1336, loss = 0.70104572\n",
      "Iteration 1337, loss = 0.69996464\n",
      "Iteration 1338, loss = 0.69888456\n",
      "Iteration 1339, loss = 0.69780714\n",
      "Iteration 1340, loss = 0.69672932\n",
      "Iteration 1341, loss = 0.69565312\n",
      "Iteration 1342, loss = 0.69457874\n",
      "Iteration 1343, loss = 0.69350522\n",
      "Iteration 1344, loss = 0.69243258\n",
      "Iteration 1345, loss = 0.69136084\n",
      "Iteration 1346, loss = 0.69029001\n",
      "Iteration 1347, loss = 0.68922010\n",
      "Iteration 1348, loss = 0.68815114\n",
      "Iteration 1349, loss = 0.68708315\n",
      "Iteration 1350, loss = 0.68601615\n",
      "Iteration 1351, loss = 0.68495187\n",
      "Iteration 1352, loss = 0.68388667\n",
      "Iteration 1353, loss = 0.68282402\n",
      "Iteration 1354, loss = 0.68176276\n",
      "Iteration 1355, loss = 0.68070237\n",
      "Iteration 1356, loss = 0.67964286\n",
      "Iteration 1357, loss = 0.67858425\n",
      "Iteration 1358, loss = 0.67752656\n",
      "Iteration 1359, loss = 0.67646981\n",
      "Iteration 1360, loss = 0.67541403\n",
      "Iteration 1361, loss = 0.67435924\n",
      "Iteration 1362, loss = 0.67330547\n",
      "Iteration 1363, loss = 0.67225271\n",
      "Iteration 1364, loss = 0.67120529\n",
      "Iteration 1365, loss = 0.67015482\n",
      "Iteration 1366, loss = 0.66910341\n",
      "Iteration 1367, loss = 0.66805646\n",
      "Iteration 1368, loss = 0.66701042\n",
      "Iteration 1369, loss = 0.66596529\n",
      "Iteration 1370, loss = 0.66492110\n",
      "Iteration 1371, loss = 0.66387785\n",
      "Iteration 1372, loss = 0.66283556\n",
      "Iteration 1373, loss = 0.66179426\n",
      "Iteration 1374, loss = 0.66075397\n",
      "Iteration 1375, loss = 0.65971552\n",
      "Iteration 1376, loss = 0.65867744\n",
      "Iteration 1377, loss = 0.65764112\n",
      "Iteration 1378, loss = 0.65660577\n",
      "Iteration 1379, loss = 0.65557254\n",
      "Iteration 1380, loss = 0.65453900\n",
      "Iteration 1381, loss = 0.65350752\n",
      "Iteration 1382, loss = 0.65247698\n",
      "Iteration 1383, loss = 0.65144739\n",
      "Iteration 1384, loss = 0.65041878\n",
      "Iteration 1385, loss = 0.64939117\n",
      "Iteration 1386, loss = 0.64836487\n",
      "Iteration 1387, loss = 0.64733996\n",
      "Iteration 1388, loss = 0.64631630\n",
      "Iteration 1389, loss = 0.64529360\n",
      "Iteration 1390, loss = 0.64427189\n",
      "Iteration 1391, loss = 0.64325118\n",
      "Iteration 1392, loss = 0.64223339\n",
      "Iteration 1393, loss = 0.64121378\n",
      "Iteration 1394, loss = 0.64019703\n",
      "Iteration 1395, loss = 0.63918125\n",
      "Iteration 1396, loss = 0.63816646\n",
      "Iteration 1397, loss = 0.63715323\n",
      "Iteration 1398, loss = 0.63614087\n",
      "Iteration 1399, loss = 0.63513001\n",
      "Iteration 1400, loss = 0.63412012\n",
      "Iteration 1401, loss = 0.63311120\n",
      "Iteration 1402, loss = 0.63210328\n",
      "Iteration 1403, loss = 0.63109800\n",
      "Iteration 1404, loss = 0.63009337\n",
      "Iteration 1405, loss = 0.62908733\n",
      "Iteration 1406, loss = 0.62808491\n",
      "Iteration 1407, loss = 0.62708343\n",
      "Iteration 1408, loss = 0.62608294\n",
      "Iteration 1409, loss = 0.62508433\n",
      "Iteration 1410, loss = 0.62408661\n",
      "Iteration 1411, loss = 0.62308975\n",
      "Iteration 1412, loss = 0.62209379\n",
      "Iteration 1413, loss = 0.62109876\n",
      "Iteration 1414, loss = 0.62010473\n",
      "Iteration 1415, loss = 0.61911297\n",
      "Iteration 1416, loss = 0.61812077\n",
      "Iteration 1417, loss = 0.61713077\n",
      "Iteration 1418, loss = 0.61614337\n",
      "Iteration 1419, loss = 0.61515579\n",
      "Iteration 1420, loss = 0.61416849\n",
      "Iteration 1421, loss = 0.61318389\n",
      "Iteration 1422, loss = 0.61220021\n",
      "Iteration 1423, loss = 0.61121746\n",
      "Iteration 1424, loss = 0.61023563\n",
      "Iteration 1425, loss = 0.60925476\n",
      "Iteration 1426, loss = 0.60827665\n",
      "Iteration 1427, loss = 0.60729712\n",
      "Iteration 1428, loss = 0.60632109\n",
      "Iteration 1429, loss = 0.60534602\n",
      "Iteration 1430, loss = 0.60437183\n",
      "Iteration 1431, loss = 0.60339857\n",
      "Iteration 1432, loss = 0.60242627\n",
      "Iteration 1433, loss = 0.60145497\n",
      "Iteration 1434, loss = 0.60048613\n",
      "Iteration 1435, loss = 0.59951655\n",
      "Iteration 1436, loss = 0.59854801\n",
      "Iteration 1437, loss = 0.59758132\n",
      "Iteration 1438, loss = 0.59661705\n",
      "Iteration 1439, loss = 0.59565300\n",
      "Iteration 1440, loss = 0.59469038\n",
      "Iteration 1441, loss = 0.59372960\n",
      "Iteration 1442, loss = 0.59276973\n",
      "Iteration 1443, loss = 0.59181079\n",
      "Iteration 1444, loss = 0.59085281\n",
      "Iteration 1445, loss = 0.58989581\n",
      "Iteration 1446, loss = 0.58893985\n",
      "Iteration 1447, loss = 0.58798494\n",
      "Iteration 1448, loss = 0.58703203\n",
      "Iteration 1449, loss = 0.58607935\n",
      "Iteration 1450, loss = 0.58512859\n",
      "Iteration 1451, loss = 0.58418011\n",
      "Iteration 1452, loss = 0.58323129\n",
      "Iteration 1453, loss = 0.58228470\n",
      "Iteration 1454, loss = 0.58133908\n",
      "Iteration 1455, loss = 0.58039446\n",
      "Iteration 1456, loss = 0.57945085\n",
      "Iteration 1457, loss = 0.57850827\n",
      "Iteration 1458, loss = 0.57756715\n",
      "Iteration 1459, loss = 0.57662968\n",
      "Iteration 1460, loss = 0.57569031\n",
      "Iteration 1461, loss = 0.57475197\n",
      "Iteration 1462, loss = 0.57381660\n",
      "Iteration 1463, loss = 0.57288214\n",
      "Iteration 1464, loss = 0.57194860\n",
      "Iteration 1465, loss = 0.57101603\n",
      "Iteration 1466, loss = 0.57008446\n",
      "Iteration 1467, loss = 0.56915393\n",
      "Iteration 1468, loss = 0.56822470\n",
      "Iteration 1469, loss = 0.56729634\n",
      "Iteration 1470, loss = 0.56637226\n",
      "Iteration 1471, loss = 0.56544670\n",
      "Iteration 1472, loss = 0.56452110\n",
      "Iteration 1473, loss = 0.56359893\n",
      "Iteration 1474, loss = 0.56267773\n",
      "Iteration 1475, loss = 0.56175750\n",
      "Iteration 1476, loss = 0.56083828\n",
      "Iteration 1477, loss = 0.55992040\n",
      "Iteration 1478, loss = 0.55900316\n",
      "Iteration 1479, loss = 0.55808728\n",
      "Iteration 1480, loss = 0.55717476\n",
      "Iteration 1481, loss = 0.55626114\n",
      "Iteration 1482, loss = 0.55534883\n",
      "Iteration 1483, loss = 0.55443892\n",
      "Iteration 1484, loss = 0.55352995\n",
      "Iteration 1485, loss = 0.55262196\n",
      "Iteration 1486, loss = 0.55171496\n",
      "Iteration 1487, loss = 0.55080956\n",
      "Iteration 1488, loss = 0.54990430\n",
      "Iteration 1489, loss = 0.54900064\n",
      "Iteration 1490, loss = 0.54809866\n",
      "Iteration 1491, loss = 0.54719752\n",
      "Iteration 1492, loss = 0.54629801\n",
      "Iteration 1493, loss = 0.54540040\n",
      "Iteration 1494, loss = 0.54450379\n",
      "Iteration 1495, loss = 0.54360816\n",
      "Iteration 1496, loss = 0.54271362\n",
      "Iteration 1497, loss = 0.54182013\n",
      "Iteration 1498, loss = 0.54092775\n",
      "Iteration 1499, loss = 0.54003643\n",
      "Iteration 1500, loss = 0.53914711\n",
      "Iteration 1501, loss = 0.53825793\n",
      "Iteration 1502, loss = 0.53737077\n",
      "Iteration 1503, loss = 0.53648544\n",
      "Iteration 1504, loss = 0.53560114\n",
      "Iteration 1505, loss = 0.53471781\n",
      "Iteration 1506, loss = 0.53383549\n",
      "Iteration 1507, loss = 0.53295442\n",
      "Iteration 1508, loss = 0.53207415\n",
      "Iteration 1509, loss = 0.53119515\n",
      "Iteration 1510, loss = 0.53031793\n",
      "Iteration 1511, loss = 0.52944132\n",
      "Iteration 1512, loss = 0.52856642\n",
      "Iteration 1513, loss = 0.52769343\n",
      "Iteration 1514, loss = 0.52682068\n",
      "Iteration 1515, loss = 0.52594979\n",
      "Iteration 1516, loss = 0.52507989\n",
      "Iteration 1517, loss = 0.52421103\n",
      "Iteration 1518, loss = 0.52334392\n",
      "Iteration 1519, loss = 0.52247663\n",
      "Iteration 1520, loss = 0.52161385\n",
      "Iteration 1521, loss = 0.52074923\n",
      "Iteration 1522, loss = 0.51988604\n",
      "Iteration 1523, loss = 0.51903418\n",
      "Iteration 1524, loss = 0.51817052\n",
      "Iteration 1525, loss = 0.51732088\n",
      "Iteration 1526, loss = 0.51647438\n",
      "Iteration 1527, loss = 0.51562911\n",
      "Iteration 1528, loss = 0.51478357\n",
      "Iteration 1529, loss = 0.51393712\n",
      "Iteration 1530, loss = 0.51308828\n",
      "Iteration 1531, loss = 0.51224034\n",
      "Iteration 1532, loss = 0.51138895\n",
      "Iteration 1533, loss = 0.51053664\n",
      "Iteration 1534, loss = 0.50968585\n",
      "Iteration 1535, loss = 0.50883558\n",
      "Iteration 1536, loss = 0.50798636\n",
      "Iteration 1537, loss = 0.50713856\n",
      "Iteration 1538, loss = 0.50629243\n",
      "Iteration 1539, loss = 0.50544805\n",
      "Iteration 1540, loss = 0.50460594\n",
      "Iteration 1541, loss = 0.50376587\n",
      "Iteration 1542, loss = 0.50292811\n",
      "Iteration 1543, loss = 0.50209183\n",
      "Iteration 1544, loss = 0.50125647\n",
      "Iteration 1545, loss = 0.50042196\n",
      "Iteration 1546, loss = 0.49958828\n",
      "Iteration 1547, loss = 0.49875548\n",
      "Iteration 1548, loss = 0.49792409\n",
      "Iteration 1549, loss = 0.49709598\n",
      "Iteration 1550, loss = 0.49626709\n",
      "Iteration 1551, loss = 0.49543758\n",
      "Iteration 1552, loss = 0.49461190\n",
      "Iteration 1553, loss = 0.49378730\n",
      "Iteration 1554, loss = 0.49296384\n",
      "Iteration 1555, loss = 0.49214154\n",
      "Iteration 1556, loss = 0.49132041\n",
      "Iteration 1557, loss = 0.49050109\n",
      "Iteration 1558, loss = 0.48968255\n",
      "Iteration 1559, loss = 0.48887792\n",
      "Iteration 1560, loss = 0.48805645\n",
      "Iteration 1561, loss = 0.48725244\n",
      "Iteration 1562, loss = 0.48645024\n",
      "Iteration 1563, loss = 0.48565001\n",
      "Iteration 1564, loss = 0.48484901\n",
      "Iteration 1565, loss = 0.48404633\n",
      "Iteration 1566, loss = 0.48324163\n",
      "Iteration 1567, loss = 0.48243649\n",
      "Iteration 1568, loss = 0.48162798\n",
      "Iteration 1569, loss = 0.48082021\n",
      "Iteration 1570, loss = 0.48001254\n",
      "Iteration 1571, loss = 0.47920609\n",
      "Iteration 1572, loss = 0.47840108\n",
      "Iteration 1573, loss = 0.47759828\n",
      "Iteration 1574, loss = 0.47679731\n",
      "Iteration 1575, loss = 0.47599815\n",
      "Iteration 1576, loss = 0.47520071\n",
      "Iteration 1577, loss = 0.47440722\n",
      "Iteration 1578, loss = 0.47361291\n",
      "Iteration 1579, loss = 0.47281971\n",
      "Iteration 1580, loss = 0.47202903\n",
      "Iteration 1581, loss = 0.47123911\n",
      "Iteration 1582, loss = 0.47044996\n",
      "Iteration 1583, loss = 0.46966165\n",
      "Iteration 1584, loss = 0.46887427\n",
      "Iteration 1585, loss = 0.46808854\n",
      "Iteration 1586, loss = 0.46730515\n",
      "Iteration 1587, loss = 0.46652167\n",
      "Iteration 1588, loss = 0.46573871\n",
      "Iteration 1589, loss = 0.46496741\n",
      "Iteration 1590, loss = 0.46418352\n",
      "Iteration 1591, loss = 0.46341419\n",
      "Iteration 1592, loss = 0.46264853\n",
      "Iteration 1593, loss = 0.46188445\n",
      "Iteration 1594, loss = 0.46112021\n",
      "Iteration 1595, loss = 0.46035569\n",
      "Iteration 1596, loss = 0.45958801\n",
      "Iteration 1597, loss = 0.45882007\n",
      "Iteration 1598, loss = 0.45805057\n",
      "Iteration 1599, loss = 0.45728057\n",
      "Iteration 1600, loss = 0.45650960\n",
      "Iteration 1601, loss = 0.45573962\n",
      "Iteration 1602, loss = 0.45497078\n",
      "Iteration 1603, loss = 0.45420353\n",
      "Iteration 1604, loss = 0.45343813\n",
      "Iteration 1605, loss = 0.45267496\n",
      "Iteration 1606, loss = 0.45191452\n",
      "Iteration 1607, loss = 0.45115614\n",
      "Iteration 1608, loss = 0.45039949\n",
      "Iteration 1609, loss = 0.44964399\n",
      "Iteration 1610, loss = 0.44888946\n",
      "Iteration 1611, loss = 0.44813580\n",
      "Iteration 1612, loss = 0.44738295\n",
      "Iteration 1613, loss = 0.44663335\n",
      "Iteration 1614, loss = 0.44588267\n",
      "Iteration 1615, loss = 0.44513219\n",
      "Iteration 1616, loss = 0.44438460\n",
      "Iteration 1617, loss = 0.44363804\n",
      "Iteration 1618, loss = 0.44290819\n",
      "Iteration 1619, loss = 0.44215093\n",
      "Iteration 1620, loss = 0.44141523\n",
      "Iteration 1621, loss = 0.44068342\n",
      "Iteration 1622, loss = 0.43995559\n",
      "Iteration 1623, loss = 0.43922577\n",
      "Iteration 1624, loss = 0.43849508\n",
      "Iteration 1625, loss = 0.43776398\n",
      "Iteration 1626, loss = 0.43703109\n",
      "Iteration 1627, loss = 0.43629669\n",
      "Iteration 1628, loss = 0.43556135\n",
      "Iteration 1629, loss = 0.43482579\n",
      "Iteration 1630, loss = 0.43409075\n",
      "Iteration 1631, loss = 0.43335681\n",
      "Iteration 1632, loss = 0.43262526\n",
      "Iteration 1633, loss = 0.43189554\n",
      "Iteration 1634, loss = 0.43116779\n",
      "Iteration 1635, loss = 0.43044258\n",
      "Iteration 1636, loss = 0.42971892\n",
      "Iteration 1637, loss = 0.42899664\n",
      "Iteration 1638, loss = 0.42827555\n",
      "Iteration 1639, loss = 0.42755562\n",
      "Iteration 1640, loss = 0.42683733\n",
      "Iteration 1641, loss = 0.42611991\n",
      "Iteration 1642, loss = 0.42540372\n",
      "Iteration 1643, loss = 0.42468830\n",
      "Iteration 1644, loss = 0.42397416\n",
      "Iteration 1645, loss = 0.42326094\n",
      "Iteration 1646, loss = 0.42254876\n",
      "Iteration 1647, loss = 0.42184658\n",
      "Iteration 1648, loss = 0.42113216\n",
      "Iteration 1649, loss = 0.42043014\n",
      "Iteration 1650, loss = 0.41973227\n",
      "Iteration 1651, loss = 0.41903722\n",
      "Iteration 1652, loss = 0.41834234\n",
      "Iteration 1653, loss = 0.41764655\n",
      "Iteration 1654, loss = 0.41694924\n",
      "Iteration 1655, loss = 0.41625030\n",
      "Iteration 1656, loss = 0.41554998\n",
      "Iteration 1657, loss = 0.41485468\n",
      "Iteration 1658, loss = 0.41415968\n",
      "Iteration 1659, loss = 0.41345712\n",
      "Iteration 1660, loss = 0.41276249\n",
      "Iteration 1661, loss = 0.41206798\n",
      "Iteration 1662, loss = 0.41137668\n",
      "Iteration 1663, loss = 0.41068694\n",
      "Iteration 1664, loss = 0.40999817\n",
      "Iteration 1665, loss = 0.40931035\n",
      "Iteration 1666, loss = 0.40862369\n",
      "Iteration 1667, loss = 0.40793814\n",
      "Iteration 1668, loss = 0.40725371\n",
      "Iteration 1669, loss = 0.40657037\n",
      "Iteration 1670, loss = 0.40588809\n",
      "Iteration 1671, loss = 0.40520763\n",
      "Iteration 1672, loss = 0.40453346\n",
      "Iteration 1673, loss = 0.40385400\n",
      "Iteration 1674, loss = 0.40319563\n",
      "Iteration 1675, loss = 0.40250253\n",
      "Iteration 1676, loss = 0.40183372\n",
      "Iteration 1677, loss = 0.40116808\n",
      "Iteration 1678, loss = 0.40050482\n",
      "Iteration 1679, loss = 0.39984064\n",
      "Iteration 1680, loss = 0.39917641\n",
      "Iteration 1681, loss = 0.39851083\n",
      "Iteration 1682, loss = 0.39784981\n",
      "Iteration 1683, loss = 0.39718699\n",
      "Iteration 1684, loss = 0.39651686\n",
      "Iteration 1685, loss = 0.39585330\n",
      "Iteration 1686, loss = 0.39518879\n",
      "Iteration 1687, loss = 0.39452468\n",
      "Iteration 1688, loss = 0.39386013\n",
      "Iteration 1689, loss = 0.39319985\n",
      "Iteration 1690, loss = 0.39253793\n",
      "Iteration 1691, loss = 0.39187956\n",
      "Iteration 1692, loss = 0.39122406\n",
      "Iteration 1693, loss = 0.39056881\n",
      "Iteration 1694, loss = 0.38991573\n",
      "Iteration 1695, loss = 0.38926512\n",
      "Iteration 1696, loss = 0.38861508\n",
      "Iteration 1697, loss = 0.38796558\n",
      "Iteration 1698, loss = 0.38731662\n",
      "Iteration 1699, loss = 0.38666822\n",
      "Iteration 1700, loss = 0.38602042\n",
      "Iteration 1701, loss = 0.38537774\n",
      "Iteration 1702, loss = 0.38472935\n",
      "Iteration 1703, loss = 0.38408606\n",
      "Iteration 1704, loss = 0.38344425\n",
      "Iteration 1705, loss = 0.38280224\n",
      "Iteration 1706, loss = 0.38217132\n",
      "Iteration 1707, loss = 0.38152840\n",
      "Iteration 1708, loss = 0.38089741\n",
      "Iteration 1709, loss = 0.38026889\n",
      "Iteration 1710, loss = 0.37964142\n",
      "Iteration 1711, loss = 0.37901331\n",
      "Iteration 1712, loss = 0.37839441\n",
      "Iteration 1713, loss = 0.37777732\n",
      "Iteration 1714, loss = 0.37713397\n",
      "Iteration 1715, loss = 0.37649489\n",
      "Iteration 1716, loss = 0.37586256\n",
      "Iteration 1717, loss = 0.37523165\n",
      "Iteration 1718, loss = 0.37460226\n",
      "Iteration 1719, loss = 0.37397416\n",
      "Iteration 1720, loss = 0.37334762\n",
      "Iteration 1721, loss = 0.37272275\n",
      "Iteration 1722, loss = 0.37209952\n",
      "Iteration 1723, loss = 0.37147780\n",
      "Iteration 1724, loss = 0.37085820\n",
      "Iteration 1725, loss = 0.37025760\n",
      "Iteration 1726, loss = 0.36962543\n",
      "Iteration 1727, loss = 0.36900684\n",
      "Iteration 1728, loss = 0.36839610\n",
      "Iteration 1729, loss = 0.36778708\n",
      "Iteration 1730, loss = 0.36717888\n",
      "Iteration 1731, loss = 0.36657074\n",
      "Iteration 1732, loss = 0.36597120\n",
      "Iteration 1733, loss = 0.36536008\n",
      "Iteration 1734, loss = 0.36473755\n",
      "Iteration 1735, loss = 0.36412929\n",
      "Iteration 1736, loss = 0.36352016\n",
      "Iteration 1737, loss = 0.36291289\n",
      "Iteration 1738, loss = 0.36230914\n",
      "Iteration 1739, loss = 0.36170604\n",
      "Iteration 1740, loss = 0.36110349\n",
      "Iteration 1741, loss = 0.36050150\n",
      "Iteration 1742, loss = 0.35990011\n",
      "Iteration 1743, loss = 0.35930231\n",
      "Iteration 1744, loss = 0.35870365\n",
      "Iteration 1745, loss = 0.35810692\n",
      "Iteration 1746, loss = 0.35750795\n",
      "Iteration 1747, loss = 0.35691379\n",
      "Iteration 1748, loss = 0.35633217\n",
      "Iteration 1749, loss = 0.35572889\n",
      "Iteration 1750, loss = 0.35514186\n",
      "Iteration 1751, loss = 0.35455736\n",
      "Iteration 1752, loss = 0.35397389\n",
      "Iteration 1753, loss = 0.35340808\n",
      "Iteration 1754, loss = 0.35282833\n",
      "Iteration 1755, loss = 0.35223611\n",
      "Iteration 1756, loss = 0.35162802\n",
      "Iteration 1757, loss = 0.35103572\n",
      "Iteration 1758, loss = 0.35045029\n",
      "Iteration 1759, loss = 0.34986801\n",
      "Iteration 1760, loss = 0.34928751\n",
      "Iteration 1761, loss = 0.34870838\n",
      "Iteration 1762, loss = 0.34815222\n",
      "Iteration 1763, loss = 0.34754978\n",
      "Iteration 1764, loss = 0.34697894\n",
      "Iteration 1765, loss = 0.34640195\n",
      "Iteration 1766, loss = 0.34582736\n",
      "Iteration 1767, loss = 0.34525830\n",
      "Iteration 1768, loss = 0.34468945\n",
      "Iteration 1769, loss = 0.34412043\n",
      "Iteration 1770, loss = 0.34355295\n",
      "Iteration 1771, loss = 0.34297966\n",
      "Iteration 1772, loss = 0.34240922\n",
      "Iteration 1773, loss = 0.34183833\n",
      "Iteration 1774, loss = 0.34127556\n",
      "Iteration 1775, loss = 0.34070919\n",
      "Iteration 1776, loss = 0.34013995\n",
      "Iteration 1777, loss = 0.33957759\n",
      "Iteration 1778, loss = 0.33901569\n",
      "Iteration 1779, loss = 0.33845426\n",
      "Iteration 1780, loss = 0.33789333\n",
      "Iteration 1781, loss = 0.33733364\n",
      "Iteration 1782, loss = 0.33677403\n",
      "Iteration 1783, loss = 0.33621806\n",
      "Iteration 1784, loss = 0.33566060\n",
      "Iteration 1785, loss = 0.33510546\n",
      "Iteration 1786, loss = 0.33455074\n",
      "Iteration 1787, loss = 0.33399729\n",
      "Iteration 1788, loss = 0.33344478\n",
      "Iteration 1789, loss = 0.33289356\n",
      "Iteration 1790, loss = 0.33234438\n",
      "Iteration 1791, loss = 0.33179462\n",
      "Iteration 1792, loss = 0.33124458\n",
      "Iteration 1793, loss = 0.33069642\n",
      "Iteration 1794, loss = 0.33015378\n",
      "Iteration 1795, loss = 0.32960634\n",
      "Iteration 1796, loss = 0.32906154\n",
      "Iteration 1797, loss = 0.32851953\n",
      "Iteration 1798, loss = 0.32797809\n",
      "Iteration 1799, loss = 0.32743669\n",
      "Iteration 1800, loss = 0.32689583\n",
      "Iteration 1801, loss = 0.32635561\n",
      "Iteration 1802, loss = 0.32581569\n",
      "Iteration 1803, loss = 0.32527664\n",
      "Iteration 1804, loss = 0.32474629\n",
      "Iteration 1805, loss = 0.32421056\n",
      "Iteration 1806, loss = 0.32366929\n",
      "Iteration 1807, loss = 0.32313793\n",
      "Iteration 1808, loss = 0.32260537\n",
      "Iteration 1809, loss = 0.32207329\n",
      "Iteration 1810, loss = 0.32154252\n",
      "Iteration 1811, loss = 0.32101216\n",
      "Iteration 1812, loss = 0.32048901\n",
      "Iteration 1813, loss = 0.31995731\n",
      "Iteration 1814, loss = 0.31945104\n",
      "Iteration 1815, loss = 0.31895113\n",
      "Iteration 1816, loss = 0.31843770\n",
      "Iteration 1817, loss = 0.31791056\n",
      "Iteration 1818, loss = 0.31737266\n",
      "Iteration 1819, loss = 0.31682755\n",
      "Iteration 1820, loss = 0.31629383\n",
      "Iteration 1821, loss = 0.31577347\n",
      "Iteration 1822, loss = 0.31526764\n",
      "Iteration 1823, loss = 0.31473814\n",
      "Iteration 1824, loss = 0.31422205\n",
      "Iteration 1825, loss = 0.31370611\n",
      "Iteration 1826, loss = 0.31319255\n",
      "Iteration 1827, loss = 0.31268988\n",
      "Iteration 1828, loss = 0.31217895\n",
      "Iteration 1829, loss = 0.31166031\n",
      "Iteration 1830, loss = 0.31114211\n",
      "Iteration 1831, loss = 0.31063462\n",
      "Iteration 1832, loss = 0.31012352\n",
      "Iteration 1833, loss = 0.30961594\n",
      "Iteration 1834, loss = 0.30911041\n",
      "Iteration 1835, loss = 0.30860518\n",
      "Iteration 1836, loss = 0.30810096\n",
      "Iteration 1837, loss = 0.30759610\n",
      "Iteration 1838, loss = 0.30710246\n",
      "Iteration 1839, loss = 0.30658737\n",
      "Iteration 1840, loss = 0.30608596\n",
      "Iteration 1841, loss = 0.30560041\n",
      "Iteration 1842, loss = 0.30511740\n",
      "Iteration 1843, loss = 0.30462397\n",
      "Iteration 1844, loss = 0.30411922\n",
      "Iteration 1845, loss = 0.30360959\n",
      "Iteration 1846, loss = 0.30309580\n",
      "Iteration 1847, loss = 0.30259971\n",
      "Iteration 1848, loss = 0.30210481\n",
      "Iteration 1849, loss = 0.30161149\n",
      "Iteration 1850, loss = 0.30112023\n",
      "Iteration 1851, loss = 0.30062982\n",
      "Iteration 1852, loss = 0.30016180\n",
      "Iteration 1853, loss = 0.29964771\n",
      "Iteration 1854, loss = 0.29915808\n",
      "Iteration 1855, loss = 0.29867123\n",
      "Iteration 1856, loss = 0.29819627\n",
      "Iteration 1857, loss = 0.29772060\n",
      "Iteration 1858, loss = 0.29723585\n",
      "Iteration 1859, loss = 0.29674548\n",
      "Iteration 1860, loss = 0.29624668\n",
      "Iteration 1861, loss = 0.29576325\n",
      "Iteration 1862, loss = 0.29528088\n",
      "Iteration 1863, loss = 0.29480071\n",
      "Iteration 1864, loss = 0.29432328\n",
      "Iteration 1865, loss = 0.29384523\n",
      "Iteration 1866, loss = 0.29336631\n",
      "Iteration 1867, loss = 0.29288941\n",
      "Iteration 1868, loss = 0.29241334\n",
      "Iteration 1869, loss = 0.29193832\n",
      "Iteration 1870, loss = 0.29146344\n",
      "Iteration 1871, loss = 0.29098954\n",
      "Iteration 1872, loss = 0.29051593\n",
      "Iteration 1873, loss = 0.29004292\n",
      "Iteration 1874, loss = 0.28957164\n",
      "Iteration 1875, loss = 0.28910103\n",
      "Iteration 1876, loss = 0.28863225\n",
      "Iteration 1877, loss = 0.28816340\n",
      "Iteration 1878, loss = 0.28769351\n",
      "Iteration 1879, loss = 0.28722716\n",
      "Iteration 1880, loss = 0.28676107\n",
      "Iteration 1881, loss = 0.28629625\n",
      "Iteration 1882, loss = 0.28583173\n",
      "Iteration 1883, loss = 0.28537058\n",
      "Iteration 1884, loss = 0.28490558\n",
      "Iteration 1885, loss = 0.28444384\n",
      "Iteration 1886, loss = 0.28398236\n",
      "Iteration 1887, loss = 0.28352299\n",
      "Iteration 1888, loss = 0.28306227\n",
      "Iteration 1889, loss = 0.28260358\n",
      "Iteration 1890, loss = 0.28214795\n",
      "Iteration 1891, loss = 0.28169115\n",
      "Iteration 1892, loss = 0.28123587\n",
      "Iteration 1893, loss = 0.28077748\n",
      "Iteration 1894, loss = 0.28032393\n",
      "Iteration 1895, loss = 0.27987067\n",
      "Iteration 1896, loss = 0.27941989\n",
      "Iteration 1897, loss = 0.27896688\n",
      "Iteration 1898, loss = 0.27851629\n",
      "Iteration 1899, loss = 0.27806602\n",
      "Iteration 1900, loss = 0.27761617\n",
      "Iteration 1901, loss = 0.27716804\n",
      "Iteration 1902, loss = 0.27672062\n",
      "Iteration 1903, loss = 0.27627410\n",
      "Iteration 1904, loss = 0.27582849\n",
      "Iteration 1905, loss = 0.27538377\n",
      "Iteration 1906, loss = 0.27493942\n",
      "Iteration 1907, loss = 0.27449607\n",
      "Iteration 1908, loss = 0.27405382\n",
      "Iteration 1909, loss = 0.27361300\n",
      "Iteration 1910, loss = 0.27317179\n",
      "Iteration 1911, loss = 0.27273204\n",
      "Iteration 1912, loss = 0.27229285\n",
      "Iteration 1913, loss = 0.27185494\n",
      "Iteration 1914, loss = 0.27141732\n",
      "Iteration 1915, loss = 0.27098004\n",
      "Iteration 1916, loss = 0.27054411\n",
      "Iteration 1917, loss = 0.27010876\n",
      "Iteration 1918, loss = 0.26967454\n",
      "Iteration 1919, loss = 0.26924152\n",
      "Iteration 1920, loss = 0.26880902\n",
      "Iteration 1921, loss = 0.26837889\n",
      "Iteration 1922, loss = 0.26794657\n",
      "Iteration 1923, loss = 0.26751654\n",
      "Iteration 1924, loss = 0.26708680\n",
      "Iteration 1925, loss = 0.26665832\n",
      "Iteration 1926, loss = 0.26623136\n",
      "Iteration 1927, loss = 0.26580472\n",
      "Iteration 1928, loss = 0.26537755\n",
      "Iteration 1929, loss = 0.26495279\n",
      "Iteration 1930, loss = 0.26452713\n",
      "Iteration 1931, loss = 0.26411315\n",
      "Iteration 1932, loss = 0.26370002\n",
      "Iteration 1933, loss = 0.26329387\n",
      "Iteration 1934, loss = 0.26288382\n",
      "Iteration 1935, loss = 0.26246673\n",
      "Iteration 1936, loss = 0.26204311\n",
      "Iteration 1937, loss = 0.26161454\n",
      "Iteration 1938, loss = 0.26118386\n",
      "Iteration 1939, loss = 0.26075140\n",
      "Iteration 1940, loss = 0.26033499\n",
      "Iteration 1941, loss = 0.25999712\n",
      "Iteration 1942, loss = 0.25951186\n",
      "Iteration 1943, loss = 0.25910270\n",
      "Iteration 1944, loss = 0.25871588\n",
      "Iteration 1945, loss = 0.25832465\n",
      "Iteration 1946, loss = 0.25792609\n",
      "Iteration 1947, loss = 0.25752028\n",
      "Iteration 1948, loss = 0.25710552\n",
      "Iteration 1949, loss = 0.25668516\n",
      "Iteration 1950, loss = 0.25626266\n",
      "Iteration 1951, loss = 0.25583889\n",
      "Iteration 1952, loss = 0.25541580\n",
      "Iteration 1953, loss = 0.25499575\n",
      "Iteration 1954, loss = 0.25459321\n",
      "Iteration 1955, loss = 0.25419138\n",
      "Iteration 1956, loss = 0.25381295\n",
      "Iteration 1957, loss = 0.25338184\n",
      "Iteration 1958, loss = 0.25297773\n",
      "Iteration 1959, loss = 0.25257146\n",
      "Iteration 1960, loss = 0.25216957\n",
      "Iteration 1961, loss = 0.25177582\n",
      "Iteration 1962, loss = 0.25137774\n",
      "Iteration 1963, loss = 0.25097932\n",
      "Iteration 1964, loss = 0.25057446\n",
      "Iteration 1965, loss = 0.25017789\n",
      "Iteration 1966, loss = 0.24978157\n",
      "Iteration 1967, loss = 0.24938629\n",
      "Iteration 1968, loss = 0.24899124\n",
      "Iteration 1969, loss = 0.24859719\n",
      "Iteration 1970, loss = 0.24820342\n",
      "Iteration 1971, loss = 0.24780995\n",
      "Iteration 1972, loss = 0.24741846\n",
      "Iteration 1973, loss = 0.24702980\n",
      "Iteration 1974, loss = 0.24663695\n",
      "Iteration 1975, loss = 0.24624675\n",
      "Iteration 1976, loss = 0.24585893\n",
      "Iteration 1977, loss = 0.24547116\n",
      "Iteration 1978, loss = 0.24508344\n",
      "Iteration 1979, loss = 0.24469586\n",
      "Iteration 1980, loss = 0.24430852\n",
      "Iteration 1981, loss = 0.24392153\n",
      "Iteration 1982, loss = 0.24353572\n",
      "Iteration 1983, loss = 0.24315060\n",
      "Iteration 1984, loss = 0.24276881\n",
      "Iteration 1985, loss = 0.24238686\n",
      "Iteration 1986, loss = 0.24200392\n",
      "Iteration 1987, loss = 0.24162252\n",
      "Iteration 1988, loss = 0.24124178\n",
      "Iteration 1989, loss = 0.24086186\n",
      "Iteration 1990, loss = 0.24048150\n",
      "Iteration 1991, loss = 0.24010595\n",
      "Iteration 1992, loss = 0.23972773\n",
      "Iteration 1993, loss = 0.23934817\n",
      "Iteration 1994, loss = 0.23897216\n",
      "Iteration 1995, loss = 0.23859643\n",
      "Iteration 1996, loss = 0.23822100\n",
      "Iteration 1997, loss = 0.23784738\n",
      "Iteration 1998, loss = 0.23747364\n",
      "Iteration 1999, loss = 0.23710011\n",
      "Iteration 2000, loss = 0.23672813\n",
      "Iteration 2001, loss = 0.23635624\n",
      "Iteration 2002, loss = 0.23598620\n",
      "Iteration 2003, loss = 0.23561704\n",
      "Iteration 2004, loss = 0.23524808\n",
      "Iteration 2005, loss = 0.23487931\n",
      "Iteration 2006, loss = 0.23451080\n",
      "Iteration 2007, loss = 0.23414351\n",
      "Iteration 2008, loss = 0.23377509\n",
      "Iteration 2009, loss = 0.23341003\n",
      "Iteration 2010, loss = 0.23304567\n",
      "Iteration 2011, loss = 0.23267987\n",
      "Iteration 2012, loss = 0.23231525\n",
      "Iteration 2013, loss = 0.23195306\n",
      "Iteration 2014, loss = 0.23159107\n",
      "Iteration 2015, loss = 0.23122959\n",
      "Iteration 2016, loss = 0.23086834\n",
      "Iteration 2017, loss = 0.23050734\n",
      "Iteration 2018, loss = 0.23014665\n",
      "Iteration 2019, loss = 0.22978794\n",
      "Iteration 2020, loss = 0.22942832\n",
      "Iteration 2021, loss = 0.22907149\n",
      "Iteration 2022, loss = 0.22871601\n",
      "Iteration 2023, loss = 0.22835830\n",
      "Iteration 2024, loss = 0.22800287\n",
      "Iteration 2025, loss = 0.22764803\n",
      "Iteration 2026, loss = 0.22729339\n",
      "Iteration 2027, loss = 0.22693901\n",
      "Iteration 2028, loss = 0.22658584\n",
      "Iteration 2029, loss = 0.22623275\n",
      "Iteration 2030, loss = 0.22588098\n",
      "Iteration 2031, loss = 0.22553013\n",
      "Iteration 2032, loss = 0.22517972\n",
      "Iteration 2033, loss = 0.22482998\n",
      "Iteration 2034, loss = 0.22448080\n",
      "Iteration 2035, loss = 0.22413203\n",
      "Iteration 2036, loss = 0.22378328\n",
      "Iteration 2037, loss = 0.22343647\n",
      "Iteration 2038, loss = 0.22308912\n",
      "Iteration 2039, loss = 0.22274294\n",
      "Iteration 2040, loss = 0.22239909\n",
      "Iteration 2041, loss = 0.22205402\n",
      "Iteration 2042, loss = 0.22171035\n",
      "Iteration 2043, loss = 0.22136744\n",
      "Iteration 2044, loss = 0.22102470\n",
      "Iteration 2045, loss = 0.22068218\n",
      "Iteration 2046, loss = 0.22034084\n",
      "Iteration 2047, loss = 0.21999989\n",
      "Iteration 2048, loss = 0.21965888\n",
      "Iteration 2049, loss = 0.21932059\n",
      "Iteration 2050, loss = 0.21898314\n",
      "Iteration 2051, loss = 0.21864332\n",
      "Iteration 2052, loss = 0.21830629\n",
      "Iteration 2053, loss = 0.21796939\n",
      "Iteration 2054, loss = 0.21763266\n",
      "Iteration 2055, loss = 0.21729715\n",
      "Iteration 2056, loss = 0.21696181\n",
      "Iteration 2057, loss = 0.21662685\n",
      "Iteration 2058, loss = 0.21629272\n",
      "Iteration 2059, loss = 0.21595951\n",
      "Iteration 2060, loss = 0.21562765\n",
      "Iteration 2061, loss = 0.21529614\n",
      "Iteration 2062, loss = 0.21496489\n",
      "Iteration 2063, loss = 0.21463381\n",
      "Iteration 2064, loss = 0.21430296\n",
      "Iteration 2065, loss = 0.21397574\n",
      "Iteration 2066, loss = 0.21364584\n",
      "Iteration 2067, loss = 0.21331642\n",
      "Iteration 2068, loss = 0.21299032\n",
      "Iteration 2069, loss = 0.21266333\n",
      "Iteration 2070, loss = 0.21233693\n",
      "Iteration 2071, loss = 0.21201110\n",
      "Iteration 2072, loss = 0.21168723\n",
      "Iteration 2073, loss = 0.21136806\n",
      "Iteration 2074, loss = 0.21105383\n",
      "Iteration 2075, loss = 0.21073695\n",
      "Iteration 2076, loss = 0.21041880\n",
      "Iteration 2077, loss = 0.21009781\n",
      "Iteration 2078, loss = 0.20977397\n",
      "Iteration 2079, loss = 0.20944812\n",
      "Iteration 2080, loss = 0.20912128\n",
      "Iteration 2081, loss = 0.20879439\n",
      "Iteration 2082, loss = 0.20847185\n",
      "Iteration 2083, loss = 0.20815247\n",
      "Iteration 2084, loss = 0.20784241\n",
      "Iteration 2085, loss = 0.20751546\n",
      "Iteration 2086, loss = 0.20719843\n",
      "Iteration 2087, loss = 0.20688595\n",
      "Iteration 2088, loss = 0.20657333\n",
      "Iteration 2089, loss = 0.20625860\n",
      "Iteration 2090, loss = 0.20594288\n",
      "Iteration 2091, loss = 0.20562644\n",
      "Iteration 2092, loss = 0.20531062\n",
      "Iteration 2093, loss = 0.20499682\n",
      "Iteration 2094, loss = 0.20468548\n",
      "Iteration 2095, loss = 0.20437515\n",
      "Iteration 2096, loss = 0.20406490\n",
      "Iteration 2097, loss = 0.20375505\n",
      "Iteration 2098, loss = 0.20344533\n",
      "Iteration 2099, loss = 0.20313598\n",
      "Iteration 2100, loss = 0.20282699\n",
      "Iteration 2101, loss = 0.20251819\n",
      "Iteration 2102, loss = 0.20221174\n",
      "Iteration 2103, loss = 0.20190389\n",
      "Iteration 2104, loss = 0.20159598\n",
      "Iteration 2105, loss = 0.20129072\n",
      "Iteration 2106, loss = 0.20098532\n",
      "Iteration 2107, loss = 0.20068017\n",
      "Iteration 2108, loss = 0.20037576\n",
      "Iteration 2109, loss = 0.20007158\n",
      "Iteration 2110, loss = 0.19976971\n",
      "Iteration 2111, loss = 0.19946756\n",
      "Iteration 2112, loss = 0.19916422\n",
      "Iteration 2113, loss = 0.19886324\n",
      "Iteration 2114, loss = 0.19856177\n",
      "Iteration 2115, loss = 0.19826207\n",
      "Iteration 2116, loss = 0.19796282\n",
      "Iteration 2117, loss = 0.19766374\n",
      "Iteration 2118, loss = 0.19736660\n",
      "Iteration 2119, loss = 0.19706726\n",
      "Iteration 2120, loss = 0.19677020\n",
      "Iteration 2121, loss = 0.19647402\n",
      "Iteration 2122, loss = 0.19617782\n",
      "Iteration 2123, loss = 0.19588197\n",
      "Iteration 2124, loss = 0.19558659\n",
      "Iteration 2125, loss = 0.19529152\n",
      "Iteration 2126, loss = 0.19499699\n",
      "Iteration 2127, loss = 0.19470262\n",
      "Iteration 2128, loss = 0.19440885\n",
      "Iteration 2129, loss = 0.19411895\n",
      "Iteration 2130, loss = 0.19382704\n",
      "Iteration 2131, loss = 0.19353360\n",
      "Iteration 2132, loss = 0.19324313\n",
      "Iteration 2133, loss = 0.19295390\n",
      "Iteration 2134, loss = 0.19266452\n",
      "Iteration 2135, loss = 0.19237552\n",
      "Iteration 2136, loss = 0.19208693\n",
      "Iteration 2137, loss = 0.19179818\n",
      "Iteration 2138, loss = 0.19151126\n",
      "Iteration 2139, loss = 0.19122369\n",
      "Iteration 2140, loss = 0.19093553\n",
      "Iteration 2141, loss = 0.19064890\n",
      "Iteration 2142, loss = 0.19036331\n",
      "Iteration 2143, loss = 0.19007864\n",
      "Iteration 2144, loss = 0.18979451\n",
      "Iteration 2145, loss = 0.18951105\n",
      "Iteration 2146, loss = 0.18922772\n",
      "Iteration 2147, loss = 0.18894454\n",
      "Iteration 2148, loss = 0.18866155\n",
      "Iteration 2149, loss = 0.18837882\n",
      "Iteration 2150, loss = 0.18809708\n",
      "Iteration 2151, loss = 0.18781671\n",
      "Iteration 2152, loss = 0.18753615\n",
      "Iteration 2153, loss = 0.18725580\n",
      "Iteration 2154, loss = 0.18697703\n",
      "Iteration 2155, loss = 0.18669849\n",
      "Iteration 2156, loss = 0.18642010\n",
      "Iteration 2157, loss = 0.18614276\n",
      "Iteration 2158, loss = 0.18586484\n",
      "Iteration 2159, loss = 0.18558822\n",
      "Iteration 2160, loss = 0.18531225\n",
      "Iteration 2161, loss = 0.18503626\n",
      "Iteration 2162, loss = 0.18476110\n",
      "Iteration 2163, loss = 0.18448692\n",
      "Iteration 2164, loss = 0.18421200\n",
      "Iteration 2165, loss = 0.18393815\n",
      "Iteration 2166, loss = 0.18366468\n",
      "Iteration 2167, loss = 0.18339204\n",
      "Iteration 2168, loss = 0.18312086\n",
      "Iteration 2169, loss = 0.18284839\n",
      "Iteration 2170, loss = 0.18257800\n",
      "Iteration 2171, loss = 0.18230788\n",
      "Iteration 2172, loss = 0.18203784\n",
      "Iteration 2173, loss = 0.18176869\n",
      "Iteration 2174, loss = 0.18149901\n",
      "Iteration 2175, loss = 0.18122993\n",
      "Iteration 2176, loss = 0.18096136\n",
      "Iteration 2177, loss = 0.18069320\n",
      "Iteration 2178, loss = 0.18042641\n",
      "Iteration 2179, loss = 0.18015966\n",
      "Iteration 2180, loss = 0.17989272\n",
      "Iteration 2181, loss = 0.17962697\n",
      "Iteration 2182, loss = 0.17936157\n",
      "Iteration 2183, loss = 0.17909748\n",
      "Iteration 2184, loss = 0.17883312\n",
      "Iteration 2185, loss = 0.17856844\n",
      "Iteration 2186, loss = 0.17830575\n",
      "Iteration 2187, loss = 0.17804259\n",
      "Iteration 2188, loss = 0.17778087\n",
      "Iteration 2189, loss = 0.17751982\n",
      "Iteration 2190, loss = 0.17725842\n",
      "Iteration 2191, loss = 0.17699739\n",
      "Iteration 2192, loss = 0.17673678\n",
      "Iteration 2193, loss = 0.17647630\n",
      "Iteration 2194, loss = 0.17621645\n",
      "Iteration 2195, loss = 0.17595700\n",
      "Iteration 2196, loss = 0.17569882\n",
      "Iteration 2197, loss = 0.17544074\n",
      "Iteration 2198, loss = 0.17518225\n",
      "Iteration 2199, loss = 0.17492557\n",
      "Iteration 2200, loss = 0.17466919\n",
      "Iteration 2201, loss = 0.17441297\n",
      "Iteration 2202, loss = 0.17415688\n",
      "Iteration 2203, loss = 0.17390097\n",
      "Iteration 2204, loss = 0.17364667\n",
      "Iteration 2205, loss = 0.17339224\n",
      "Iteration 2206, loss = 0.17313807\n",
      "Iteration 2207, loss = 0.17288518\n",
      "Iteration 2208, loss = 0.17263265\n",
      "Iteration 2209, loss = 0.17238019\n",
      "Iteration 2210, loss = 0.17212840\n",
      "Iteration 2211, loss = 0.17187622\n",
      "Iteration 2212, loss = 0.17162445\n",
      "Iteration 2213, loss = 0.17137327\n",
      "Iteration 2214, loss = 0.17112399\n",
      "Iteration 2215, loss = 0.17087432\n",
      "Iteration 2216, loss = 0.17062433\n",
      "Iteration 2217, loss = 0.17037575\n",
      "Iteration 2218, loss = 0.17012834\n",
      "Iteration 2219, loss = 0.16988061\n",
      "Iteration 2220, loss = 0.16963257\n",
      "Iteration 2221, loss = 0.16938523\n",
      "Iteration 2222, loss = 0.16914003\n",
      "Iteration 2223, loss = 0.16889342\n",
      "Iteration 2224, loss = 0.16864686\n",
      "Iteration 2225, loss = 0.16840158\n",
      "Iteration 2226, loss = 0.16815649\n",
      "Iteration 2227, loss = 0.16791209\n",
      "Iteration 2228, loss = 0.16766841\n",
      "Iteration 2229, loss = 0.16742491\n",
      "Iteration 2230, loss = 0.16718111\n",
      "Iteration 2231, loss = 0.16693867\n",
      "Iteration 2232, loss = 0.16669635\n",
      "Iteration 2233, loss = 0.16645521\n",
      "Iteration 2234, loss = 0.16621407\n",
      "Iteration 2235, loss = 0.16597311\n",
      "Iteration 2236, loss = 0.16573271\n",
      "Iteration 2237, loss = 0.16549245\n",
      "Iteration 2238, loss = 0.16525267\n",
      "Iteration 2239, loss = 0.16501310\n",
      "Iteration 2240, loss = 0.16477525\n",
      "Iteration 2241, loss = 0.16453675\n",
      "Iteration 2242, loss = 0.16429798\n",
      "Iteration 2243, loss = 0.16406152\n",
      "Iteration 2244, loss = 0.16382485\n",
      "Iteration 2245, loss = 0.16358803\n",
      "Iteration 2246, loss = 0.16335184\n",
      "Iteration 2247, loss = 0.16311578\n",
      "Iteration 2248, loss = 0.16288009\n",
      "Iteration 2249, loss = 0.16264534\n",
      "Iteration 2250, loss = 0.16241072\n",
      "Iteration 2251, loss = 0.16217711\n",
      "Iteration 2252, loss = 0.16194380\n",
      "Iteration 2253, loss = 0.16171089\n",
      "Iteration 2254, loss = 0.16147806\n",
      "Iteration 2255, loss = 0.16124536\n",
      "Iteration 2256, loss = 0.16101308\n",
      "Iteration 2257, loss = 0.16078242\n",
      "Iteration 2258, loss = 0.16055080\n",
      "Iteration 2259, loss = 0.16032044\n",
      "Iteration 2260, loss = 0.16009072\n",
      "Iteration 2261, loss = 0.15986136\n",
      "Iteration 2262, loss = 0.15963189\n",
      "Iteration 2263, loss = 0.15940275\n",
      "Iteration 2264, loss = 0.15917387\n",
      "Iteration 2265, loss = 0.15894534\n",
      "Iteration 2266, loss = 0.15871700\n",
      "Iteration 2267, loss = 0.15848970\n",
      "Iteration 2268, loss = 0.15826328\n",
      "Iteration 2269, loss = 0.15803712\n",
      "Iteration 2270, loss = 0.15781090\n",
      "Iteration 2271, loss = 0.15758518\n",
      "Iteration 2272, loss = 0.15735979\n",
      "Iteration 2273, loss = 0.15713485\n",
      "Iteration 2274, loss = 0.15690995\n",
      "Iteration 2275, loss = 0.15668589\n",
      "Iteration 2276, loss = 0.15646216\n",
      "Iteration 2277, loss = 0.15623908\n",
      "Iteration 2278, loss = 0.15601671\n",
      "Iteration 2279, loss = 0.15579353\n",
      "Iteration 2280, loss = 0.15557112\n",
      "Iteration 2281, loss = 0.15534974\n",
      "Iteration 2282, loss = 0.15512836\n",
      "Iteration 2283, loss = 0.15490746\n",
      "Iteration 2284, loss = 0.15468709\n",
      "Iteration 2285, loss = 0.15446717\n",
      "Iteration 2286, loss = 0.15424733\n",
      "Iteration 2287, loss = 0.15402751\n",
      "Iteration 2288, loss = 0.15380871\n",
      "Iteration 2289, loss = 0.15359069\n",
      "Iteration 2290, loss = 0.15337244\n",
      "Iteration 2291, loss = 0.15315448\n",
      "Iteration 2292, loss = 0.15293715\n",
      "Iteration 2293, loss = 0.15272011\n",
      "Iteration 2294, loss = 0.15250357\n",
      "Iteration 2295, loss = 0.15228724\n",
      "Iteration 2296, loss = 0.15207150\n",
      "Iteration 2297, loss = 0.15185584\n",
      "Iteration 2298, loss = 0.15164084\n",
      "Iteration 2299, loss = 0.15142597\n",
      "Iteration 2300, loss = 0.15121190\n",
      "Iteration 2301, loss = 0.15099807\n",
      "Iteration 2302, loss = 0.15078483\n",
      "Iteration 2303, loss = 0.15057153\n",
      "Iteration 2304, loss = 0.15035815\n",
      "Iteration 2305, loss = 0.15014663\n",
      "Iteration 2306, loss = 0.14993428\n",
      "Iteration 2307, loss = 0.14972263\n",
      "Iteration 2308, loss = 0.14951181\n",
      "Iteration 2309, loss = 0.14930104\n",
      "Iteration 2310, loss = 0.14909041\n",
      "Iteration 2311, loss = 0.14888011\n",
      "Iteration 2312, loss = 0.14866987\n",
      "Iteration 2313, loss = 0.14846084\n",
      "Iteration 2314, loss = 0.14825143\n",
      "Iteration 2315, loss = 0.14804237\n",
      "Iteration 2316, loss = 0.14783460\n",
      "Iteration 2317, loss = 0.14762681\n",
      "Iteration 2318, loss = 0.14741941\n",
      "Iteration 2319, loss = 0.14721207\n",
      "Iteration 2320, loss = 0.14700515\n",
      "Iteration 2321, loss = 0.14679840\n",
      "Iteration 2322, loss = 0.14659205\n",
      "Iteration 2323, loss = 0.14638753\n",
      "Iteration 2324, loss = 0.14618208\n",
      "Iteration 2325, loss = 0.14597609\n",
      "Iteration 2326, loss = 0.14577192\n",
      "Iteration 2327, loss = 0.14556774\n",
      "Iteration 2328, loss = 0.14536389\n",
      "Iteration 2329, loss = 0.14515997\n",
      "Iteration 2330, loss = 0.14495697\n",
      "Iteration 2331, loss = 0.14475471\n",
      "Iteration 2332, loss = 0.14455192\n",
      "Iteration 2333, loss = 0.14434996\n",
      "Iteration 2334, loss = 0.14414863\n",
      "Iteration 2335, loss = 0.14394737\n",
      "Iteration 2336, loss = 0.14374603\n",
      "Iteration 2337, loss = 0.14354517\n",
      "Iteration 2338, loss = 0.14334475\n",
      "Iteration 2339, loss = 0.14314440\n",
      "Iteration 2340, loss = 0.14294468\n",
      "Iteration 2341, loss = 0.14274522\n",
      "Iteration 2342, loss = 0.14254657\n",
      "Iteration 2343, loss = 0.14234821\n",
      "Iteration 2344, loss = 0.14215005\n",
      "Iteration 2345, loss = 0.14195207\n",
      "Iteration 2346, loss = 0.14175442\n",
      "Iteration 2347, loss = 0.14155718\n",
      "Iteration 2348, loss = 0.14135994\n",
      "Iteration 2349, loss = 0.14116468\n",
      "Iteration 2350, loss = 0.14096840\n",
      "Iteration 2351, loss = 0.14077153\n",
      "Iteration 2352, loss = 0.14057676\n",
      "Iteration 2353, loss = 0.14038147\n",
      "Iteration 2354, loss = 0.14018670\n",
      "Iteration 2355, loss = 0.13999230\n",
      "Iteration 2356, loss = 0.13979826\n",
      "Iteration 2357, loss = 0.13960520\n",
      "Iteration 2358, loss = 0.13941128\n",
      "Iteration 2359, loss = 0.13921814\n",
      "Iteration 2360, loss = 0.13902562\n",
      "Iteration 2361, loss = 0.13883384\n",
      "Iteration 2362, loss = 0.13864179\n",
      "Iteration 2363, loss = 0.13844936\n",
      "Iteration 2364, loss = 0.13825787\n",
      "Iteration 2365, loss = 0.13806668\n",
      "Iteration 2366, loss = 0.13787615\n",
      "Iteration 2367, loss = 0.13768558\n",
      "Iteration 2368, loss = 0.13749579\n",
      "Iteration 2369, loss = 0.13730610\n",
      "Iteration 2370, loss = 0.13711674\n",
      "Iteration 2371, loss = 0.13692798\n",
      "Iteration 2372, loss = 0.13673928\n",
      "Iteration 2373, loss = 0.13655058\n",
      "Iteration 2374, loss = 0.13636255\n",
      "Iteration 2375, loss = 0.13617453\n",
      "Iteration 2376, loss = 0.13598724\n",
      "Iteration 2377, loss = 0.13580089\n",
      "Iteration 2378, loss = 0.13561434\n",
      "Iteration 2379, loss = 0.13542736\n",
      "Iteration 2380, loss = 0.13524142\n",
      "Iteration 2381, loss = 0.13505588\n",
      "Iteration 2382, loss = 0.13487044\n",
      "Iteration 2383, loss = 0.13468567\n",
      "Iteration 2384, loss = 0.13450071\n",
      "Iteration 2385, loss = 0.13431588\n",
      "Iteration 2386, loss = 0.13413186\n",
      "Iteration 2387, loss = 0.13394786\n",
      "Iteration 2388, loss = 0.13376412\n",
      "Iteration 2389, loss = 0.13358058\n",
      "Iteration 2390, loss = 0.13339835\n",
      "Iteration 2391, loss = 0.13321594\n",
      "Iteration 2392, loss = 0.13303314\n",
      "Iteration 2393, loss = 0.13285062\n",
      "Iteration 2394, loss = 0.13266950\n",
      "Iteration 2395, loss = 0.13248850\n",
      "Iteration 2396, loss = 0.13230769\n",
      "Iteration 2397, loss = 0.13212694\n",
      "Iteration 2398, loss = 0.13194634\n",
      "Iteration 2399, loss = 0.13176595\n",
      "Iteration 2400, loss = 0.13158657\n",
      "Iteration 2401, loss = 0.13140760\n",
      "Iteration 2402, loss = 0.13122826\n",
      "Iteration 2403, loss = 0.13104905\n",
      "Iteration 2404, loss = 0.13087098\n",
      "Iteration 2405, loss = 0.13069315\n",
      "Iteration 2406, loss = 0.13051534\n",
      "Iteration 2407, loss = 0.13033760\n",
      "Iteration 2408, loss = 0.13016012\n",
      "Iteration 2409, loss = 0.12998319\n",
      "Iteration 2410, loss = 0.12980634\n",
      "Iteration 2411, loss = 0.12962999\n",
      "Iteration 2412, loss = 0.12945367\n",
      "Iteration 2413, loss = 0.12927798\n",
      "Iteration 2414, loss = 0.12910266\n",
      "Iteration 2415, loss = 0.12892716\n",
      "Iteration 2416, loss = 0.12875256\n",
      "Iteration 2417, loss = 0.12857789\n",
      "Iteration 2418, loss = 0.12840359\n",
      "Iteration 2419, loss = 0.12822982\n",
      "Iteration 2420, loss = 0.12805646\n",
      "Iteration 2421, loss = 0.12788278\n",
      "Iteration 2422, loss = 0.12770972\n",
      "Iteration 2423, loss = 0.12753730\n",
      "Iteration 2424, loss = 0.12736468\n",
      "Iteration 2425, loss = 0.12719272\n",
      "Iteration 2426, loss = 0.12702116\n",
      "Iteration 2427, loss = 0.12684975\n",
      "Iteration 2428, loss = 0.12667842\n",
      "Iteration 2429, loss = 0.12650771\n",
      "Iteration 2430, loss = 0.12633655\n",
      "Iteration 2431, loss = 0.12616646\n",
      "Iteration 2432, loss = 0.12599646\n",
      "Iteration 2433, loss = 0.12582743\n",
      "Iteration 2434, loss = 0.12565819\n",
      "Iteration 2435, loss = 0.12548845\n",
      "Iteration 2436, loss = 0.12531924\n",
      "Iteration 2437, loss = 0.12515064\n",
      "Iteration 2438, loss = 0.12498223\n",
      "Iteration 2439, loss = 0.12481523\n",
      "Iteration 2440, loss = 0.12464751\n",
      "Iteration 2441, loss = 0.12447928\n",
      "Iteration 2442, loss = 0.12431214\n",
      "Iteration 2443, loss = 0.12414544\n",
      "Iteration 2444, loss = 0.12397877\n",
      "Iteration 2445, loss = 0.12381291\n",
      "Iteration 2446, loss = 0.12364709\n",
      "Iteration 2447, loss = 0.12348080\n",
      "Iteration 2448, loss = 0.12331525\n",
      "Iteration 2449, loss = 0.12314996\n",
      "Iteration 2450, loss = 0.12298494\n",
      "Iteration 2451, loss = 0.12282017\n",
      "Iteration 2452, loss = 0.12265602\n",
      "Iteration 2453, loss = 0.12249181\n",
      "Iteration 2454, loss = 0.12232796\n",
      "Iteration 2455, loss = 0.12216439\n",
      "Iteration 2456, loss = 0.12200095\n",
      "Iteration 2457, loss = 0.12183828\n",
      "Iteration 2458, loss = 0.12167556\n",
      "Iteration 2459, loss = 0.12151289\n",
      "Iteration 2460, loss = 0.12135089\n",
      "Iteration 2461, loss = 0.12118921\n",
      "Iteration 2462, loss = 0.12102801\n",
      "Iteration 2463, loss = 0.12086640\n",
      "Iteration 2464, loss = 0.12070549\n",
      "Iteration 2465, loss = 0.12054465\n",
      "Iteration 2466, loss = 0.12038406\n",
      "Iteration 2467, loss = 0.12022372\n",
      "Iteration 2468, loss = 0.12006368\n",
      "Iteration 2469, loss = 0.11990458\n",
      "Iteration 2470, loss = 0.11974526\n",
      "Iteration 2471, loss = 0.11958591\n",
      "Iteration 2472, loss = 0.11942667\n",
      "Iteration 2473, loss = 0.11926835\n",
      "Iteration 2474, loss = 0.11911027\n",
      "Iteration 2475, loss = 0.11895239\n",
      "Iteration 2476, loss = 0.11879473\n",
      "Iteration 2477, loss = 0.11863717\n",
      "Iteration 2478, loss = 0.11847971\n",
      "Iteration 2479, loss = 0.11832243\n",
      "Iteration 2480, loss = 0.11816608\n",
      "Iteration 2481, loss = 0.11801017\n",
      "Iteration 2482, loss = 0.11785394\n",
      "Iteration 2483, loss = 0.11769782\n",
      "Iteration 2484, loss = 0.11754203\n",
      "Iteration 2485, loss = 0.11738687\n",
      "Iteration 2486, loss = 0.11723177\n",
      "Iteration 2487, loss = 0.11707683\n",
      "Iteration 2488, loss = 0.11692205\n",
      "Iteration 2489, loss = 0.11676847\n",
      "Iteration 2490, loss = 0.11661428\n",
      "Iteration 2491, loss = 0.11645953\n",
      "Iteration 2492, loss = 0.11630629\n",
      "Iteration 2493, loss = 0.11615345\n",
      "Iteration 2494, loss = 0.11600067\n",
      "Iteration 2495, loss = 0.11584809\n",
      "Iteration 2496, loss = 0.11569560\n",
      "Iteration 2497, loss = 0.11554323\n",
      "Iteration 2498, loss = 0.11539094\n",
      "Iteration 2499, loss = 0.11523898\n",
      "Iteration 2500, loss = 0.11508715\n",
      "Iteration 2501, loss = 0.11493556\n",
      "Iteration 2502, loss = 0.11478559\n",
      "Iteration 2503, loss = 0.11463501\n",
      "Iteration 2504, loss = 0.11448403\n",
      "Iteration 2505, loss = 0.11433336\n",
      "Iteration 2506, loss = 0.11418363\n",
      "Iteration 2507, loss = 0.11403416\n",
      "Iteration 2508, loss = 0.11388479\n",
      "Iteration 2509, loss = 0.11373560\n",
      "Iteration 2510, loss = 0.11358650\n",
      "Iteration 2511, loss = 0.11343797\n",
      "Iteration 2512, loss = 0.11328909\n",
      "Iteration 2513, loss = 0.11314066\n",
      "Iteration 2514, loss = 0.11299350\n",
      "Iteration 2515, loss = 0.11284592\n",
      "Iteration 2516, loss = 0.11269772\n",
      "Iteration 2517, loss = 0.11255069\n",
      "Iteration 2518, loss = 0.11240390\n",
      "Iteration 2519, loss = 0.11225720\n",
      "Iteration 2520, loss = 0.11211122\n",
      "Iteration 2521, loss = 0.11196504\n",
      "Iteration 2522, loss = 0.11181852\n",
      "Iteration 2523, loss = 0.11167267\n",
      "Iteration 2524, loss = 0.11152704\n",
      "Iteration 2525, loss = 0.11138152\n",
      "Iteration 2526, loss = 0.11123610\n",
      "Iteration 2527, loss = 0.11109258\n",
      "Iteration 2528, loss = 0.11094814\n",
      "Iteration 2529, loss = 0.11080333\n",
      "Iteration 2530, loss = 0.11065876\n",
      "Iteration 2531, loss = 0.11051529\n",
      "Iteration 2532, loss = 0.11037189\n",
      "Iteration 2533, loss = 0.11022854\n",
      "Iteration 2534, loss = 0.11008538\n",
      "Iteration 2535, loss = 0.10994232\n",
      "Iteration 2536, loss = 0.10979996\n",
      "Iteration 2537, loss = 0.10965736\n",
      "Iteration 2538, loss = 0.10951447\n",
      "Iteration 2539, loss = 0.10937351\n",
      "Iteration 2540, loss = 0.10923193\n",
      "Iteration 2541, loss = 0.10908985\n",
      "Iteration 2542, loss = 0.10894848\n",
      "Iteration 2543, loss = 0.10880815\n",
      "Iteration 2544, loss = 0.10866750\n",
      "Iteration 2545, loss = 0.10852698\n",
      "Iteration 2546, loss = 0.10838683\n",
      "Iteration 2547, loss = 0.10824674\n",
      "Iteration 2548, loss = 0.10810685\n",
      "Iteration 2549, loss = 0.10796704\n",
      "Iteration 2550, loss = 0.10782732\n",
      "Iteration 2551, loss = 0.10768812\n",
      "Iteration 2552, loss = 0.10754988\n",
      "Iteration 2553, loss = 0.10741087\n",
      "Iteration 2554, loss = 0.10727204\n",
      "Iteration 2555, loss = 0.10713410\n",
      "Iteration 2556, loss = 0.10699631\n",
      "Iteration 2557, loss = 0.10685859\n",
      "Iteration 2558, loss = 0.10672157\n",
      "Iteration 2559, loss = 0.10658405\n",
      "Iteration 2560, loss = 0.10644694\n",
      "Iteration 2561, loss = 0.10631017\n",
      "Iteration 2562, loss = 0.10617354\n",
      "Iteration 2563, loss = 0.10603702\n",
      "Iteration 2564, loss = 0.10590058\n",
      "Iteration 2565, loss = 0.10576515\n",
      "Iteration 2566, loss = 0.10562949\n",
      "Iteration 2567, loss = 0.10549372\n",
      "Iteration 2568, loss = 0.10535797\n",
      "Iteration 2569, loss = 0.10522306\n",
      "Iteration 2570, loss = 0.10508849\n",
      "Iteration 2571, loss = 0.10495399\n",
      "Iteration 2572, loss = 0.10481956\n",
      "Iteration 2573, loss = 0.10468524\n",
      "Iteration 2574, loss = 0.10455105\n",
      "Iteration 2575, loss = 0.10441750\n",
      "Iteration 2576, loss = 0.10428420\n",
      "Iteration 2577, loss = 0.10415082\n",
      "Iteration 2578, loss = 0.10401750\n",
      "Iteration 2579, loss = 0.10388479\n",
      "Iteration 2580, loss = 0.10375218\n",
      "Iteration 2581, loss = 0.10361968\n",
      "Iteration 2582, loss = 0.10348740\n",
      "Iteration 2583, loss = 0.10335544\n",
      "Iteration 2584, loss = 0.10322360\n",
      "Iteration 2585, loss = 0.10309215\n",
      "Iteration 2586, loss = 0.10296109\n",
      "Iteration 2587, loss = 0.10282993\n",
      "Iteration 2588, loss = 0.10269905\n",
      "Iteration 2589, loss = 0.10256825\n",
      "Iteration 2590, loss = 0.10243786\n",
      "Iteration 2591, loss = 0.10230727\n",
      "Iteration 2592, loss = 0.10217786\n",
      "Iteration 2593, loss = 0.10204803\n",
      "Iteration 2594, loss = 0.10191803\n",
      "Iteration 2595, loss = 0.10178868\n",
      "Iteration 2596, loss = 0.10165979\n",
      "Iteration 2597, loss = 0.10153095\n",
      "Iteration 2598, loss = 0.10140248\n",
      "Iteration 2599, loss = 0.10127374\n",
      "Iteration 2600, loss = 0.10114533\n",
      "Iteration 2601, loss = 0.10101701\n",
      "Iteration 2602, loss = 0.10088910\n",
      "Iteration 2603, loss = 0.10076098\n",
      "Iteration 2604, loss = 0.10063325\n",
      "Iteration 2605, loss = 0.10050606\n",
      "Iteration 2606, loss = 0.10037906\n",
      "Iteration 2607, loss = 0.10025205\n",
      "Iteration 2608, loss = 0.10012541\n",
      "Iteration 2609, loss = 0.09999884\n",
      "Iteration 2610, loss = 0.09987269\n",
      "Iteration 2611, loss = 0.09974674\n",
      "Iteration 2612, loss = 0.09962073\n",
      "Iteration 2613, loss = 0.09949504\n",
      "Iteration 2614, loss = 0.09936992\n",
      "Iteration 2615, loss = 0.09924453\n",
      "Iteration 2616, loss = 0.09911951\n",
      "Iteration 2617, loss = 0.09899459\n",
      "Iteration 2618, loss = 0.09887002\n",
      "Iteration 2619, loss = 0.09874524\n",
      "Iteration 2620, loss = 0.09862118\n",
      "Iteration 2621, loss = 0.09849709\n",
      "Iteration 2622, loss = 0.09837331\n",
      "Iteration 2623, loss = 0.09824958\n",
      "Iteration 2624, loss = 0.09812620\n",
      "Iteration 2625, loss = 0.09800292\n",
      "Iteration 2626, loss = 0.09788000\n",
      "Iteration 2627, loss = 0.09775684\n",
      "Iteration 2628, loss = 0.09763460\n",
      "Iteration 2629, loss = 0.09751204\n",
      "Iteration 2630, loss = 0.09738973\n",
      "Iteration 2631, loss = 0.09726762\n",
      "Iteration 2632, loss = 0.09714582\n",
      "Iteration 2633, loss = 0.09702413\n",
      "Iteration 2634, loss = 0.09690278\n",
      "Iteration 2635, loss = 0.09678122\n",
      "Iteration 2636, loss = 0.09666041\n",
      "Iteration 2637, loss = 0.09653945\n",
      "Iteration 2638, loss = 0.09641884\n",
      "Iteration 2639, loss = 0.09629828\n",
      "Iteration 2640, loss = 0.09617807\n",
      "Iteration 2641, loss = 0.09605793\n",
      "Iteration 2642, loss = 0.09593814\n",
      "Iteration 2643, loss = 0.09581812\n",
      "Iteration 2644, loss = 0.09569873\n",
      "Iteration 2645, loss = 0.09557932\n",
      "Iteration 2646, loss = 0.09546036\n",
      "Iteration 2647, loss = 0.09534135\n",
      "Iteration 2648, loss = 0.09522267\n",
      "Iteration 2649, loss = 0.09510406\n",
      "Iteration 2650, loss = 0.09498579\n",
      "Iteration 2651, loss = 0.09486732\n",
      "Iteration 2652, loss = 0.09474940\n",
      "Iteration 2653, loss = 0.09463151\n",
      "Iteration 2654, loss = 0.09451408\n",
      "Iteration 2655, loss = 0.09439661\n",
      "Iteration 2656, loss = 0.09427944\n",
      "Iteration 2657, loss = 0.09416235\n",
      "Iteration 2658, loss = 0.09404557\n",
      "Iteration 2659, loss = 0.09392863\n",
      "Iteration 2660, loss = 0.09381217\n",
      "Iteration 2661, loss = 0.09369577\n",
      "Iteration 2662, loss = 0.09357989\n",
      "Iteration 2663, loss = 0.09346394\n",
      "Iteration 2664, loss = 0.09334825\n",
      "Iteration 2665, loss = 0.09323264\n",
      "Iteration 2666, loss = 0.09311733\n",
      "Iteration 2667, loss = 0.09300190\n",
      "Iteration 2668, loss = 0.09288687\n",
      "Iteration 2669, loss = 0.09277195\n",
      "Iteration 2670, loss = 0.09265756\n",
      "Iteration 2671, loss = 0.09254309\n",
      "Iteration 2672, loss = 0.09242887\n",
      "Iteration 2673, loss = 0.09231472\n",
      "Iteration 2674, loss = 0.09220083\n",
      "Iteration 2675, loss = 0.09208690\n",
      "Iteration 2676, loss = 0.09197332\n",
      "Iteration 2677, loss = 0.09185986\n",
      "Iteration 2678, loss = 0.09174688\n",
      "Iteration 2679, loss = 0.09163390\n",
      "Iteration 2680, loss = 0.09152113\n",
      "Iteration 2681, loss = 0.09140843\n",
      "Iteration 2682, loss = 0.09129593\n",
      "Iteration 2683, loss = 0.09118348\n",
      "Iteration 2684, loss = 0.09107133\n",
      "Iteration 2685, loss = 0.09095930\n",
      "Iteration 2686, loss = 0.09084771\n",
      "Iteration 2687, loss = 0.09073619\n",
      "Iteration 2688, loss = 0.09062484\n",
      "Iteration 2689, loss = 0.09051356\n",
      "Iteration 2690, loss = 0.09040246\n",
      "Iteration 2691, loss = 0.09029146\n",
      "Iteration 2692, loss = 0.09018069\n",
      "Iteration 2693, loss = 0.09007009\n",
      "Iteration 2694, loss = 0.08995981\n",
      "Iteration 2695, loss = 0.08984964\n",
      "Iteration 2696, loss = 0.08973971\n",
      "Iteration 2697, loss = 0.08962986\n",
      "Iteration 2698, loss = 0.08952014\n",
      "Iteration 2699, loss = 0.08941058\n",
      "Iteration 2700, loss = 0.08930151\n",
      "Iteration 2701, loss = 0.08919224\n",
      "Iteration 2702, loss = 0.08908317\n",
      "Iteration 2703, loss = 0.08897454\n",
      "Iteration 2704, loss = 0.08886598\n",
      "Iteration 2705, loss = 0.08875749\n",
      "Iteration 2706, loss = 0.08864908\n",
      "Iteration 2707, loss = 0.08854116\n",
      "Iteration 2708, loss = 0.08843299\n",
      "Iteration 2709, loss = 0.08832513\n",
      "Iteration 2710, loss = 0.08821766\n",
      "Iteration 2711, loss = 0.08811033\n",
      "Iteration 2712, loss = 0.08800311\n",
      "Iteration 2713, loss = 0.08789598\n",
      "Iteration 2714, loss = 0.08778892\n",
      "Iteration 2715, loss = 0.08768195\n",
      "Iteration 2716, loss = 0.08757589\n",
      "Iteration 2717, loss = 0.08746936\n",
      "Iteration 2718, loss = 0.08736271\n",
      "Iteration 2719, loss = 0.08725679\n",
      "Iteration 2720, loss = 0.08715093\n",
      "Iteration 2721, loss = 0.08704514\n",
      "Iteration 2722, loss = 0.08693942\n",
      "Iteration 2723, loss = 0.08683410\n",
      "Iteration 2724, loss = 0.08672868\n",
      "Iteration 2725, loss = 0.08662354\n",
      "Iteration 2726, loss = 0.08651882\n",
      "Iteration 2727, loss = 0.08641418\n",
      "Iteration 2728, loss = 0.08630961\n",
      "Iteration 2729, loss = 0.08620510\n",
      "Iteration 2730, loss = 0.08610068\n",
      "Iteration 2731, loss = 0.08599634\n",
      "Iteration 2732, loss = 0.08589261\n",
      "Iteration 2733, loss = 0.08578878\n",
      "Iteration 2734, loss = 0.08568495\n",
      "Iteration 2735, loss = 0.08558164\n",
      "Iteration 2736, loss = 0.08547838\n",
      "Iteration 2737, loss = 0.08537519\n",
      "Iteration 2738, loss = 0.08527208\n",
      "Iteration 2739, loss = 0.08516922\n",
      "Iteration 2740, loss = 0.08506658\n",
      "Iteration 2741, loss = 0.08496417\n",
      "Iteration 2742, loss = 0.08486185\n",
      "Iteration 2743, loss = 0.08475979\n",
      "Iteration 2744, loss = 0.08465780\n",
      "Iteration 2745, loss = 0.08455587\n",
      "Iteration 2746, loss = 0.08445402\n",
      "Iteration 2747, loss = 0.08435226\n",
      "Iteration 2748, loss = 0.08425110\n",
      "Iteration 2749, loss = 0.08414982\n",
      "Iteration 2750, loss = 0.08404854\n",
      "Iteration 2751, loss = 0.08394777\n",
      "Iteration 2752, loss = 0.08384705\n",
      "Iteration 2753, loss = 0.08374640\n",
      "Iteration 2754, loss = 0.08364582\n",
      "Iteration 2755, loss = 0.08354533\n",
      "Iteration 2756, loss = 0.08344582\n",
      "Iteration 2757, loss = 0.08334576\n",
      "Iteration 2758, loss = 0.08324539\n",
      "Iteration 2759, loss = 0.08314586\n",
      "Iteration 2760, loss = 0.08304639\n",
      "Iteration 2761, loss = 0.08294698\n",
      "Iteration 2762, loss = 0.08284765\n",
      "Iteration 2763, loss = 0.08274849\n",
      "Iteration 2764, loss = 0.08264956\n",
      "Iteration 2765, loss = 0.08255079\n",
      "Iteration 2766, loss = 0.08245223\n",
      "Iteration 2767, loss = 0.08235379\n",
      "Iteration 2768, loss = 0.08225555\n",
      "Iteration 2769, loss = 0.08215737\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=10, max_iter=5000, random_state=42, verbose=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model = MLPClassifier(hidden_layer_sizes = (10), activation = \"relu\", random_state = 42, max_iter = 5000, verbose = 1)\n",
    "\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/multi_class.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/multi_class.ipynb#ch0000003?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mpredict(X)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedEvoClassifierM01:\n",
    "    def __init__(self, n = 20, hidden_layers = False, activation = \"relu\", random_state = None):\n",
    "\n",
    "        self.n = n // 2 * 2\n",
    "        self.validation_loss_history = []\n",
    "        self.training_loss_history = []\n",
    "        self.random_state = random_state\n",
    "        self.activation = activation\n",
    "        self.number_of_layers = 0\n",
    "        \n",
    "        if hidden_layers:\n",
    "            self.layers = hidden_layers + [10]\n",
    "        else:\n",
    "            self.layers = [10]\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs = 100, validation_data = False, verbose = 0):\n",
    "\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        if validation_data:\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        else:\n",
    "            activation_function = lambda x: np.maximum(0, x)\n",
    "\n",
    "        #output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        output_activation_function = lambda x: np.exp(x) / np.sum(np.exp(x), axis = 2, keepdims = True)\n",
    "\n",
    "        X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "        y_train = y_train.astype(\"int8\")\n",
    "\n",
    "        n = self.n\n",
    "        layers = [X_train.shape[1]] + self.layers\n",
    "        number_of_layers_minus_one = len(layers) - 1\n",
    "        y_preds = np.zeros((n, y_train.shape[0], y_train.shape[1]))\n",
    "        nets_loss = np.zeros(n)\n",
    "        sorted_indices = np.arange(-(n // 2), n, 1)\n",
    "        #sorted_indices = np.zeros(n)\n",
    "        best_net_index = -1\n",
    "        weights = []\n",
    "\n",
    "        for i in range(number_of_layers_minus_one):\n",
    "            weights += [np.random.normal(0, 1, (n, layers[i], layers[i + 1]))]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            forward_pass = X_train.T\n",
    "            \n",
    "            for j in range(number_of_layers_minus_one - 1):\n",
    "                forward_pass = activation_function(weights[j][sorted_indices[n // 2:]].transpose(0, 2, 1) @ forward_pass)\n",
    "            \n",
    "            forward_pass = weights[-1][sorted_indices[n // 2:]].transpose(0, 2, 1) @ forward_pass\n",
    "            \n",
    "            y_preds[sorted_indices[n // 2:]] = output_activation_function(forward_pass.transpose(0, 2, 1))\n",
    "\n",
    "            nets_loss[sorted_indices[n // 2:]] = np.mean(np.sum(-y_train * np.log10(y_preds[sorted_indices[n // 2:]]), axis = 2), axis = 1)\n",
    "\n",
    "            sorted_indices = np.argsort(nets_loss)\n",
    "\n",
    "            mutation_sigma = 0.08 + 0.5 * 1 / math.exp(epoch / ((epochs + 1) / (60 * math.log10(epochs + 1))))\n",
    "\n",
    "            for j in range(number_of_layers_minus_one):\n",
    "                weights[j][sorted_indices[n // 2::2]] = (weights[j][sorted_indices[:n // 2:2]] + weights[j][sorted_indices[1:1 + n // 2:2]]) / 2 + np.random.normal(0, mutation_sigma, (n // 4, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[1 + n // 2::2]] = (weights[j][sorted_indices[:n // 2:2]] + weights[j][sorted_indices[1:1 + n // 2:2]]) / 2 + np.random.normal(0, mutation_sigma, (n // 4, layers[j], layers[j + 1]))\n",
    "\n",
    "            if best_net_index != sorted_indices[0]:\n",
    "                best_net_index = sorted_indices[0]\n",
    "                self.training_loss_history += [nets_loss[best_net_index]]\n",
    "                \n",
    "\n",
    "                self.best_net_weights = []\n",
    "                for j in range(number_of_layers_minus_one):\n",
    "                    self.best_net_weights += [weights[j][best_net_index]]\n",
    "                \n",
    "                if validation_data:\n",
    "                    self.validation_loss_history += [np.mean(np.abs(y_val - self.predict(X_val)))]\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]}\")\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        pass\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - {mutation_sigma}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        else:\n",
    "            activation_function = lambda x: np.maximum(0, x)\n",
    "\n",
    "        #output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        output_activation_function = lambda x: np.exp(x) / np.sum(np.exp(x), axis = 1, keepdims = True)\n",
    "\n",
    "        forward_pass = X.T\n",
    "        for j in range(len(self.best_net_weights) - 1):\n",
    "            forward_pass = activation_function(self.best_net_weights[j].T @ forward_pass)\n",
    "\n",
    "        forward_pass = self.best_net_weights[-1].T @ forward_pass\n",
    "        \n",
    "        return output_activation_function(forward_pass.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 0.9998311820346312 - 1.29197000149995\n",
      "Epoch 7 - loss: 0.9493318441214512 - 1.137112388781039\n",
      "Epoch 8 - loss: 0.945449826545742 - 1.1170352695971997\n",
      "Epoch 10 - loss: 0.9084038714228551 - 1.0782959928363374\n",
      "Epoch 11 - loss: 0.7883575093520281 - 1.0596115544075175\n",
      "Epoch 18 - loss: 0.7825842323168328 - 0.9406368729628037\n",
      "Epoch 21 - loss: 0.7821526861229309 - 0.8954468027447978\n",
      "Epoch 23 - loss: 0.772074035496816 - 0.8670744764788247\n",
      "Epoch 26 - loss: 0.6475596629216347 - 0.826984420356625\n",
      "Epoch 28 - loss: 0.6458369904993683 - 0.8018137178058867\n",
      "Epoch 30 - loss: 0.5164098736104002 - 0.7778198910879496\n",
      "Epoch 39 - loss: 0.5056881722204376 - 0.6829918872536017\n",
      "Epoch 42 - loss: 0.5033540862274465 - 0.6556569221318098\n",
      "Epoch 45 - loss: 0.4994401150710219 - 0.6302147129256686\n",
      "Epoch 49 - loss: 0.497508282466169 - 0.5990101199070317\n",
      "Epoch 50 - loss: 0.47200657881450925 - 0.5916641828003322\n",
      "Epoch 61 - loss: 0.4538528599723891 - 0.5214512626607359\n",
      "Epoch 66 - loss: 0.4489289979809142 - 0.4951460338805037\n",
      "Epoch 68 - loss: 0.4424817259673976 - 0.4854717671017367\n",
      "Epoch 70 - loss: 0.43121880414178854 - 0.4762481203465925\n",
      "Epoch 73 - loss: 0.416562734221192 - 0.46321163791183306\n",
      "Epoch 85 - loss: 0.35067370023627187 - 0.41945689331143376\n",
      "Epoch 89 - loss: 0.3477902485898014 - 0.4074357791815596\n",
      "Epoch 90 - loss: 0.34055668816873674 - 0.4046048005185673\n",
      "Epoch 93 - loss: 0.3382393621474246 - 0.3965042003208088\n",
      "Epoch 94 - loss: 0.33771431316003686 - 0.39392965127748114\n",
      "Epoch 95 - loss: 0.3278251528623542 - 0.3914154547190374\n",
      "Epoch 97 - loss: 0.32216732061269615 - 0.3865624283500497\n",
      "Epoch 106 - loss: 0.3126515267664953 - 0.3673565071492032\n",
      "Epoch 110 - loss: 0.3108052378970108 - 0.3600476375365833\n",
      "Epoch 116 - loss: 0.30302180331350903 - 0.35029943502390726\n",
      "Epoch 122 - loss: 0.29268412948968403 - 0.34183483903164946\n",
      "Epoch 127 - loss: 0.2897307803062248 - 0.3356366382046472\n",
      "Epoch 128 - loss: 0.28647910014209815 - 0.33448171471288113\n",
      "Epoch 131 - loss: 0.2828326230103581 - 0.33117456240647547\n",
      "Epoch 133 - loss: 0.28200911369600745 - 0.32909499964842304\n",
      "Epoch 139 - loss: 0.27806360169637634 - 0.32340696411646064\n",
      "Epoch 140 - loss: 0.2765691423408227 - 0.3225336736950033\n",
      "Epoch 153 - loss: 0.27651324391820964 - 0.3128399464118842\n",
      "Epoch 154 - loss: 0.2756940647279143 - 0.31220747462156984\n",
      "Epoch 155 - loss: 0.2754778098897267 - 0.31158930403648777\n",
      "Epoch 161 - loss: 0.2742205887541615 - 0.3081621698545043\n",
      "Epoch 169 - loss: 0.27393997418701566 - 0.30426049616603335\n",
      "Epoch 172 - loss: 0.27359381289989265 - 0.3029689902850127\n",
      "Epoch 179 - loss: 0.26934251207692317 - 0.300270616277696\n",
      "Epoch 181 - loss: 0.26846320070758967 - 0.2995732598218599\n",
      "Epoch 184 - loss: 0.2659167571585204 - 0.29858290872936644\n",
      "Epoch 189 - loss: 0.2499538953964917 - 0.29706946663180855\n",
      "Epoch 190 - loss: 0.24099708768870176 - 0.29678591577378954\n",
      "Epoch 191 - loss: 0.23486978602152062 - 0.2965083941267387\n",
      "Epoch 192 - loss: 0.20194605652060996 - 0.29623675878016686\n",
      "Epoch 194 - loss: 0.19131537635318124 - 0.2957105922113845\n",
      "Epoch 197 - loss: 0.18847543975024433 - 0.29496210747318363\n",
      "Epoch 200 - loss: 0.17401173132386386 - 0.29425951568766684\n",
      "Epoch 201 - loss: 0.17207413197167845 - 0.29403495960465964\n",
      "Epoch 202 - loss: 0.16184801687644096 - 0.29381503446255075\n",
      "Epoch 206 - loss: 0.15666653632056524 - 0.29297948695008214\n",
      "Epoch 208 - loss: 0.15555552236628709 - 0.29258676607284995\n",
      "Epoch 209 - loss: 0.15229712414264515 - 0.2923963259939727\n",
      "Epoch 212 - loss: 0.14772795153461693 - 0.291847580295844\n",
      "Epoch 214 - loss: 0.14753812381003112 - 0.2914996827085775\n",
      "Epoch 215 - loss: 0.13797837153235215 - 0.29133086086229665\n",
      "Epoch 216 - loss: 0.13451466263162304 - 0.2911653492162162\n",
      "Epoch 219 - loss: 0.13257828163141877 - 0.29068790042915404\n",
      "Epoch 223 - loss: 0.1308840796610969 - 0.290093094013796\n",
      "Epoch 224 - loss: 0.13050046835992576 - 0.28995139127611524\n",
      "Epoch 225 - loss: 0.12972505114922453 - 0.28981235624504265\n",
      "Epoch 226 - loss: 0.12047595942969935 - 0.28967592572639833\n",
      "Epoch 230 - loss: 0.12001330629838734 - 0.28915503660356534\n",
      "Epoch 232 - loss: 0.11850516507798224 - 0.2889086829190174\n",
      "Epoch 234 - loss: 0.1045344992612603 - 0.28867113922963084\n",
      "Epoch 246 - loss: 0.10272873678061116 - 0.2874090987062806\n",
      "Epoch 264 - loss: 0.09590907250588888 - 0.2859256648832146\n",
      "Epoch 278 - loss: 0.09395082415976769 - 0.28501145594689914\n",
      "Epoch 281 - loss: 0.09377847819624584 - 0.2848360123461836\n",
      "Epoch 287 - loss: 0.09375364609697777 - 0.28450310559165704\n",
      "Epoch 292 - loss: 0.09337044416033959 - 0.2842420721586656\n",
      "Epoch 293 - loss: 0.09278818751628502 - 0.2841914889381585\n",
      "Epoch 294 - loss: 0.09255447790591628 - 0.28414141732964526\n",
      "Epoch 296 - loss: 0.09199497746833142 - 0.28404276097926495\n",
      "Epoch 300 - loss: 0.08531517350574454 - 0.2838510756048839\n",
      "Epoch 308 - loss: 0.08481630882329047 - 0.28348787059256264\n",
      "Epoch 318 - loss: 0.08339452254865942 - 0.2830656816219028\n",
      "Epoch 319 - loss: 0.0810852048048722 - 0.2830251317093292\n",
      "Epoch 331 - loss: 0.08058677646736524 - 0.28255827526860133\n",
      "Epoch 332 - loss: 0.07192682825938757 - 0.2825208435402259\n",
      "Epoch 339 - loss: 0.06764876887300363 - 0.2822642316117564\n",
      "Epoch 343 - loss: 0.06319832323522355 - 0.28212146602304466\n",
      "Epoch 349 - loss: 0.05997728125635064 - 0.2819119333557869\n",
      "Epoch 351 - loss: 0.05815085867140035 - 0.2818432090234706\n",
      "Epoch 355 - loss: 0.04533895536049003 - 0.28170728901265973\n",
      "Epoch 382 - loss: 0.03900950705205968 - 0.2808315656656267\n",
      "Epoch 384 - loss: 0.03859017334952672 - 0.2807689371090166\n",
      "Epoch 386 - loss: 0.020166927511282746 - 0.2807065492249322\n",
      "Epoch 401 - loss: 0.01648711707451175 - 0.2802453058327494\n",
      "Epoch 405 - loss: 0.008792068892168471 - 0.28012400518070146\n",
      "Epoch 407 - loss: 0.006887931044233192 - 0.280063583167096\n",
      "Epoch 410 - loss: 0.0066190071186247625 - 0.27997321716988566\n",
      "Epoch 411 - loss: 0.003987557649115078 - 0.27994316330267854\n",
      "Epoch 416 - loss: 0.002420511318634888 - 0.2797933725687788\n",
      "Epoch 422 - loss: 0.0012615694763107566 - 0.2796145897244996\n",
      "Epoch 448 - loss: 0.0011253705857586155 - 0.2788493259705622\n",
      "Epoch 451 - loss: 0.0004226458153368725 - 0.2787618038400437\n",
      "Epoch 464 - loss: 0.000311481344377273 - 0.27838394436508507\n",
      "Epoch 467 - loss: 0.00027668825776446036 - 0.2782970338342181\n",
      "Epoch 479 - loss: 0.00013634889517272604 - 0.2779503055490482\n",
      "Epoch 481 - loss: 8.96956060844415e-05 - 0.27789264663271906\n",
      "Epoch 484 - loss: 7.232293614919244e-05 - 0.2778062212515685\n",
      "Epoch 500 - loss: 5.803475067458429e-05 - 0.27734643530620195\n",
      "Epoch 504 - loss: 5.355812579601346e-05 - 0.27723175966455205\n",
      "Epoch 508 - loss: 5.166264649887683e-05 - 0.27711718113479933\n",
      "Epoch 518 - loss: 4.9151700683340875e-05 - 0.27683112968483437\n",
      "Epoch 525 - loss: 3.45479181014971e-05 - 0.27663120366433047\n",
      "Epoch 531 - loss: 3.366135477605386e-05 - 0.27646002601656705\n",
      "Epoch 533 - loss: 2.470918191263677e-05 - 0.27640300346214064\n",
      "Epoch 538 - loss: 2.2130368478013337e-05 - 0.2762605242418109\n",
      "Epoch 540 - loss: 1.2548599093776857e-05 - 0.2762035626526509\n",
      "Epoch 550 - loss: 6.5856080969289985e-06 - 0.2759190015872939\n",
      "Epoch 553 - loss: 5.494818195344873e-06 - 0.275833710674382\n",
      "Epoch 558 - loss: 3.195475679643043e-06 - 0.27569163545138387\n",
      "Epoch 572 - loss: 1.0275177020412378e-06 - 0.275294311385586\n",
      "Epoch 590 - loss: 1.6498959924541604e-07 - 0.2747844594311778\n",
      "Epoch 616 - loss: 7.471627302954734e-08 - 0.2740498477819908\n",
      "Epoch 631 - loss: 5.1865513711392926e-08 - 0.27362697596512897\n",
      "Epoch 643 - loss: 3.250349047222073e-08 - 0.27328916032977846\n",
      "Epoch 653 - loss: 1.931183942150221e-08 - 0.27300796959774676\n",
      "Epoch 661 - loss: 1.4521874184144837e-08 - 0.272783225959347\n",
      "Epoch 667 - loss: 9.191114789420833e-09 - 0.2726147893437427\n",
      "Epoch 673 - loss: 8.89875557404683e-09 - 0.27244645606801143\n",
      "Epoch 676 - loss: 8.075903839003031e-09 - 0.2723623280647893\n",
      "Epoch 688 - loss: 8.037203932780033e-09 - 0.27202607265366674\n",
      "Epoch 689 - loss: 3.782748077959326e-09 - 0.27199806985416153\n",
      "Epoch 697 - loss: 3.55758778244497e-09 - 0.2717741495261311\n",
      "Epoch 700 - loss: 1.9626541741058253e-09 - 0.2716902261118229\n",
      "Epoch 709 - loss: 1.1834931220773132e-09 - 0.2714386083730097\n",
      "Epoch 716 - loss: 1.0737783053205083e-09 - 0.2712430635226073\n",
      "Epoch 733 - loss: 5.336788578214688e-10 - 0.2707687421141589\n",
      "Epoch 741 - loss: 1.0380240022413125e-10 - 0.2705458123207293\n",
      "Epoch 787 - loss: 4.047676437409286e-11 - 0.2692674315391168\n",
      "Epoch 827 - loss: 3.5970448971078684e-11 - 0.2681605731563417\n",
      "Epoch 851 - loss: 1.3959972604663612e-11 - 0.2674985814426485\n",
      "Epoch 852 - loss: 4.698637364493006e-12 - 0.26747103293010543\n",
      "Epoch 916 - loss: 4.5240634099520215e-12 - 0.2657136469250153\n",
      "Epoch 925 - loss: 4.143057627871045e-12 - 0.2654674154087652\n",
      "Epoch 934 - loss: 1.6150600242967052e-12 - 0.2652214054118307\n",
      "Epoch 954 - loss: 1.0792884504895953e-12 - 0.26467550860993894\n",
      "Epoch 977 - loss: 1.0674842056397354e-12 - 0.2640490755662046\n",
      "Epoch 995 - loss: 4.557280107743786e-13 - 0.2635598276624995\n",
      "Epoch 1000 - loss: 3.4092920918708105e-13 - 0.2634240816831992\n",
      "Epoch 1022 - loss: 8.705685369646029e-14 - 0.262827605052952\n",
      "Epoch 1035 - loss: 4.957081508751834e-14 - 0.2624757579106366\n",
      "Epoch 1052 - loss: 2.7733181276745968e-14 - 0.2620163397328472\n",
      "Epoch 1065 - loss: 2.4436934663798514e-14 - 0.2616655465532729\n",
      "Epoch 1076 - loss: 1.5560738664837213e-15 - 0.26136907753808425\n",
      "Epoch 1099 - loss: 1.034460373211717e-15 - 0.26075024059246543\n",
      "Epoch 1104 - loss: 5.698298665996698e-17 - 0.2606158990599562\n",
      "Epoch 1119 - loss: 2.191653333075653e-17 - 0.26021327725227716\n",
      "Epoch 1140 - loss: 1.7533226664605225e-17 - 0.2596506203656993\n",
      "Epoch 1163 - loss: 1.3149919998453918e-17 - 0.25903573133175706\n",
      "Epoch 1204 - loss: 8.766613332302611e-18 - 0.2579431262038957\n",
      "Epoch 1211 - loss: 8.766613332302611e-18 - 0.25775703115637505\n",
      "Epoch 1212 - loss: 8.766613332302611e-18 - 0.2577304567819946\n",
      "Epoch 1214 - loss: 0.0 - 0.2576773160048822\n",
      "Epoch 1217 - loss: 0.0 - 0.2575976247643491\n",
      "Epoch 1225 - loss: 0.0 - 0.2573852316330819\n",
      "Epoch 1226 - loss: 0.0 - 0.2573586944367965\n",
      "Epoch 1234 - loss: 0.0 - 0.25714649237336085\n",
      "Epoch 1235 - loss: 0.0 - 0.25711997904980854\n",
      "Epoch 1236 - loss: 0.0 - 0.25709346837745617\n",
      "Epoch 1237 - loss: 0.0 - 0.2570669603560385\n",
      "Epoch 1238 - loss: 0.0 - 0.2570404549852905\n",
      "Epoch 1239 - loss: 0.0 - 0.25701395226494705\n",
      "Epoch 1240 - loss: 0.0 - 0.2569874521947433\n",
      "Epoch 1241 - loss: 0.0 - 0.25696095477441405\n",
      "Epoch 1242 - loss: 0.0 - 0.2569344600036946\n",
      "Epoch 1243 - loss: 0.0 - 0.2569079678823196\n",
      "Epoch 1244 - loss: 0.0 - 0.2568814784100245\n",
      "Epoch 1245 - loss: 0.0 - 0.2568549915865441\n",
      "Epoch 1247 - loss: 0.0 - 0.2568020258849687\n",
      "Epoch 1248 - loss: 0.0 - 0.25677554700634375\n",
      "Epoch 1249 - loss: 0.0 - 0.2567490707754745\n",
      "Epoch 1250 - loss: 0.0 - 0.2567225971920959\n",
      "Epoch 1251 - loss: 0.0 - 0.2566961262559433\n",
      "Epoch 1252 - loss: 0.0 - 0.2566696579667521\n",
      "Epoch 1253 - loss: 0.0 - 0.2566431923242575\n",
      "Epoch 1254 - loss: 0.0 - 0.2566167293281949\n",
      "Epoch 1255 - loss: 0.0 - 0.2565902689782996\n",
      "Epoch 1256 - loss: 0.0 - 0.25656381127430705\n",
      "Epoch 1257 - loss: 0.0 - 0.25653735621595264\n",
      "Epoch 1258 - loss: 0.0 - 0.2565109038029719\n",
      "Epoch 1259 - loss: 0.0 - 0.25648445403510023\n",
      "Epoch 1261 - loss: 0.0 - 0.2564315624336261\n",
      "Epoch 1262 - loss: 0.0 - 0.2564051205994947\n",
      "Epoch 1263 - loss: 0.0 - 0.2563786814094148\n",
      "Epoch 1264 - loss: 0.0 - 0.25635224486312147\n",
      "Epoch 1265 - loss: 0.0 - 0.2563258109603508\n",
      "Epoch 1266 - loss: 0.0 - 0.25629937970083827\n",
      "Epoch 1267 - loss: 0.0 - 0.25627295108431947\n",
      "Epoch 1268 - loss: 0.0 - 0.25624652511053025\n",
      "Epoch 1269 - loss: 0.0 - 0.2562201017792064\n",
      "Epoch 1270 - loss: 0.0 - 0.25619368109008356\n",
      "Epoch 1271 - loss: 0.0 - 0.2561672630428975\n",
      "Epoch 1272 - loss: 0.0 - 0.2561408476373842\n",
      "Epoch 1273 - loss: 0.0 - 0.25611443487327934\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import math\n",
    "\n",
    "class VectorizedEvoClassifierM:\n",
    "    def __init__(self, n = 24, hidden_layers = False, activation = \"relu\", lr_target = 0.04, lr_initial_decay = 60, lr_final_decay = 0.03, random_state = None):\n",
    "\n",
    "        self.n = int(round(n / 8) * 8)\n",
    "        self.validation_loss_history = []\n",
    "        self.training_loss_history = []\n",
    "        self.random_state = random_state\n",
    "        self.activation = activation\n",
    "        self.lr_target = lr_target\n",
    "        self.lr_initial_decay = lr_initial_decay\n",
    "        self.lr_final_decay = lr_final_decay\n",
    "\n",
    "        \n",
    "        if hidden_layers:\n",
    "            self.hidden_layers = hidden_layers\n",
    "        else:\n",
    "            self.hidden_layers = False\n",
    "\n",
    "        \n",
    "\n",
    "    def fit(self, X_train, y_train, epochs = 100, validation_data = False, verbose = 0):\n",
    "\n",
    "        n = self.n\n",
    "        ndiv4 = n // 4\n",
    "\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "        y_train = y_train.astype(\"int8\")\n",
    "\n",
    "        if len(y_train.shape) == 1:\n",
    "            self.multiclass = False\n",
    "        elif len(y_train.shape) == 2 and y_train.shape[1] == 1:\n",
    "            self.multiclass = False\n",
    "            y_train = y_train.ravel()\n",
    "        else:\n",
    "            self.multiclass = True\n",
    "            \n",
    "\n",
    "        if validation_data:\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        else:\n",
    "            activation_function = lambda x: np.maximum(0, x)\n",
    "\n",
    "        if self.multiclass == True:\n",
    "            output_activation_function = lambda x: np.exp(x) / np.sum(np.exp(x), axis = 2, keepdims = True)\n",
    "            \n",
    "            def loss_function(y_train, y_preds, sorted_indices):\n",
    "                return np.mean(np.sum(-y_train * np.log10(y_preds[sorted_indices[ndiv4:]]), axis = 2), axis = 1)\n",
    "\n",
    "        elif self.multiclass == False:\n",
    "            output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "            def loss_function(y_train, y_preds, sorted_indices):\n",
    "                return np.mean(np.abs(y_preds[sorted_indices[ndiv4:]] - y_train), axis = 1)\n",
    "\n",
    "        lr_target = self.lr_target\n",
    "        lr_initial_decay = self.lr_initial_decay\n",
    "        lr_final_decay = self.lr_final_decay\n",
    "\n",
    "        layers = [X_train.shape[1]]\n",
    "\n",
    "        if self.hidden_layers:\n",
    "            layers = [X_train.shape[1]] + self.hidden_layers\n",
    "\n",
    "        if self.multiclass == True:\n",
    "            layers = layers + [y_train.shape[1]]\n",
    "        elif self.multiclass == False:\n",
    "            layers = layers + [1]\n",
    "\n",
    "        number_of_layers_minus_one = len(layers) - 1\n",
    "        \n",
    "        if self.multiclass == True:\n",
    "            y_preds = np.zeros((n, y_train.shape[0], y_train.shape[1]))\n",
    "        elif self.multiclass == False:\n",
    "            y_preds = np.zeros((n, y_train.shape[0]))\n",
    "\n",
    "        nets_loss = np.zeros(n)\n",
    "        sorted_indices = np.arange(-(ndiv4), n, 1)\n",
    "\n",
    "        best_net_index = -1\n",
    "\n",
    "        weights = []\n",
    "\n",
    "        for i in range(number_of_layers_minus_one):\n",
    "            weights += [np.random.normal(0, 1, (n, layers[i], layers[i + 1]))]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            forward_pass = X_train.T\n",
    "            \n",
    "            for j in range(number_of_layers_minus_one - 1):\n",
    "                forward_pass = activation_function(weights[j][sorted_indices[ndiv4:]].transpose(0, 2, 1) @ forward_pass)\n",
    "            \n",
    "            forward_pass = weights[-1][sorted_indices[ndiv4:]].transpose(0, 2, 1) @ forward_pass\n",
    "            \n",
    "            y_preds[sorted_indices[ndiv4:]] = output_activation_function(forward_pass.transpose(0, 2, 1))\n",
    "\n",
    "            nets_loss[sorted_indices[ndiv4:]] = loss_function(y_train, y_preds, sorted_indices)\n",
    "\n",
    "            sorted_indices = np.argsort(nets_loss)\n",
    "            mutation_sigma = math.exp(-epoch / (epochs / (lr_initial_decay * math.log10(epochs + 1)))) + lr_final_decay * math.exp(-(epoch + 1) * (1 / (epochs))) + lr_target + (-0.036 * 10 * lr_final_decay)\n",
    "            #mutation_sigma = math.exp(-epoch / (epochs / (lr_initial_decay * math.log10(epochs + 1)))) + 0.08 * math.exp(-(epoch + 1) * (1 / (epochs + 1))) + 0.02\n",
    "            #mutation_sigma = math.exp(-epoch / (epochs / (lr_decay * math.log10(epochs + 1)))) + 0.02 * math.exp(-(epoch + 1) * (1 / (epochs + 1))) - 0.005\n",
    "            #mutation_sigma = 0.08 + 0.5 * 1 / math.exp(epoch / ((epochs + 1) / (60 * math.log10(epochs + 1))))\n",
    "\n",
    "            for j in range(number_of_layers_minus_one):\n",
    "                weights[j][sorted_indices[0 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[1 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[2 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[3 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[4 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[5 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "\n",
    "            if best_net_index != sorted_indices[0]:\n",
    "                best_net_index = sorted_indices[0]\n",
    "                self.training_loss_history += [nets_loss[best_net_index]]\n",
    "                \n",
    "\n",
    "                self.best_net_weights = []\n",
    "                for j in range(number_of_layers_minus_one):\n",
    "                    self.best_net_weights += [weights[j][best_net_index]]\n",
    "                \n",
    "                if validation_data:\n",
    "                    self.validation_loss_history += [np.mean(np.abs(y_val - self.predict(X_val)))]\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]}\")\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        pass\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - {mutation_sigma}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        else:\n",
    "            activation_function = lambda x: np.maximum(0, x)\n",
    "\n",
    "        if self.multiclass == True:\n",
    "            output_activation_function = lambda x: np.exp(x) / np.sum(np.exp(x), axis = 1, keepdims = True)\n",
    "        elif self.multiclass == False:\n",
    "            output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "        forward_pass = X.T\n",
    "        for j in range(len(self.best_net_weights) - 1):\n",
    "            forward_pass = activation_function(self.best_net_weights[j].T @ forward_pass)\n",
    "\n",
    "        forward_pass = self.best_net_weights[-1].T @ forward_pass\n",
    "        \n",
    "        return output_activation_function(forward_pass.T)\n",
    "\n",
    "evomodel = VectorizedEvoClassifierM(n = 24 * 4, hidden_layers = [10], lr_target = 0.1, lr_initial_decay = 60, lr_final_decay = 0.3, random_state = 42)\n",
    "evomodel.fit(X, y, epochs = 10000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 0.9998311820346312 - 1.29197000149995\n",
      "Epoch 7 - loss: 0.9493318441214512 - 1.137112388781039\n",
      "Epoch 8 - loss: 0.945449826545742 - 1.1170352695971997\n",
      "Epoch 10 - loss: 0.9084038714228551 - 1.0782959928363374\n",
      "Epoch 11 - loss: 0.7883575093520281 - 1.0596115544075175\n",
      "Epoch 18 - loss: 0.7825842323168328 - 0.9406368729628037\n",
      "Epoch 21 - loss: 0.7821526861229309 - 0.8954468027447978\n",
      "Epoch 23 - loss: 0.772074035496816 - 0.8670744764788247\n",
      "Epoch 26 - loss: 0.6475596629216347 - 0.826984420356625\n",
      "Epoch 28 - loss: 0.6458369904993683 - 0.8018137178058867\n",
      "Epoch 30 - loss: 0.5164098736104002 - 0.7778198910879496\n",
      "Epoch 39 - loss: 0.5056881722204376 - 0.6829918872536017\n",
      "Epoch 42 - loss: 0.5033540862274465 - 0.6556569221318098\n",
      "Epoch 45 - loss: 0.4994401150710219 - 0.6302147129256686\n",
      "Epoch 49 - loss: 0.497508282466169 - 0.5990101199070317\n",
      "Epoch 50 - loss: 0.47200657881450925 - 0.5916641828003322\n",
      "Epoch 61 - loss: 0.4538528599723891 - 0.5214512626607359\n",
      "Epoch 66 - loss: 0.4489289979809142 - 0.4951460338805037\n",
      "Epoch 68 - loss: 0.4424817259673976 - 0.4854717671017367\n",
      "Epoch 70 - loss: 0.43121880414178854 - 0.4762481203465925\n",
      "Epoch 73 - loss: 0.416562734221192 - 0.46321163791183306\n",
      "Epoch 85 - loss: 0.35067370023627187 - 0.41945689331143376\n",
      "Epoch 89 - loss: 0.3477902485898014 - 0.4074357791815596\n",
      "Epoch 90 - loss: 0.34055668816873674 - 0.4046048005185673\n",
      "Epoch 93 - loss: 0.3382393621474246 - 0.3965042003208088\n",
      "Epoch 94 - loss: 0.33771431316003686 - 0.39392965127748114\n",
      "Epoch 95 - loss: 0.3278251528623542 - 0.3914154547190374\n",
      "Epoch 97 - loss: 0.32216732061269615 - 0.3865624283500497\n",
      "Epoch 106 - loss: 0.3126515267664953 - 0.3673565071492032\n",
      "Epoch 110 - loss: 0.3108052378970108 - 0.3600476375365833\n",
      "Epoch 116 - loss: 0.30302180331350903 - 0.35029943502390726\n",
      "Epoch 122 - loss: 0.29268412948968403 - 0.34183483903164946\n",
      "Epoch 127 - loss: 0.2897307803062248 - 0.3356366382046472\n",
      "Epoch 128 - loss: 0.28647910014209815 - 0.33448171471288113\n",
      "Epoch 131 - loss: 0.2828326230103581 - 0.33117456240647547\n",
      "Epoch 133 - loss: 0.28200911369600745 - 0.32909499964842304\n",
      "Epoch 139 - loss: 0.27806360169637634 - 0.32340696411646064\n",
      "Epoch 140 - loss: 0.2765691423408227 - 0.3225336736950033\n",
      "Epoch 153 - loss: 0.27651324391820964 - 0.3128399464118842\n",
      "Epoch 154 - loss: 0.2756940647279143 - 0.31220747462156984\n",
      "Epoch 155 - loss: 0.2754778098897267 - 0.31158930403648777\n",
      "Epoch 161 - loss: 0.2742205887541615 - 0.3081621698545043\n",
      "Epoch 169 - loss: 0.27393997418701566 - 0.30426049616603335\n",
      "Epoch 172 - loss: 0.27359381289989265 - 0.3029689902850127\n",
      "Epoch 179 - loss: 0.26934251207692317 - 0.300270616277696\n",
      "Epoch 181 - loss: 0.26846320070758967 - 0.2995732598218599\n",
      "Epoch 184 - loss: 0.2659167571585204 - 0.29858290872936644\n",
      "Epoch 189 - loss: 0.2499538953964917 - 0.29706946663180855\n",
      "Epoch 190 - loss: 0.24099708768870176 - 0.29678591577378954\n",
      "Epoch 191 - loss: 0.23486978602152062 - 0.2965083941267387\n",
      "Epoch 192 - loss: 0.20194605652060996 - 0.29623675878016686\n",
      "Epoch 194 - loss: 0.19131537635318124 - 0.2957105922113845\n",
      "Epoch 197 - loss: 0.18847543975024433 - 0.29496210747318363\n",
      "Epoch 200 - loss: 0.17401173132386386 - 0.29425951568766684\n",
      "Epoch 201 - loss: 0.17207413197167845 - 0.29403495960465964\n",
      "Epoch 202 - loss: 0.16184801687644096 - 0.29381503446255075\n",
      "Epoch 206 - loss: 0.15666653632056524 - 0.29297948695008214\n",
      "Epoch 208 - loss: 0.15555552236628709 - 0.29258676607284995\n",
      "Epoch 209 - loss: 0.15229712414264515 - 0.2923963259939727\n",
      "Epoch 212 - loss: 0.14772795153461693 - 0.291847580295844\n",
      "Epoch 214 - loss: 0.14753812381003112 - 0.2914996827085775\n",
      "Epoch 215 - loss: 0.13797837153235215 - 0.29133086086229665\n",
      "Epoch 216 - loss: 0.13451466263162304 - 0.2911653492162162\n",
      "Epoch 219 - loss: 0.13257828163141877 - 0.29068790042915404\n",
      "Epoch 223 - loss: 0.1308840796610969 - 0.290093094013796\n",
      "Epoch 224 - loss: 0.13050046835992576 - 0.28995139127611524\n",
      "Epoch 225 - loss: 0.12972505114922453 - 0.28981235624504265\n",
      "Epoch 226 - loss: 0.12047595942969935 - 0.28967592572639833\n",
      "Epoch 230 - loss: 0.12001330629838734 - 0.28915503660356534\n",
      "Epoch 232 - loss: 0.11850516507798224 - 0.2889086829190174\n",
      "Epoch 234 - loss: 0.1045344992612603 - 0.28867113922963084\n",
      "Epoch 246 - loss: 0.10272873678061116 - 0.2874090987062806\n",
      "Epoch 264 - loss: 0.09590907250588888 - 0.2859256648832146\n",
      "Epoch 278 - loss: 0.09395082415976769 - 0.28501145594689914\n",
      "Epoch 281 - loss: 0.09377847819624584 - 0.2848360123461836\n",
      "Epoch 287 - loss: 0.09375364609697777 - 0.28450310559165704\n",
      "Epoch 292 - loss: 0.09337044416033959 - 0.2842420721586656\n",
      "Epoch 293 - loss: 0.09278818751628502 - 0.2841914889381585\n",
      "Epoch 294 - loss: 0.09255447790591628 - 0.28414141732964526\n",
      "Epoch 296 - loss: 0.09199497746833142 - 0.28404276097926495\n",
      "Epoch 300 - loss: 0.08531517350574454 - 0.2838510756048839\n",
      "Epoch 308 - loss: 0.08481630882329047 - 0.28348787059256264\n",
      "Epoch 318 - loss: 0.08339452254865942 - 0.2830656816219028\n",
      "Epoch 319 - loss: 0.0810852048048722 - 0.2830251317093292\n",
      "Epoch 331 - loss: 0.08058677646736524 - 0.28255827526860133\n",
      "Epoch 332 - loss: 0.07192682825938757 - 0.2825208435402259\n",
      "Epoch 339 - loss: 0.06764876887300363 - 0.2822642316117564\n",
      "Epoch 343 - loss: 0.06319832323522355 - 0.28212146602304466\n",
      "Epoch 349 - loss: 0.05997728125635064 - 0.2819119333557869\n",
      "Epoch 351 - loss: 0.05815085867140035 - 0.2818432090234706\n",
      "Epoch 355 - loss: 0.04533895536049003 - 0.28170728901265973\n",
      "Epoch 382 - loss: 0.03900950705205968 - 0.2808315656656267\n",
      "Epoch 384 - loss: 0.03859017334952672 - 0.2807689371090166\n",
      "Epoch 386 - loss: 0.020166927511282746 - 0.2807065492249322\n",
      "Epoch 401 - loss: 0.01648711707451175 - 0.2802453058327494\n",
      "Epoch 405 - loss: 0.008792068892168471 - 0.28012400518070146\n",
      "Epoch 407 - loss: 0.006887931044233192 - 0.280063583167096\n",
      "Epoch 410 - loss: 0.0066190071186247625 - 0.27997321716988566\n",
      "Epoch 411 - loss: 0.003987557649115078 - 0.27994316330267854\n",
      "Epoch 416 - loss: 0.002420511318634888 - 0.2797933725687788\n",
      "Epoch 422 - loss: 0.0012615694763107566 - 0.2796145897244996\n",
      "Epoch 448 - loss: 0.0011253705857586155 - 0.2788493259705622\n",
      "Epoch 451 - loss: 0.0004226458153368725 - 0.2787618038400437\n",
      "Epoch 464 - loss: 0.000311481344377273 - 0.27838394436508507\n",
      "Epoch 467 - loss: 0.00027668825776446036 - 0.2782970338342181\n",
      "Epoch 479 - loss: 0.00013634889517272604 - 0.2779503055490482\n",
      "Epoch 481 - loss: 8.96956060844415e-05 - 0.27789264663271906\n",
      "Epoch 484 - loss: 7.232293614919244e-05 - 0.2778062212515685\n",
      "Epoch 500 - loss: 5.803475067458429e-05 - 0.27734643530620195\n",
      "Epoch 504 - loss: 5.355812579601346e-05 - 0.27723175966455205\n",
      "Epoch 508 - loss: 5.166264649887683e-05 - 0.27711718113479933\n",
      "Epoch 518 - loss: 4.9151700683340875e-05 - 0.27683112968483437\n",
      "Epoch 525 - loss: 3.45479181014971e-05 - 0.27663120366433047\n",
      "Epoch 531 - loss: 3.366135477605386e-05 - 0.27646002601656705\n",
      "Epoch 533 - loss: 2.470918191263677e-05 - 0.27640300346214064\n",
      "Epoch 538 - loss: 2.2130368478013337e-05 - 0.2762605242418109\n",
      "Epoch 540 - loss: 1.2548599093776857e-05 - 0.2762035626526509\n",
      "Epoch 550 - loss: 6.5856080969289985e-06 - 0.2759190015872939\n",
      "Epoch 553 - loss: 5.494818195344873e-06 - 0.275833710674382\n",
      "Epoch 558 - loss: 3.195475679643043e-06 - 0.27569163545138387\n",
      "Epoch 572 - loss: 1.0275177020412378e-06 - 0.275294311385586\n",
      "Epoch 590 - loss: 1.6498959924541604e-07 - 0.2747844594311778\n",
      "Epoch 616 - loss: 7.471627302954734e-08 - 0.2740498477819908\n",
      "Epoch 631 - loss: 5.1865513711392926e-08 - 0.27362697596512897\n",
      "Epoch 643 - loss: 3.250349047222073e-08 - 0.27328916032977846\n",
      "Epoch 653 - loss: 1.931183942150221e-08 - 0.27300796959774676\n",
      "Epoch 661 - loss: 1.4521874184144837e-08 - 0.272783225959347\n",
      "Epoch 667 - loss: 9.191114789420833e-09 - 0.2726147893437427\n",
      "Epoch 673 - loss: 8.89875557404683e-09 - 0.27244645606801143\n",
      "Epoch 676 - loss: 8.075903839003031e-09 - 0.2723623280647893\n",
      "Epoch 688 - loss: 8.037203932780033e-09 - 0.27202607265366674\n",
      "Epoch 689 - loss: 3.782748077959326e-09 - 0.27199806985416153\n",
      "Epoch 697 - loss: 3.55758778244497e-09 - 0.2717741495261311\n",
      "Epoch 700 - loss: 1.9626541741058253e-09 - 0.2716902261118229\n",
      "Epoch 709 - loss: 1.1834931220773132e-09 - 0.2714386083730097\n",
      "Epoch 716 - loss: 1.0737783053205083e-09 - 0.2712430635226073\n",
      "Epoch 733 - loss: 5.336788578214688e-10 - 0.2707687421141589\n",
      "Epoch 741 - loss: 1.0380240022413125e-10 - 0.2705458123207293\n",
      "Epoch 787 - loss: 4.047676437409286e-11 - 0.2692674315391168\n",
      "Epoch 827 - loss: 3.5970448971078684e-11 - 0.2681605731563417\n",
      "Epoch 851 - loss: 1.3959972604663612e-11 - 0.2674985814426485\n",
      "Epoch 852 - loss: 4.698637364493006e-12 - 0.26747103293010543\n",
      "Epoch 916 - loss: 4.5240634099520215e-12 - 0.2657136469250153\n",
      "Epoch 925 - loss: 4.143057627871045e-12 - 0.2654674154087652\n",
      "Epoch 934 - loss: 1.6150600242967052e-12 - 0.2652214054118307\n",
      "Epoch 954 - loss: 1.0792884504895953e-12 - 0.26467550860993894\n",
      "Epoch 977 - loss: 1.0674842056397354e-12 - 0.2640490755662046\n",
      "Epoch 995 - loss: 4.557280107743786e-13 - 0.2635598276624995\n",
      "Epoch 1000 - loss: 3.4092920918708105e-13 - 0.2634240816831992\n",
      "Epoch 1022 - loss: 8.705685369646029e-14 - 0.262827605052952\n",
      "Epoch 1035 - loss: 4.957081508751834e-14 - 0.2624757579106366\n",
      "Epoch 1052 - loss: 2.7733181276745968e-14 - 0.2620163397328472\n",
      "Epoch 1065 - loss: 2.4436934663798514e-14 - 0.2616655465532729\n",
      "Epoch 1076 - loss: 1.5560738664837213e-15 - 0.26136907753808425\n",
      "Epoch 1099 - loss: 1.034460373211717e-15 - 0.26075024059246543\n",
      "Epoch 1104 - loss: 5.698298665996698e-17 - 0.2606158990599562\n",
      "Epoch 1119 - loss: 2.191653333075653e-17 - 0.26021327725227716\n",
      "Epoch 1140 - loss: 1.7533226664605225e-17 - 0.2596506203656993\n",
      "Epoch 1163 - loss: 1.3149919998453918e-17 - 0.25903573133175706\n",
      "Epoch 1204 - loss: 8.766613332302611e-18 - 0.2579431262038957\n",
      "Epoch 1211 - loss: 8.766613332302611e-18 - 0.25775703115637505\n",
      "Epoch 1212 - loss: 8.766613332302611e-18 - 0.2577304567819946\n",
      "Epoch 1214 - loss: 0.0 - 0.2576773160048822\n",
      "Epoch 1217 - loss: 0.0 - 0.2575976247643491\n",
      "Epoch 1225 - loss: 0.0 - 0.2573852316330819\n",
      "Epoch 1226 - loss: 0.0 - 0.2573586944367965\n",
      "Epoch 1234 - loss: 0.0 - 0.25714649237336085\n",
      "Epoch 1235 - loss: 0.0 - 0.25711997904980854\n",
      "Epoch 1236 - loss: 0.0 - 0.25709346837745617\n",
      "Epoch 1237 - loss: 0.0 - 0.2570669603560385\n",
      "Epoch 1238 - loss: 0.0 - 0.2570404549852905\n",
      "Epoch 1239 - loss: 0.0 - 0.25701395226494705\n",
      "Epoch 1240 - loss: 0.0 - 0.2569874521947433\n",
      "Epoch 1241 - loss: 0.0 - 0.25696095477441405\n",
      "Epoch 1242 - loss: 0.0 - 0.2569344600036946\n",
      "Epoch 1243 - loss: 0.0 - 0.2569079678823196\n",
      "Epoch 1244 - loss: 0.0 - 0.2568814784100245\n",
      "Epoch 1245 - loss: 0.0 - 0.2568549915865441\n",
      "Epoch 1247 - loss: 0.0 - 0.2568020258849687\n",
      "Epoch 1248 - loss: 0.0 - 0.25677554700634375\n",
      "Epoch 1249 - loss: 0.0 - 0.2567490707754745\n",
      "Epoch 1250 - loss: 0.0 - 0.2567225971920959\n",
      "Epoch 1251 - loss: 0.0 - 0.2566961262559433\n",
      "Epoch 1252 - loss: 0.0 - 0.2566696579667521\n",
      "Epoch 1253 - loss: 0.0 - 0.2566431923242575\n",
      "Epoch 1254 - loss: 0.0 - 0.2566167293281949\n",
      "Epoch 1255 - loss: 0.0 - 0.2565902689782996\n",
      "Epoch 1256 - loss: 0.0 - 0.25656381127430705\n",
      "Epoch 1257 - loss: 0.0 - 0.25653735621595264\n",
      "Epoch 1258 - loss: 0.0 - 0.2565109038029719\n",
      "Epoch 1259 - loss: 0.0 - 0.25648445403510023\n",
      "Epoch 1261 - loss: 0.0 - 0.2564315624336261\n",
      "Epoch 1262 - loss: 0.0 - 0.2564051205994947\n",
      "Epoch 1263 - loss: 0.0 - 0.2563786814094148\n",
      "Epoch 1264 - loss: 0.0 - 0.25635224486312147\n",
      "Epoch 1265 - loss: 0.0 - 0.2563258109603508\n",
      "Epoch 1266 - loss: 0.0 - 0.25629937970083827\n",
      "Epoch 1267 - loss: 0.0 - 0.25627295108431947\n",
      "Epoch 1268 - loss: 0.0 - 0.25624652511053025\n",
      "Epoch 1269 - loss: 0.0 - 0.2562201017792064\n",
      "Epoch 1270 - loss: 0.0 - 0.25619368109008356\n",
      "Epoch 1271 - loss: 0.0 - 0.2561672630428975\n",
      "Epoch 1272 - loss: 0.0 - 0.2561408476373842\n",
      "Epoch 1273 - loss: 0.0 - 0.25611443487327934\n"
     ]
    }
   ],
   "source": [
    "evomodel = VectorizedEvoClassifierM(n = 24 * 4, hidden_layers = [10], lr_target = 0.1, lr_initial_decay = 60, lr_final_decay = 0.3, random_state = 42)\n",
    "evomodel.fit(X, y, epochs = 10000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(evomodel.training_loss_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 1.1082605459524975 - 0.58\n",
      "Epoch 2 - loss: 0.9543084041914337 - 0.12621184586265957\n",
      "Epoch 3 - loss: 0.9359153862388222 - 0.09404898574986237\n",
      "Epoch 4 - loss: 0.9268551659953826 - 0.08427106939606842\n",
      "Epoch 5 - loss: 0.9114848113842181 - 0.08129845912799868\n",
      "Epoch 6 - loss: 0.909553585999841 - 0.08039474800119967\n",
      "Epoch 7 - loss: 0.8952747188709296 - 0.08012000838616408\n",
      "Epoch 8 - loss: 0.8915235571537253 - 0.08003648406757206\n",
      "Epoch 9 - loss: 0.8545270712003362 - 0.08001109161808728\n",
      "Epoch 10 - loss: 0.8381145037108438 - 0.08000337199221416\n",
      "Epoch 12 - loss: 0.8112831889965421 - 0.0800003116519689\n",
      "Epoch 13 - loss: 0.7953168669221099 - 0.08000009474614113\n",
      "Epoch 14 - loss: 0.7923423801548026 - 0.0800000288040255\n",
      "Epoch 15 - loss: 0.7862474367635817 - 0.08000000875678814\n",
      "Epoch 17 - loss: 0.7696366769355939 - 0.08000000080933468\n",
      "Epoch 18 - loss: 0.7672391522298984 - 0.08000000024604799\n",
      "Epoch 20 - loss: 0.7523209175194037 - 0.08000000002274066\n",
      "Epoch 21 - loss: 0.7466652546095311 - 0.08000000000691346\n",
      "Epoch 23 - loss: 0.7255198958173092 - 0.08000000000063896\n",
      "Epoch 25 - loss: 0.7058590080259738 - 0.08000000000005905\n",
      "Epoch 27 - loss: 0.6990665619423027 - 0.08000000000000546\n",
      "Epoch 28 - loss: 0.6909209930196192 - 0.08000000000000167\n",
      "Epoch 29 - loss: 0.6846982419927229 - 0.0800000000000005\n",
      "Epoch 30 - loss: 0.674919506517508 - 0.08000000000000015\n",
      "Epoch 32 - loss: 0.6479022902974371 - 0.08000000000000002\n",
      "Epoch 35 - loss: 0.631369541177049 - 0.08\n",
      "Epoch 36 - loss: 0.6293241691401025 - 0.08\n",
      "Epoch 38 - loss: 0.6196026702235303 - 0.08\n",
      "Epoch 39 - loss: 0.6105273285010863 - 0.08\n",
      "Epoch 41 - loss: 0.6024983494061142 - 0.08\n",
      "Epoch 43 - loss: 0.5942257296681462 - 0.08\n",
      "Epoch 47 - loss: 0.5781253191164887 - 0.08\n",
      "Epoch 49 - loss: 0.5692478664922944 - 0.08\n",
      "Epoch 50 - loss: 0.56542633174807 - 0.08\n",
      "Epoch 53 - loss: 0.5617568170956522 - 0.08\n",
      "Epoch 54 - loss: 0.5503299209081821 - 0.08\n",
      "Epoch 55 - loss: 0.5496500443638271 - 0.08\n",
      "Epoch 57 - loss: 0.5475617828626654 - 0.08\n",
      "Epoch 58 - loss: 0.5312255604422151 - 0.08\n",
      "Epoch 62 - loss: 0.5297412325314602 - 0.08\n",
      "Epoch 65 - loss: 0.5219499648331954 - 0.08\n",
      "Epoch 67 - loss: 0.5184394642983349 - 0.08\n",
      "Epoch 68 - loss: 0.5066343521476693 - 0.08\n",
      "Epoch 72 - loss: 0.5066191279926724 - 0.08\n",
      "Epoch 73 - loss: 0.5037985618795499 - 0.08\n",
      "Epoch 74 - loss: 0.49585442281713754 - 0.08\n",
      "Epoch 75 - loss: 0.48832142343879464 - 0.08\n",
      "Epoch 76 - loss: 0.48163768517974453 - 0.08\n",
      "Epoch 77 - loss: 0.4784238699194795 - 0.08\n",
      "Epoch 79 - loss: 0.47766020371593876 - 0.08\n",
      "Epoch 81 - loss: 0.46361843667876185 - 0.08\n",
      "Epoch 85 - loss: 0.44457238455768305 - 0.08\n",
      "Epoch 89 - loss: 0.43580101119360626 - 0.08\n",
      "Epoch 91 - loss: 0.43537983367190997 - 0.08\n",
      "Epoch 92 - loss: 0.4336667785074875 - 0.08\n",
      "Epoch 94 - loss: 0.4316411029423816 - 0.08\n",
      "Epoch 96 - loss: 0.429176255682667 - 0.08\n",
      "Epoch 97 - loss: 0.4254703051829105 - 0.08\n",
      "Epoch 98 - loss: 0.4247034871900297 - 0.08\n",
      "Epoch 99 - loss: 0.423748811424556 - 0.08\n"
     ]
    }
   ],
   "source": [
    "evomodel = VectorizedEvoClassifierM01(n = 48, hidden_layers = [10], random_state = 42)\n",
    "evomodel.fit(X, y, epochs = 100, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AxisError",
     "evalue": "axis 2 is out of bounds for array of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/multi_class.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/multi_class.ipynb#ch0000011?line=0'>1</a>\u001b[0m y_pred \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/multi_class.ipynb#ch0000011?line=2'>3</a>\u001b[0m y_pred\n",
      "File \u001b[0;32m~/Documents/GitHub/Deep-learning-Daniel-Petersson/evolutionary_algos.py:281\u001b[0m, in \u001b[0;36mEvoMLPClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/evolutionary_algos.py?line=276'>277</a>\u001b[0m     forward_pass \u001b[39m=\u001b[39m activation_function(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_net_weights[j]\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m forward_pass)\n\u001b[1;32m    <a href='file:///Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/evolutionary_algos.py?line=278'>279</a>\u001b[0m forward_pass \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_net_weights[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m forward_pass\n\u001b[0;32m--> <a href='file:///Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/evolutionary_algos.py?line=280'>281</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output_activation_function(forward_pass\u001b[39m.\u001b[39;49mT)\n",
      "File \u001b[0;32m~/Documents/GitHub/Deep-learning-Daniel-Petersson/evolutionary_algos.py:269\u001b[0m, in \u001b[0;36mEvoMLPClassifier.predict.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/evolutionary_algos.py?line=265'>266</a>\u001b[0m     activation_function \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: np\u001b[39m.\u001b[39mmaximum(\u001b[39m0\u001b[39m, x)\n\u001b[1;32m    <a href='file:///Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/evolutionary_algos.py?line=267'>268</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulticlass \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/evolutionary_algos.py?line=268'>269</a>\u001b[0m     output_activation_function \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: np\u001b[39m.\u001b[39mexp(x) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39;49msum(np\u001b[39m.\u001b[39;49mexp(x), axis \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m, keepdims \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='file:///Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/evolutionary_algos.py?line=270'>271</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulticlass \u001b[39m==\u001b[39m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/dp/Documents/GitHub/Deep-learning-Daniel-Petersson/evolutionary_algos.py?line=271'>272</a>\u001b[0m     output_activation_function \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mx)))\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[:\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Deep-learning-Daniel-Petersson-bXusHwTH/lib/python3.9/site-packages/numpy/core/fromnumeric.py:2296\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/dp/.local/share/virtualenvs/Deep-learning-Daniel-Petersson-bXusHwTH/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=2292'>2293</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m   <a href='file:///Users/dp/.local/share/virtualenvs/Deep-learning-Daniel-Petersson-bXusHwTH/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=2293'>2294</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[0;32m-> <a href='file:///Users/dp/.local/share/virtualenvs/Deep-learning-Daniel-Petersson-bXusHwTH/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=2295'>2296</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _wrapreduction(a, np\u001b[39m.\u001b[39;49madd, \u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims,\n\u001b[1;32m   <a href='file:///Users/dp/.local/share/virtualenvs/Deep-learning-Daniel-Petersson-bXusHwTH/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=2296'>2297</a>\u001b[0m                       initial\u001b[39m=\u001b[39;49minitial, where\u001b[39m=\u001b[39;49mwhere)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/Deep-learning-Daniel-Petersson-bXusHwTH/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/dp/.local/share/virtualenvs/Deep-learning-Daniel-Petersson-bXusHwTH/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=82'>83</a>\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/dp/.local/share/virtualenvs/Deep-learning-Daniel-Petersson-bXusHwTH/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=83'>84</a>\u001b[0m             \u001b[39mreturn\u001b[39;00m reduction(axis\u001b[39m=\u001b[39maxis, out\u001b[39m=\u001b[39mout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpasskwargs)\n\u001b[0;32m---> <a href='file:///Users/dp/.local/share/virtualenvs/Deep-learning-Daniel-Petersson-bXusHwTH/lib/python3.9/site-packages/numpy/core/fromnumeric.py?line=85'>86</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ufunc\u001b[39m.\u001b[39;49mreduce(obj, axis, dtype, out, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpasskwargs)\n",
      "\u001b[0;31mAxisError\u001b[0m: axis 2 is out of bounds for array of dimension 2"
     ]
    }
   ],
   "source": [
    "y_pred = classifier.predict(X)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 10)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "a = np.array([\n",
    "    [[15.48941891, -2.09846987, -8.58431883, 0.92524371, 4.67316101, 15.6263859, 12.23011818, 10.22337356, -5.97467029, 0.7186], \n",
    "    [-1.67894923, 4.82557838, -4.92448585, 1.07249996, -4.52925044, 20.74225489, 6.74903597, 15.98542702, -3.82235716, 1.74474188]],\n",
    "    \n",
    "    [[15.48941891, -2.09846987, -8.58431883, 0.92524371, 4.67316101, 15.6263859, 12.23011818, 10.22337356, -5.97467029, 0.7186], \n",
    "    [-1.67894923, 4.82557838, -4.92448585, 1.07249996, -4.52925044, 20.74225489, 6.74903597, 15.98542702, -3.82235716, 1.74474188]]\n",
    "])\n",
    "\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_function = lambda x: np.exp(x) / np.sum(np.exp(x), axis = 2, keepdims = True)\n",
    "\n",
    "test = lambda x: np.sum(np.exp(x), axis = 2)\n",
    "\n",
    "b = activation_function(a)\n",
    "b\n",
    "\n",
    "bt = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.04082153, 5.04082153])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.sum(-bt * np.log10(b), axis = 2), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47d3b7ff548c1bae2d6b155a9b3d6f1122689b634566f833764ba5dd9fcfa2e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Deep-learning-Daniel-Petersson-bXusHwTH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
