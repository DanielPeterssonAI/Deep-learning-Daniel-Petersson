{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Disease prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import recall_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "folder = \"kaggle-cardio/\"\n",
    "cardio_file = \"cardio_train.csv\"\n",
    "\n",
    "df_cardio = pd.read_csv(f\"{folder}{cardio_file}\", sep = \";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cardio[\"bmi\"] = df_cardio[\"weight\"] / (df_cardio[\"height\"] / 100) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_column(df, col_name, min, max):\n",
    "    return df[(df[col_name] > min) & (df[col_name] < max)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cardio = plot_column(df_cardio, \"bmi\", min = 15, max = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cardio[\"bmi_category\"] = pd.cut(\n",
    "    df_cardio[\"bmi\"], \n",
    "    bins = [0, 25, 30, 35, 40, 1000], \n",
    "    labels = [\"Normal\", \"Overweight\", \"Obese (Class I)\", \"Obese (Class II)\", \"Obese (Class III)\"], \n",
    "    right = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cardio = plot_column(df_cardio, \"ap_hi\", min = 75, max = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cardio = plot_column(df_cardio, \"ap_lo\", min = 50, max = 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_categorizer(hi, lo):\n",
    "    '''ap_categorizer() takes blood two pressure values as arguments and returns the blood pressure category'''\n",
    "\n",
    "    if hi >= 180 or lo >= 120: return \"Hypertension crisis\"\n",
    "    if hi >= 140 or lo >= 90: return \"Stage 2 hypertension\"\n",
    "    if hi < 120 and lo < 80: return \"Healthy\"\n",
    "    if hi < 130 and lo < 80: return \"Elevated\"\n",
    "    return \"Stage 1 hypertension\"\n",
    "\n",
    "df_cardio[\"ap_category\"] = df_cardio.apply(lambda x: ap_categorizer(x[\"ap_hi\"], x[\"ap_lo\"]), axis = 1).astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cardio_first = df_cardio.drop([\"ap_hi\", \"ap_lo\", \"height\", \"weight\", \"bmi\"], axis = 1)\n",
    "df_cardio_first = pd.get_dummies(df_cardio_first, columns = [\"bmi_category\", \"ap_category\", \"gender\"], drop_first = True)\n",
    "\n",
    "df_cardio_second = df_cardio.drop([\"bmi_category\", \"ap_category\", \"height\", \"weight\"], axis = 1)\n",
    "df_cardio_second = pd.get_dummies(df_cardio_second, columns = [\"gender\"], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>ap_hi</th>\n",
       "      <th>ap_lo</th>\n",
       "      <th>cholesterol</th>\n",
       "      <th>gluc</th>\n",
       "      <th>smoke</th>\n",
       "      <th>alco</th>\n",
       "      <th>active</th>\n",
       "      <th>cardio</th>\n",
       "      <th>bmi</th>\n",
       "      <th>gender_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>18393</td>\n",
       "      <td>110</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>21.967120</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>20228</td>\n",
       "      <td>140</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>34.927679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>18857</td>\n",
       "      <td>130</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23.507805</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17623</td>\n",
       "      <td>150</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28.710479</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>17474</td>\n",
       "      <td>100</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.011177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id    age  ap_hi  ap_lo  cholesterol  gluc  smoke  alco  active  cardio  \\\n",
       "0   0  18393    110     80            1     1      0     0       1       0   \n",
       "1   1  20228    140     90            3     1      0     0       1       1   \n",
       "2   2  18857    130     70            3     1      0     0       0       1   \n",
       "3   3  17623    150    100            1     1      0     0       1       1   \n",
       "4   4  17474    100     60            1     1      0     0       0       0   \n",
       "\n",
       "         bmi  gender_2  \n",
       "0  21.967120         1  \n",
       "1  34.927679         0  \n",
       "2  23.507805         0  \n",
       "3  28.710479         1  \n",
       "4  23.011177         0  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cardio_second.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tvt_split(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size = 0.5, random_state = 42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 42)\n",
    "\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = tvt_split(df_cardio_first.drop([\"id\", \"cardio\"], axis = 1), df_cardio_first[\"cardio\"])\n",
    "#datasets[\"first\"] = {\"X_train\": X_train, \"X_val\": X_val, \"X_test\": X_test, \"y_train\": y_train, \"y_val\": y_val, \"y_test\": y_test}\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = tvt_split(df_cardio_second.drop([\"id\", \"cardio\"], axis = 1), df_cardio_second[\"cardio\"])\n",
    "#datasets[\"second\"] = {\"X_train\": X_train, \"X_val\": X_val, \"X_test\": X_test, \"y_train\": y_train, \"y_val\": y_val, \"y_test\": y_test}\n",
    "\n",
    "# To minimize risk of bugs these are deleted\n",
    "#X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvoClassifier:\n",
    "    def __init__(self, n = 100, hidden_layers = False, activation = \"sigmoid\", random_state = None):\n",
    "\n",
    "        self.n = n // 2 * 2\n",
    "        self.nets = []\n",
    "        self.best_net = -1\n",
    "        self.best_result = None\n",
    "        self.validation_loss_history = []\n",
    "        self.training_loss_history = []\n",
    "        self.mutation_sigma = 0\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif activation == \"relu\":\n",
    "            self.activation_function = lambda x: np.maximum(0, x)\n",
    "        elif activation == \"leaky_relu\":\n",
    "            self.activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "\n",
    "        self.output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        if hidden_layers:\n",
    "            self.layers = hidden_layers + [1]\n",
    "        else:\n",
    "            self.layers = [1]\n",
    "        \n",
    "        if random_state != None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs = 100, validation_data = False, verbose = 0):\n",
    "        X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "\n",
    "        if validation_data:\n",
    "            X_val, y_val = validation_data\n",
    "        \n",
    "        self.layers = [X_train.shape[1]] + self.layers\n",
    "\n",
    "        for i in range(self.n):\n",
    "            self.nets += [[]]\n",
    "            for j in range(len(self.layers) - 1):\n",
    "                self.nets[i] += [np.random.uniform(-3, 3, (self.layers[j], self.layers[j + 1]))]\n",
    "\n",
    "        self.y_preds = np.zeros((len(self.nets), y_train.shape[0]))\n",
    "        self.nets_loss = np.zeros(len(self.nets))\n",
    "        self.sorted_indecies = np.zeros(len(self.nets))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(self.nets)):\n",
    "                forward_pass = X_train.T\n",
    "\n",
    "                for j in range(0, len(self.layers) - 2):\n",
    "                    forward_pass = self.activation_function(self.nets[i][j].T @ forward_pass)\n",
    "\n",
    "                forward_pass = self.nets[i][-1].T @ forward_pass\n",
    "                forward_pass = self.output_activation_function(forward_pass)\n",
    "                \n",
    "                self.y_preds[i] = forward_pass.reshape(-1)\n",
    "            \n",
    "            self.nets_loss = np.mean(np.abs(self.y_preds - y_train), axis = 1)\n",
    "            \n",
    "            self.sorted_indecies = np.argsort(self.nets_loss)\n",
    "            \n",
    "            self.mutation_sigma = 0.1 + 5 * 1 / math.exp(epoch / (epochs / (10 * math.log10(epochs + 1))))\n",
    "            \n",
    "            for i in range(0, self.n // 2, 2):\n",
    "                for j in range(len(self.layers) - 1):\n",
    "                    self.nets[self.sorted_indecies[self.n // 2 + i]][j] = (self.nets[self.sorted_indecies[i]][j] + self.nets[self.sorted_indecies[1 + i]][j]) / 2 + np.random.normal(0, self.mutation_sigma, (self.layers[j], self.layers[j + 1]))\n",
    "                    self.nets[self.sorted_indecies[self.n // 2 + 1 + i]][j] = (self.nets[self.sorted_indecies[i]][j] + self.nets[self.sorted_indecies[1 + i]][j]) / 2 + np.random.normal(0, self.mutation_sigma, (self.layers[j], self.layers[j + 1]))\n",
    "\n",
    "            if self.best_net != self.sorted_indecies[0]:\n",
    "                self.best_net = self.sorted_indecies[0]\n",
    "                self.training_loss_history += [self.nets_loss[self.best_net]]\n",
    "\n",
    "                if validation_data:\n",
    "                    self.validation_loss_history += [np.mean(np.abs(y_val - self.predict(X_val)))]\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]}\")\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        forward_pass = X.T\n",
    "        for j in range(0, len(self.layers) - 2):\n",
    "            forward_pass = self.activation_function(self.nets[self.best_net][j].T @ forward_pass)\n",
    "\n",
    "        forward_pass = self.output_activation_function(self.nets[self.best_net][-1].T @ forward_pass)\n",
    "\n",
    "\n",
    "        return forward_pass.reshape(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 0.36403634868104445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xz/f2gwbn5n3vs4pz044n49z3cw0000gn/T/ipykernel_19235/1224193588.py:19: RuntimeWarning: overflow encountered in exp\n",
      "  self.output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - loss: 0.3603206474798351\n",
      "Epoch 4 - loss: 0.3346728007210599\n",
      "Epoch 6 - loss: 0.3341773126205522\n",
      "Epoch 7 - loss: 0.3298033393084469\n",
      "Epoch 8 - loss: 0.32455883626900633\n",
      "Epoch 9 - loss: 0.3191281428301951\n"
     ]
    }
   ],
   "source": [
    "start, stop = 37, 38\n",
    "\n",
    "classifier = EvoClassifier(n = 100, hidden_layers = [2], activation = \"relu\", random_state = 42)\n",
    "#classifier.fit(datasets[\"second\"][\"X_train\"][start:stop], datasets[\"second\"][\"y_train\"][start:stop], epochs = 2, verbose = 1)\n",
    "classifier.fit(X_train, y_train, epochs = 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "#y_pred.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.78      0.74      8683\n",
      "           1       0.75      0.66      0.70      8382\n",
      "\n",
      "    accuracy                           0.72     17065\n",
      "   macro avg       0.73      0.72      0.72     17065\n",
      "weighted avg       0.73      0.72      0.72     17065\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = (y_pred > 0.5) * 1\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5108256237659907\n",
      "0.2231435513142097\n",
      "1.6094379124341003\n",
      "0.35667494393873245\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "true = [1, 0, 1, 0]\n",
    "pred = [0.6, 0.2, 0.2, 0.3]\n",
    "\n",
    "for i in range(4):\n",
    "    loss = -(true[i] * math.log(pred[i]) + (1 - true[i]) * math.log(1 - pred[i]))\n",
    "\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6750205078632583"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array(true)\n",
    "p = np.array(pred)\n",
    "\n",
    "np.mean((y * np.log(p) + (1 - y) * np.log(1 - p))) * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedEvoClassifier:\n",
    "    def __init__(self, n = 100, hidden_layers = False, activation = \"sigmoid\", random_state = None):\n",
    "\n",
    "        self.n = n // 2 * 2\n",
    "        self.best_net = -1\n",
    "        self.best_result = None\n",
    "        self.validation_loss_history = []\n",
    "        self.training_loss_history = []\n",
    "        self.mutation_sigma = 0\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif activation == \"relu\":\n",
    "            self.activation_function = lambda x: np.maximum(0, x)\n",
    "        elif activation == \"leaky_relu\":\n",
    "            self.activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        \n",
    "        self.output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        if hidden_layers:\n",
    "            self.layers = hidden_layers + [1]\n",
    "        else:\n",
    "            self.layers = [1]\n",
    "        \n",
    "        if random_state != None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs = 100, validation_data = False, verbose = 0):\n",
    "        X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "\n",
    "        if validation_data:\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        self.layers = [X_train.shape[1]] + self.layers\n",
    "\n",
    "        self.y_preds = np.zeros((self.n, y_train.shape[0]))\n",
    "        self.nets_loss = np.zeros(self.n)\n",
    "        self.sorted_indecies = np.zeros(self.n)\n",
    "\n",
    "        self.weights = []\n",
    "\n",
    "        for i in range(len(self.layers) - 1):\n",
    "            self.weights += [np.random.uniform(-1, 1, (self.n, self.layers[i], self.layers[i + 1]))]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            forward_pass = X_train.T\n",
    "            \n",
    "            for j in range(0, len(self.layers) - 2):\n",
    "                forward_pass = self.activation_function(self.weights[j].transpose(0, 2, 1) @ forward_pass)\n",
    "\n",
    "            forward_pass = self.output_activation_function(self.weights[-1].transpose(0, 2, 1) @ forward_pass)\n",
    "            \n",
    "            self.y_preds = forward_pass.reshape(self.n, -1)\n",
    "\n",
    "            self.nets_loss = np.mean(np.abs(self.y_preds - y_train), axis = 1)\n",
    "            \n",
    "            \n",
    "            self.sorted_indecies = np.argsort(self.nets_loss)\n",
    "\n",
    "            self.mutation_sigma = 0.1 + 5 * 1 / math.exp(epoch / ((epochs + 1) / (100 * math.log10(epochs + 1))))\n",
    "\n",
    "            for j in range(0, len(self.layers) - 1):\n",
    "                self.weights[j][self.sorted_indecies[self.n // 2::2]] = np.mean((self.weights[j][self.sorted_indecies[:self.n // 2:2]], self.weights[j][self.sorted_indecies[1:1 + self.n // 2:2]]), axis = 0) + np.random.normal(0, self.mutation_sigma, (self.n // 4, self.layers[j], self.layers[j + 1]))\n",
    "                self.weights[j][self.sorted_indecies[1 + self.n // 2::2]] = np.mean((self.weights[j][self.sorted_indecies[:self.n // 2:2]], self.weights[j][self.sorted_indecies[1:1 + self.n // 2:2]]), axis = 0) + np.random.normal(0, self.mutation_sigma, (self.n // 4, self.layers[j], self.layers[j + 1]))\n",
    "\n",
    "            if self.best_net != self.sorted_indecies[0]:\n",
    "                self.best_net = self.sorted_indecies[0]\n",
    "                self.training_loss_history += [self.nets_loss[self.best_net]]\n",
    "                \n",
    "                if validation_data:\n",
    "                    self.validation_loss_history += [np.mean(np.abs(y_val - self.predict(X_val)))]\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]}\")\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        pass\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        forward_pass = X.T\n",
    "        for j in range(0, len(self.layers) - 2):\n",
    "            forward_pass = self.activation_function(self.weights[j][self.best_net].T @ forward_pass)\n",
    "\n",
    "        forward_pass = self.output_activation_function(self.weights[-1][self.best_net].T @ forward_pass)\n",
    "        return forward_pass.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 0.3948271366645542\n",
      "Epoch 3 - loss: 0.36741452884444964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xz/f2gwbn5n3vs4pz044n49z3cw0000gn/T/ipykernel_19235/903138725.py:18: RuntimeWarning: overflow encountered in exp\n",
      "  self.output_activation_function = lambda x: 1 / (1 + np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - loss: 0.3305457191655617\n",
      "Epoch 9 - loss: 0.3272249104054906\n",
      "Epoch 10 - loss: 0.32680243380996044\n",
      "Epoch 11 - loss: 0.32225484629538903\n",
      "Epoch 14 - loss: 0.3177289365836428\n",
      "Epoch 15 - loss: 0.3143183520189079\n",
      "Epoch 16 - loss: 0.3122005233199692\n",
      "Epoch 17 - loss: 0.31165169857038305\n",
      "Epoch 18 - loss: 0.31019339813758706\n",
      "Epoch 19 - loss: 0.30991456704305725\n",
      "Epoch 20 - loss: 0.3087673667897483\n",
      "Epoch 21 - loss: 0.3077325099580049\n",
      "Epoch 23 - loss: 0.30708631080077037\n",
      "Epoch 24 - loss: 0.3059606106428898\n",
      "Epoch 26 - loss: 0.30590595064189424\n",
      "Epoch 27 - loss: 0.30532518050476665\n",
      "Epoch 28 - loss: 0.3048262493040859\n",
      "Epoch 29 - loss: 0.3043844988125836\n",
      "Epoch 30 - loss: 0.30303831730885833\n",
      "Epoch 31 - loss: 0.30249117143238247\n",
      "Epoch 32 - loss: 0.301601283268902\n",
      "Epoch 34 - loss: 0.2987342371916033\n",
      "Epoch 35 - loss: 0.29810107741471137\n",
      "Epoch 36 - loss: 0.2968647584722182\n",
      "Epoch 37 - loss: 0.29628619624486424\n",
      "Epoch 39 - loss: 0.29598388790517716\n",
      "Epoch 40 - loss: 0.29566831671469107\n",
      "Epoch 41 - loss: 0.29470229525916897\n",
      "Epoch 43 - loss: 0.2946883178578092\n",
      "Epoch 44 - loss: 0.29378173180951317\n",
      "Epoch 45 - loss: 0.293697738938402\n",
      "Epoch 46 - loss: 0.29352402273667494\n",
      "Epoch 47 - loss: 0.29317365864774186\n",
      "Epoch 48 - loss: 0.29287861588324454\n",
      "Epoch 50 - loss: 0.29183662887104483\n",
      "Epoch 52 - loss: 0.2917872086363931\n",
      "Epoch 53 - loss: 0.2916888344056268\n",
      "Epoch 54 - loss: 0.29106420098287145\n",
      "Epoch 56 - loss: 0.29096371044995945\n",
      "Epoch 57 - loss: 0.2902964968274036\n",
      "Epoch 58 - loss: 0.28996913018238607\n",
      "Epoch 59 - loss: 0.28929890180348217\n",
      "Epoch 60 - loss: 0.2887950108247216\n",
      "Epoch 61 - loss: 0.28871155336981746\n",
      "Epoch 62 - loss: 0.28849770968526306\n",
      "Epoch 63 - loss: 0.28722659386237687\n",
      "Epoch 66 - loss: 0.28717345291547863\n",
      "Epoch 67 - loss: 0.28677273579348583\n",
      "Epoch 68 - loss: 0.2863764836071876\n",
      "Epoch 69 - loss: 0.286021612190905\n",
      "Epoch 70 - loss: 0.28518360290035816\n",
      "Epoch 72 - loss: 0.28462122427000053\n",
      "Epoch 74 - loss: 0.2841416305952669\n",
      "Epoch 77 - loss: 0.2839649148607696\n",
      "Epoch 78 - loss: 0.2836983890803778\n",
      "Epoch 79 - loss: 0.28327887353732895\n",
      "Epoch 80 - loss: 0.2830166413485448\n",
      "Epoch 82 - loss: 0.2829035292911411\n",
      "Epoch 83 - loss: 0.282551314559803\n",
      "Epoch 84 - loss: 0.28212510434295224\n",
      "Epoch 85 - loss: 0.28196569197823995\n",
      "Epoch 86 - loss: 0.2815772119157677\n",
      "Epoch 88 - loss: 0.28095966127604\n",
      "Epoch 89 - loss: 0.28067063804522424\n",
      "Epoch 90 - loss: 0.28034033396076036\n",
      "Epoch 91 - loss: 0.27998029354503\n",
      "Epoch 92 - loss: 0.27949964112753733\n",
      "Epoch 95 - loss: 0.27915815444387987\n",
      "Epoch 98 - loss: 0.2789841996055709\n",
      "Epoch 99 - loss: 0.27847278036997397\n",
      "Epoch 100 - loss: 0.27814609171178073\n",
      "Epoch 104 - loss: 0.2778826372076188\n",
      "Epoch 107 - loss: 0.27787862976716976\n",
      "Epoch 108 - loss: 0.2775763365805921\n",
      "Epoch 110 - loss: 0.27743249251082447\n",
      "Epoch 111 - loss: 0.2774208372526588\n",
      "Epoch 113 - loss: 0.27715912732614917\n",
      "Epoch 114 - loss: 0.2770081808464651\n",
      "Epoch 115 - loss: 0.2768306862097407\n",
      "Epoch 117 - loss: 0.27682924380188373\n",
      "Epoch 118 - loss: 0.27664924391514817\n",
      "Epoch 120 - loss: 0.2765973259676844\n",
      "Epoch 122 - loss: 0.2764703221176901\n",
      "Epoch 124 - loss: 0.27642251702966536\n",
      "Epoch 127 - loss: 0.27639110810113215\n",
      "Epoch 128 - loss: 0.2763466821219112\n",
      "Epoch 132 - loss: 0.276153403785994\n",
      "Epoch 134 - loss: 0.2761453769496419\n",
      "Epoch 135 - loss: 0.27598010187933186\n",
      "Epoch 138 - loss: 0.2759320550406051\n",
      "Epoch 141 - loss: 0.2755452290253788\n",
      "Epoch 143 - loss: 0.2753032475413722\n",
      "Epoch 145 - loss: 0.27512887227255284\n",
      "Epoch 146 - loss: 0.27492048750769854\n",
      "Epoch 149 - loss: 0.27476393345495853\n",
      "Epoch 151 - loss: 0.27473766073886763\n",
      "Epoch 152 - loss: 0.2746221482698925\n",
      "Epoch 153 - loss: 0.2745576126037768\n",
      "Epoch 154 - loss: 0.2738436861394325\n",
      "Epoch 159 - loss: 0.27364013278581745\n",
      "Epoch 160 - loss: 0.27353897206935823\n",
      "Epoch 165 - loss: 0.2733190262757123\n",
      "Epoch 167 - loss: 0.273187177207398\n",
      "Epoch 171 - loss: 0.2731491918103882\n",
      "Epoch 176 - loss: 0.2729510263786533\n",
      "Epoch 178 - loss: 0.2727766494226363\n",
      "Epoch 185 - loss: 0.2725943796319251\n",
      "Epoch 195 - loss: 0.27252691650948646\n",
      "Epoch 197 - loss: 0.27251843635860384\n",
      "Epoch 202 - loss: 0.27237505417569396\n",
      "Epoch 205 - loss: 0.27216726528741525\n",
      "Epoch 209 - loss: 0.27206044171654853\n",
      "Epoch 211 - loss: 0.2720356132549722\n",
      "Epoch 220 - loss: 0.27198626498604705\n",
      "Epoch 222 - loss: 0.2718294779503072\n",
      "Epoch 227 - loss: 0.2716371297081335\n",
      "Epoch 240 - loss: 0.2716048275023143\n",
      "Epoch 242 - loss: 0.2715783965591793\n",
      "Epoch 246 - loss: 0.27156630829594325\n",
      "Epoch 249 - loss: 0.2713112732623616\n",
      "Epoch 252 - loss: 0.2712511809633935\n",
      "Epoch 256 - loss: 0.27115603140423217\n",
      "Epoch 260 - loss: 0.2710429661173843\n",
      "Epoch 265 - loss: 0.27096535461658866\n",
      "Epoch 268 - loss: 0.27088296534761414\n",
      "Epoch 271 - loss: 0.27086806020260157\n",
      "Epoch 277 - loss: 0.2708039305539997\n",
      "Epoch 278 - loss: 0.27068883408665667\n",
      "Epoch 288 - loss: 0.270598395981025\n",
      "Epoch 293 - loss: 0.2704866103825097\n",
      "Epoch 308 - loss: 0.27046189401199594\n",
      "Epoch 309 - loss: 0.2704158994837994\n",
      "Epoch 340 - loss: 0.27023883951550254\n",
      "Epoch 362 - loss: 0.2701403363854247\n",
      "Epoch 382 - loss: 0.27007859539089984\n",
      "Epoch 407 - loss: 0.27003247266503205\n",
      "Epoch 422 - loss: 0.2700248550474996\n",
      "Epoch 428 - loss: 0.27001604642951743\n",
      "Epoch 431 - loss: 0.26997128642056595\n",
      "Epoch 449 - loss: 0.26997118467261094\n",
      "Epoch 454 - loss: 0.2699376369473799\n",
      "Epoch 458 - loss: 0.2698685028488893\n",
      "Epoch 464 - loss: 0.2698671518290141\n",
      "Epoch 508 - loss: 0.26969484572838665\n",
      "Epoch 567 - loss: 0.2696656873047297\n",
      "Epoch 604 - loss: 0.26966223488846897\n",
      "Epoch 638 - loss: 0.26960903603378994\n",
      "Epoch 686 - loss: 0.2695958773012348\n",
      "Epoch 689 - loss: 0.26955641628070015\n",
      "Epoch 698 - loss: 0.2694847302408596\n",
      "Epoch 705 - loss: 0.26940760086793614\n",
      "Epoch 709 - loss: 0.2694022734204353\n",
      "Epoch 733 - loss: 0.2693332347251316\n",
      "Epoch 795 - loss: 0.26917854971757876\n",
      "Epoch 813 - loss: 0.269138000973537\n",
      "Epoch 940 - loss: 0.2691258083101461\n",
      "Epoch 944 - loss: 0.2691186039963873\n",
      "Epoch 949 - loss: 0.269058695364194\n"
     ]
    }
   ],
   "source": [
    "classifier = VectorizedEvoClassifier(n = 20, hidden_layers = [6], activation = \"relu\", random_state = 42)\n",
    "#classifier.fit(datasets[\"second\"][\"X_train\"][start:stop], datasets[\"second\"][\"y_train\"][start:stop], epochs = 2, verbose = 1)\n",
    "classifier.fit(X_train, y_train, epochs = 1000, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.91758985, 0.22492438],\n",
       "        [0.83053294, 0.84433992],\n",
       "        [0.54817229, 0.70536465],\n",
       "        [0.75789449, 0.86489857],\n",
       "        [0.07120965, 0.94340209]]),\n",
       " array([0, 1]),\n",
       " (5, 2),\n",
       " (2,))"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "y_train2 = np.random.randint(0, 2, size = (2))\n",
    "\n",
    "y_preds = activation_function(np.random.uniform(0, 2, size = (5, 2)))\n",
    "y_preds = activation_function(np.random.uniform(0, 2, size = (5, 2)))\n",
    "\n",
    "y_preds = np.array([[0.91758985, 0.22492438],\n",
    "        [0.83053294, 0.84433992],\n",
    "        [0.54817229, 0.70536465],\n",
    "        [0.75789449, 0.86489857],\n",
    "        [0.07120965, 0.94340209]])\n",
    "\n",
    "y_preds, y_train2, y_preds.shape, y_train2.shape, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.99401885, 0.97214841, 0.57174736, 0.78176235, 0.06606747])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((y_train2 * np.log(y_preds) + (1 - y_train2) * np.log(1 - y_preds)) * -1, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.06115362e-09, 2.27792704e-09, 2.51749871e-09, 2.78226636e-09,\n",
       "       3.07487987e-09, 3.39826781e-09, 3.75566675e-09, 4.15065367e-09,\n",
       "       4.58718173e-09, 5.06961984e-09, 5.60279641e-09, 6.19204764e-09,\n",
       "       6.84327098e-09, 7.56298406e-09, 8.35839003e-09, 9.23744958e-09,\n",
       "       1.02089606e-08, 1.12826464e-08, 1.24692526e-08, 1.37806554e-08,\n",
       "       1.52299795e-08, 1.68317304e-08, 1.86019389e-08, 2.05583219e-08,\n",
       "       2.27204594e-08, 2.51099909e-08, 2.77508317e-08, 3.06694120e-08,\n",
       "       3.38949421e-08, 3.74597042e-08, 4.13993755e-08, 4.57533856e-08,\n",
       "       5.05653109e-08, 5.58833108e-08, 6.17606095e-08, 6.82560291e-08,\n",
       "       7.54345778e-08, 8.33681009e-08, 9.21359999e-08, 1.01826027e-07,\n",
       "       1.12535162e-07, 1.24370587e-07, 1.37450754e-07, 1.51906574e-07,\n",
       "       1.67882725e-07, 1.85539102e-07, 2.05052416e-07, 2.26617961e-07,\n",
       "       2.50451575e-07, 2.76791789e-07, 3.05902227e-07, 3.38074234e-07,\n",
       "       3.73629798e-07, 4.12924771e-07, 4.56352429e-07, 5.04347408e-07,\n",
       "       5.57390059e-07, 6.16011247e-07, 6.80797671e-07, 7.52397733e-07,\n",
       "       8.31528028e-07, 9.18980513e-07, 1.01563044e-06, 1.12244511e-06,\n",
       "       1.24049354e-06, 1.37095721e-06, 1.51514182e-06, 1.67449041e-06,\n",
       "       1.85059777e-06, 2.04522644e-06, 2.26032430e-06, 2.49804409e-06,\n",
       "       2.76076495e-06, 3.05111625e-06, 3.37200386e-06, 3.72663928e-06,\n",
       "       4.11857174e-06, 4.55172374e-06, 5.03043030e-06, 5.55948233e-06,\n",
       "       6.14417460e-06, 6.79035870e-06, 7.50450160e-06, 8.29375037e-06,\n",
       "       9.16600372e-06, 1.01299910e-05, 1.11953595e-05, 1.23727712e-05,\n",
       "       1.36740091e-05, 1.51120954e-05, 1.67014218e-05, 1.84578933e-05,\n",
       "       2.03990873e-05, 2.25444297e-05, 2.49153889e-05, 2.75356911e-05,\n",
       "       3.04315569e-05, 3.36319640e-05, 3.71689371e-05, 4.10778678e-05,\n",
       "       4.53978687e-05, 5.01721647e-05, 5.54485247e-05, 6.12797396e-05,\n",
       "       6.77241496e-05, 7.48462275e-05, 8.27172229e-05, 9.14158739e-05,\n",
       "       1.01029194e-04, 1.11653341e-04, 1.23394576e-04, 1.36370327e-04,\n",
       "       1.50710358e-04, 1.66558065e-04, 1.84071905e-04, 2.03426978e-04,\n",
       "       2.24816770e-04, 2.48455082e-04, 2.74578156e-04, 3.03447030e-04,\n",
       "       3.35350130e-04, 3.70606141e-04, 4.09567165e-04, 4.52622223e-04,\n",
       "       5.00201107e-04, 5.52778637e-04, 6.10879359e-04, 6.75082731e-04,\n",
       "       7.46028834e-04, 8.24424686e-04, 9.11051194e-04, 1.00677082e-03,\n",
       "       1.11253603e-03, 1.22939862e-03, 1.35851995e-03, 1.50118226e-03,\n",
       "       1.65880108e-03, 1.83293894e-03, 2.02532039e-03, 2.23784852e-03,\n",
       "       2.47262316e-03, 2.73196076e-03, 3.01841632e-03, 3.33480731e-03,\n",
       "       3.68423990e-03, 4.07013772e-03, 4.49627316e-03, 4.96680165e-03,\n",
       "       5.48629890e-03, 6.05980149e-03, 6.69285092e-03, 7.39154134e-03,\n",
       "       8.16257115e-03, 9.01329865e-03, 9.95180187e-03, 1.09869426e-02,\n",
       "       1.21284350e-02, 1.33869178e-02, 1.47740317e-02, 1.63024994e-02,\n",
       "       1.79862100e-02, 1.98403057e-02, 2.18812709e-02, 2.41270214e-02,\n",
       "       2.65969936e-02, 2.93122308e-02, 3.22954647e-02, 3.55711893e-02,\n",
       "       3.91657228e-02, 4.31072549e-02, 4.74258732e-02, 5.21535631e-02,\n",
       "       5.73241759e-02, 6.29733561e-02, 6.91384203e-02, 7.58581800e-02,\n",
       "       8.31726965e-02, 9.11229610e-02, 9.97504891e-02, 1.09096821e-01,\n",
       "       1.19202922e-01, 1.30108474e-01, 1.41851065e-01, 1.54465265e-01,\n",
       "       1.67981615e-01, 1.82425524e-01, 1.97816111e-01, 2.14165017e-01,\n",
       "       2.31475217e-01, 2.49739894e-01, 2.68941421e-01, 2.89050497e-01,\n",
       "       3.10025519e-01, 3.31812228e-01, 3.54343694e-01, 3.77540669e-01,\n",
       "       4.01312340e-01, 4.25557483e-01, 4.50166003e-01, 4.75020813e-01,\n",
       "       5.00000000e-01, 5.24979187e-01, 5.49833997e-01, 5.74442517e-01,\n",
       "       5.98687660e-01, 6.22459331e-01, 6.45656306e-01, 6.68187772e-01,\n",
       "       6.89974481e-01, 7.10949503e-01, 7.31058579e-01, 7.50260106e-01,\n",
       "       7.68524783e-01, 7.85834983e-01, 8.02183889e-01, 8.17574476e-01,\n",
       "       8.32018385e-01, 8.45534735e-01, 8.58148935e-01, 8.69891526e-01,\n",
       "       8.80797078e-01, 8.90903179e-01, 9.00249511e-01, 9.08877039e-01,\n",
       "       9.16827304e-01, 9.24141820e-01, 9.30861580e-01, 9.37026644e-01,\n",
       "       9.42675824e-01, 9.47846437e-01, 9.52574127e-01, 9.56892745e-01,\n",
       "       9.60834277e-01, 9.64428811e-01, 9.67704535e-01, 9.70687769e-01,\n",
       "       9.73403006e-01, 9.75872979e-01, 9.78118729e-01, 9.80159694e-01,\n",
       "       9.82013790e-01, 9.83697501e-01, 9.85225968e-01, 9.86613082e-01,\n",
       "       9.87871565e-01, 9.89013057e-01, 9.90048198e-01, 9.90986701e-01,\n",
       "       9.91837429e-01, 9.92608459e-01, 9.93307149e-01, 9.93940199e-01,\n",
       "       9.94513701e-01, 9.95033198e-01, 9.95503727e-01, 9.95929862e-01,\n",
       "       9.96315760e-01, 9.96665193e-01, 9.96981584e-01, 9.97268039e-01,\n",
       "       9.97527377e-01, 9.97762151e-01, 9.97974680e-01, 9.98167061e-01,\n",
       "       9.98341199e-01, 9.98498818e-01, 9.98641480e-01, 9.98770601e-01,\n",
       "       9.98887464e-01, 9.98993229e-01, 9.99088949e-01, 9.99175575e-01,\n",
       "       9.99253971e-01, 9.99324917e-01, 9.99389121e-01, 9.99447221e-01,\n",
       "       9.99499799e-01, 9.99547378e-01, 9.99590433e-01, 9.99629394e-01,\n",
       "       9.99664650e-01, 9.99696553e-01, 9.99725422e-01, 9.99751545e-01,\n",
       "       9.99775183e-01, 9.99796573e-01, 9.99815928e-01, 9.99833442e-01,\n",
       "       9.99849290e-01, 9.99863630e-01, 9.99876605e-01, 9.99888347e-01,\n",
       "       9.99898971e-01, 9.99908584e-01, 9.99917283e-01, 9.99925154e-01,\n",
       "       9.99932276e-01, 9.99938720e-01, 9.99944551e-01, 9.99949828e-01,\n",
       "       9.99954602e-01, 9.99958922e-01, 9.99962831e-01, 9.99966368e-01,\n",
       "       9.99969568e-01, 9.99972464e-01, 9.99975085e-01, 9.99977456e-01,\n",
       "       9.99979601e-01, 9.99981542e-01, 9.99983299e-01, 9.99984888e-01,\n",
       "       9.99986326e-01, 9.99987627e-01, 9.99988805e-01, 9.99989870e-01,\n",
       "       9.99990834e-01, 9.99991706e-01, 9.99992495e-01, 9.99993210e-01,\n",
       "       9.99993856e-01, 9.99994441e-01, 9.99994970e-01, 9.99995448e-01,\n",
       "       9.99995881e-01, 9.99996273e-01, 9.99996628e-01, 9.99996949e-01,\n",
       "       9.99997239e-01, 9.99997502e-01, 9.99997740e-01, 9.99997955e-01,\n",
       "       9.99998149e-01, 9.99998326e-01, 9.99998485e-01, 9.99998629e-01,\n",
       "       9.99998760e-01, 9.99998878e-01, 9.99998984e-01, 9.99999081e-01,\n",
       "       9.99999168e-01, 9.99999248e-01, 9.99999319e-01, 9.99999384e-01,\n",
       "       9.99999443e-01, 9.99999496e-01, 9.99999544e-01, 9.99999587e-01,\n",
       "       9.99999626e-01, 9.99999662e-01, 9.99999694e-01, 9.99999723e-01,\n",
       "       9.99999750e-01, 9.99999773e-01, 9.99999795e-01, 9.99999814e-01,\n",
       "       9.99999832e-01, 9.99999848e-01, 9.99999863e-01, 9.99999876e-01,\n",
       "       9.99999887e-01, 9.99999898e-01, 9.99999908e-01, 9.99999917e-01,\n",
       "       9.99999925e-01, 9.99999932e-01, 9.99999938e-01, 9.99999944e-01,\n",
       "       9.99999949e-01, 9.99999954e-01, 9.99999959e-01, 9.99999963e-01,\n",
       "       9.99999966e-01, 9.99999969e-01, 9.99999972e-01, 9.99999975e-01,\n",
       "       9.99999977e-01, 9.99999979e-01, 9.99999981e-01, 9.99999983e-01,\n",
       "       9.99999985e-01, 9.99999986e-01, 9.99999988e-01, 9.99999989e-01,\n",
       "       9.99999990e-01, 9.99999991e-01, 9.99999992e-01, 9.99999992e-01,\n",
       "       9.99999993e-01, 9.99999994e-01, 9.99999994e-01, 9.99999995e-01,\n",
       "       9.99999995e-01, 9.99999996e-01, 9.99999996e-01, 9.99999997e-01,\n",
       "       9.99999997e-01, 9.99999997e-01, 9.99999997e-01, 9.99999998e-01])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_function(np.arange(-20, 20, 0.1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "89755740b7ae626787368751d18928e1c57f789b29910009fe21160c1737907e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('Maskininlarning-Daniel-Petersson-KQXPI0Ug')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
