{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin_europe</th>\n",
       "      <th>origin_japan</th>\n",
       "      <th>origin_usa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0  18.0          8         307.0       130.0    3504          12.0   \n",
       "1  15.0          8         350.0       165.0    3693          11.5   \n",
       "2  18.0          8         318.0       150.0    3436          11.0   \n",
       "3  16.0          8         304.0       150.0    3433          12.0   \n",
       "4  17.0          8         302.0       140.0    3449          10.5   \n",
       "\n",
       "   model_year  origin_europe  origin_japan  origin_usa  \n",
       "0          70              0             0           1  \n",
       "1          70              0             0           1  \n",
       "2          70              0             0           1  \n",
       "3          70              0             0           1  \n",
       "4          70              0             0           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset(\"mpg\")\n",
    "\n",
    "X_train, y_train = df[~df[\"horsepower\"].isna()][[\"displacement\", \"acceleration\"]], df[~df[\"horsepower\"].isna()][\"horsepower\"]\n",
    "X_pred = df[df[\"horsepower\"].isna()][[\"displacement\", \"acceleration\"]]\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_pred)\n",
    "y_pred = np.round(y_pred)\n",
    "df.loc[X_pred.index, \"horsepower\"] = y_pred\n",
    "df = pd.get_dummies(df.drop(\"name\", axis = 1), columns = [\"origin\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop([\"mpg\"], axis = 1).values, df[\"mpg\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_val = scaler.transform(X_val)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 86.80315411771315 - val_loss: 85.92074829122775\n",
      "Epoch 52 - loss: 48.529593658799364 - val_loss: 37.92957627771934\n",
      "Epoch 57 - loss: 46.16809098692537 - val_loss: 43.64601834537365\n",
      "Epoch 61 - loss: 37.3078808650963 - val_loss: 43.2594308856887\n",
      "Epoch 62 - loss: 24.374492409724166 - val_loss: 22.50102748409874\n",
      "Epoch 64 - loss: 24.180719007757904 - val_loss: 28.8056701432513\n",
      "Epoch 65 - loss: 21.465531030081713 - val_loss: 27.533882300689108\n",
      "Epoch 67 - loss: 18.667022843275195 - val_loss: 14.55088036528893\n",
      "Epoch 69 - loss: 15.348983235566964 - val_loss: 14.005709298107476\n",
      "Epoch 72 - loss: 13.672580233853704 - val_loss: 16.69099270184025\n",
      "Epoch 73 - loss: 13.46701468086422 - val_loss: 14.523411262108272\n",
      "Epoch 76 - loss: 12.406823812751375 - val_loss: 13.043480970327625\n",
      "Epoch 80 - loss: 10.727750323068534 - val_loss: 10.37402824278321\n",
      "Epoch 85 - loss: 10.580806571953136 - val_loss: 9.454668235858986\n",
      "Epoch 86 - loss: 9.319042955912032 - val_loss: 9.35616942541465\n",
      "Epoch 91 - loss: 9.227494211751777 - val_loss: 9.751139334346686\n",
      "Epoch 93 - loss: 8.685065580924013 - val_loss: 8.258699347539212\n",
      "Epoch 95 - loss: 8.590472621484302 - val_loss: 9.112801866718195\n",
      "Epoch 99 - loss: 8.567062623724281 - val_loss: 6.515020659011594\n",
      "Epoch 101 - loss: 8.387210595778738 - val_loss: 8.225520742514249\n",
      "Epoch 102 - loss: 8.059172606461475 - val_loss: 8.184442685654805\n",
      "Epoch 109 - loss: 7.317029638091781 - val_loss: 6.976512886461753\n",
      "Epoch 110 - loss: 6.678086829016301 - val_loss: 6.122172804916998\n",
      "Epoch 111 - loss: 6.442734196022677 - val_loss: 6.54898925841735\n",
      "Epoch 119 - loss: 6.207188718736794 - val_loss: 4.571786922746756\n",
      "Epoch 126 - loss: 6.179850901197369 - val_loss: 6.012283792460144\n",
      "Epoch 127 - loss: 5.534235022230099 - val_loss: 5.068677753200768\n",
      "Epoch 134 - loss: 5.207215101095058 - val_loss: 4.616642576121398\n",
      "Epoch 137 - loss: 5.074851512370854 - val_loss: 4.710326843698819\n",
      "Epoch 140 - loss: 4.95833228814692 - val_loss: 4.723307397879798\n",
      "Epoch 143 - loss: 4.910196837513111 - val_loss: 4.5892815615070734\n",
      "Epoch 147 - loss: 4.732930517536688 - val_loss: 4.108252826021447\n",
      "Epoch 148 - loss: 4.663781198104056 - val_loss: 6.028154572999664\n",
      "Epoch 149 - loss: 4.48819446921432 - val_loss: 4.530928050857052\n",
      "Epoch 150 - loss: 4.2054947670193865 - val_loss: 4.1586117280069095\n",
      "Epoch 152 - loss: 4.145107469305194 - val_loss: 4.26594709920441\n",
      "Epoch 159 - loss: 4.10286019356557 - val_loss: 4.131875410066675\n",
      "Epoch 160 - loss: 4.028597636591922 - val_loss: 3.8096594499552836\n",
      "Epoch 161 - loss: 3.9428987073939545 - val_loss: 3.0532354781392517\n",
      "Epoch 166 - loss: 3.651444449598221 - val_loss: 3.3208033751567543\n",
      "Epoch 173 - loss: 3.641634864296532 - val_loss: 2.9599119375806198\n",
      "Epoch 177 - loss: 3.4393902290889313 - val_loss: 3.532441714171051\n",
      "Epoch 185 - loss: 3.435879384579252 - val_loss: 3.158505170839633\n",
      "Epoch 189 - loss: 3.363907940170303 - val_loss: 3.0908643496010972\n",
      "Epoch 198 - loss: 3.3465333137198567 - val_loss: 2.8324315054073024\n",
      "Epoch 200 - loss: 3.2609138242914595 - val_loss: 3.363322403007726\n",
      "Epoch 205 - loss: 3.117475066062231 - val_loss: 2.9229987074319133\n",
      "Epoch 214 - loss: 3.1063851733860965 - val_loss: 3.1668801122739003\n",
      "Epoch 218 - loss: 2.9970997781418247 - val_loss: 3.4988414606829474\n",
      "Epoch 247 - loss: 2.9794302232204712 - val_loss: 3.1150041327652436\n",
      "Epoch 254 - loss: 2.9597027640100806 - val_loss: 3.27484221076892\n",
      "Epoch 265 - loss: 2.9051976779255044 - val_loss: 2.635476546600571\n",
      "Epoch 280 - loss: 2.8633911471470843 - val_loss: 2.903812096216422\n",
      "Epoch 292 - loss: 2.8610146175685487 - val_loss: 3.307424175893737\n",
      "Epoch 305 - loss: 2.686748455145268 - val_loss: 3.105648985531557\n",
      "Epoch 323 - loss: 2.656468215955884 - val_loss: 2.879650444865784\n",
      "Epoch 328 - loss: 2.6547416737727603 - val_loss: 2.5297930766052614\n",
      "Epoch 404 - loss: 2.6204131008028098 - val_loss: 2.837866300920448\n",
      "Epoch 463 - loss: 2.6136210991158793 - val_loss: 2.923604679427118\n",
      "Epoch 467 - loss: 2.5143837205534494 - val_loss: 2.4937707411890955\n",
      "Epoch 571 - loss: 2.4989589858750487 - val_loss: 2.628753528563479\n",
      "Epoch 624 - loss: 2.4760797177777554 - val_loss: 2.7133116545591287\n",
      "Epoch 655 - loss: 2.4149124829332043 - val_loss: 2.865516873489553\n",
      "Epoch 705 - loss: 2.3684904742298554 - val_loss: 3.023825589144791\n",
      "Epoch 726 - loss: 2.2836870655255153 - val_loss: 2.6958976199311473\n",
      "Epoch 924 - loss: 2.2077708821821482 - val_loss: 2.2344702256602207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 5.24456 s\n",
      "File: /var/folders/xz/f2gwbn5n3vs4pz044n49z3cw0000gn/T/ipykernel_97253/3792550349.py\n",
      "Function: fit at line 28\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    28                                               def fit(self, X_train, y_train, epochs = 100, validation_data = False, verbose = 0):\n",
      "    29         1         99.0     99.0      0.0          X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
      "    30                                           \n",
      "    31         1          1.0      1.0      0.0          if validation_data:\n",
      "    32         1          1.0      1.0      0.0              X_val, y_val = validation_data\n",
      "    33                                                   \n",
      "    34         1          1.0      1.0      0.0          self.layers = [X_train.shape[1]] + self.layers\n",
      "    35                                           \n",
      "    36       101         35.0      0.3      0.0          for i in range(self.n):\n",
      "    37       100         46.0      0.5      0.0              self.nets += [[]]\n",
      "    38       500        210.0      0.4      0.0              for j in range(len(self.layers) - 1):\n",
      "    39       400       1204.0      3.0      0.0                  self.nets[i] += [np.random.uniform(-3, 3, (self.layers[j], self.layers[j + 1]))]\n",
      "    40                                           \n",
      "    41         1         19.0     19.0      0.0          self.y_preds = np.zeros((len(self.nets), y_train.shape[0]))\n",
      "    42         1          3.0      3.0      0.0          self.nets_loss = np.zeros(len(self.nets))\n",
      "    43         1          1.0      1.0      0.0          self.sorted_indecies = np.zeros(len(self.nets))\n",
      "    44                                           \n",
      "    45      1001        389.0      0.4      0.0          for epoch in range(epochs):\n",
      "    46    101000      35449.0      0.4      0.7              for i in range(len(self.nets)):\n",
      "    47    100000      47586.0      0.5      0.9                  forward_pass = X_train.T\n",
      "    48                                           \n",
      "    49    400000     162300.0      0.4      3.1                  for j in range(0, len(self.layers) - 2):\n",
      "    50                                                               #forward_pass = self.activation_function(self.nets[i][j].T @ forward_pass)\n",
      "    51                                                               #forward_pass = 1 / (1 + np.exp(-(self.nets[i][j].T @ forward_pass)))\n",
      "    52    300000    3090819.0     10.3     58.9                      forward_pass = np.maximum(0, (self.nets[i][j].T @ forward_pass))\n",
      "    53                                           \n",
      "    54    100000     316067.0      3.2      6.0                  forward_pass = self.nets[i][-1].T @ forward_pass\n",
      "    55                                           \n",
      "    56    100000     105441.0      1.1      2.0                  self.y_preds[i] = forward_pass.reshape(-1)\n",
      "    57                                                       \n",
      "    58      1000      18596.0     18.6      0.4              self.test = y_train - self.y_preds\n",
      "    59      1000      50346.0     50.3      1.0              self.nets_loss = np.mean(np.abs(self.y_preds - y_train), axis = 1)\n",
      "    60                                                       \n",
      "    61      1000       6867.0      6.9      0.1              self.sorted_indecies = np.argsort(regressor.nets_loss)\n",
      "    62                                                       \n",
      "    63      1000       1482.0      1.5      0.0              self.mutation_sigma = 0.1 + 5 * 1 / math.exp(epoch / (epochs / (10 * math.log10(epochs))))\n",
      "    64                                                       \n",
      "    65     26000       9643.0      0.4      0.2              for i in range(0, self.n // 2, 2):\n",
      "    66    125000      49671.0      0.4      0.9                  for j in range(len(self.layers) - 1):\n",
      "    67    100000     669796.0      6.7     12.8                      self.nets[self.sorted_indecies[self.n // 2 + i]][j] = (self.nets[self.sorted_indecies[i]][j] + self.nets[self.sorted_indecies[1 + i]][j]) / 2 + np.random.normal(0, self.mutation_sigma, (self.layers[j], self.layers[j + 1]))\n",
      "    68    100000     658051.0      6.6     12.5                      self.nets[self.sorted_indecies[self.n // 2 + 1 + i]][j] = (self.nets[self.sorted_indecies[i]][j] + self.nets[self.sorted_indecies[1 + i]][j]) / 2 + np.random.normal(0, self.mutation_sigma, (self.layers[j], self.layers[j + 1]))\n",
      "    69                                           \n",
      "    70      1000        588.0      0.6      0.0              if self.best_net != self.sorted_indecies[0]:\n",
      "    71        66         30.0      0.5      0.0                  self.best_net = self.sorted_indecies[0]\n",
      "    72        66        109.0      1.7      0.0                  self.training_loss_history += [self.nets_loss[self.best_net]]\n",
      "    73                                           \n",
      "    74        66         32.0      0.5      0.0                  if validation_data:\n",
      "    75        66      17899.0    271.2      0.3                      self.validation_loss_history += [mean_absolute_error(y_val, self.predict(X_val))]\n",
      "    76        66         42.0      0.6      0.0                      if verbose == 1:\n",
      "    77        66       1739.0     26.3      0.0                          print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]}\")\n",
      "    78                                                           else:\n",
      "    79                                                               if verbose == 1:\n",
      "    80                                                                   print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]}\")"
     ]
    }
   ],
   "source": [
    "%reload_ext line_profiler\n",
    "class ERegressor:\n",
    "    def __init__(self, n = 100, hidden_layers = False, activation = \"sigmoid\", random_state = None):\n",
    "\n",
    "        self.n = n // 2 * 2\n",
    "        self.nets = []\n",
    "        self.best_net = -1\n",
    "        self.best_result = None\n",
    "        self.validation_loss_history = []\n",
    "        self.training_loss_history = []\n",
    "        self.mutation_sigma = 0\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            #self.activation_function = np.vectorize(lambda x: 1 / (1 + math.exp(-x)))\n",
    "            self.activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif activation == \"relu\":\n",
    "            self.activation_function = lambda x: np.maximum(0, x)\n",
    "        \n",
    "        if hidden_layers:\n",
    "            self.layers = hidden_layers + [1]\n",
    "        else:\n",
    "            self.layers = [1]\n",
    "        \n",
    "        if random_state != None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs = 100, validation_data = False, verbose = 0):\n",
    "        X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "\n",
    "        if validation_data:\n",
    "            X_val, y_val = validation_data\n",
    "        \n",
    "        self.layers = [X_train.shape[1]] + self.layers\n",
    "\n",
    "        for i in range(self.n):\n",
    "            self.nets += [[]]\n",
    "            for j in range(len(self.layers) - 1):\n",
    "                self.nets[i] += [np.random.uniform(-3, 3, (self.layers[j], self.layers[j + 1]))]\n",
    "\n",
    "        self.y_preds = np.zeros((len(self.nets), y_train.shape[0]))\n",
    "        self.nets_loss = np.zeros(len(self.nets))\n",
    "        self.sorted_indecies = np.zeros(len(self.nets))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(self.nets)):\n",
    "                forward_pass = X_train.T\n",
    "\n",
    "                for j in range(0, len(self.layers) - 2):\n",
    "                    #forward_pass = self.activation_function(self.nets[i][j].T @ forward_pass)\n",
    "                    #forward_pass = 1 / (1 + np.exp(-(self.nets[i][j].T @ forward_pass)))\n",
    "                    forward_pass = np.maximum(0, (self.nets[i][j].T @ forward_pass))\n",
    "\n",
    "                forward_pass = self.nets[i][-1].T @ forward_pass\n",
    "\n",
    "                self.y_preds[i] = forward_pass.reshape(-1)\n",
    "            \n",
    "            self.test = y_train - self.y_preds\n",
    "            self.nets_loss = np.mean(np.abs(self.y_preds - y_train), axis = 1)\n",
    "            \n",
    "            self.sorted_indecies = np.argsort(regressor.nets_loss)\n",
    "            \n",
    "            self.mutation_sigma = 0.1 + 5 * 1 / math.exp(epoch / (epochs / (10 * math.log10(epochs))))\n",
    "            \n",
    "            for i in range(0, self.n // 2, 2):\n",
    "                for j in range(len(self.layers) - 1):\n",
    "                    self.nets[self.sorted_indecies[self.n // 2 + i]][j] = (self.nets[self.sorted_indecies[i]][j] + self.nets[self.sorted_indecies[1 + i]][j]) / 2 + np.random.normal(0, self.mutation_sigma, (self.layers[j], self.layers[j + 1]))\n",
    "                    self.nets[self.sorted_indecies[self.n // 2 + 1 + i]][j] = (self.nets[self.sorted_indecies[i]][j] + self.nets[self.sorted_indecies[1 + i]][j]) / 2 + np.random.normal(0, self.mutation_sigma, (self.layers[j], self.layers[j + 1]))\n",
    "\n",
    "            if self.best_net != self.sorted_indecies[0]:\n",
    "                self.best_net = self.sorted_indecies[0]\n",
    "                self.training_loss_history += [self.nets_loss[self.best_net]]\n",
    "\n",
    "                if validation_data:\n",
    "                    self.validation_loss_history += [mean_absolute_error(y_val, self.predict(X_val))]\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]}\")\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        forward_pass = X.T\n",
    "        for j in range(0, len(self.layers) - 2):\n",
    "            forward_pass = self.activation_function(self.nets[self.best_net][j].T @ forward_pass)\n",
    "\n",
    "        forward_pass = self.nets[self.best_net][-1].T @ forward_pass\n",
    "\n",
    "        return forward_pass.reshape(-1)\n",
    "        \n",
    "regressor = ERegressor(n = 100, hidden_layers = [16, 16, 16], activation = \"relu\", random_state = 42)\n",
    "#regressor.fit(scaled_X_train, y_train, epochs = 100, validation_data = (scaled_X_val, y_val), verbose = 1)\n",
    "\n",
    "%lprun -f regressor.fit regressor.fit(scaled_X_train, y_train, epochs = 1000, validation_data = (scaled_X_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20.38907808, 27.40096933, 38.22823466, 36.39181638, 24.67730235,\n",
       "       19.19756314, 34.45134532, 33.44129524, 21.23348381, 17.5762968 ,\n",
       "       19.99514027, 23.098856  , 26.08849498, 38.93722181, 19.32271763,\n",
       "       40.59873279, 31.57602559, 24.06227958, 29.25335776, 18.80163188,\n",
       "       37.69902226, 22.23814778, 20.57754002, 21.35357074, 20.76608974,\n",
       "       19.20174384, 30.77163694, 18.57135058, 35.3205911 , 38.435405  ,\n",
       "       34.30007285, 29.79981516, 23.98802951, 32.59205133, 29.08561504,\n",
       "       21.47165532, 29.11661789, 23.32646117, 39.56458541, 18.38459341,\n",
       "       20.37645257, 22.92599093, 23.82833175, 23.10993197, 23.78347165,\n",
       "       25.6312341 , 34.5173398 , 29.63963419, 39.00109399, 33.10990631,\n",
       "       39.35769109, 33.5781582 , 42.13792762, 21.74119642, 21.61103486,\n",
       "       24.66470606, 20.32280888, 35.58398435, 32.07925002, 33.81352409,\n",
       "       22.34234932, 19.28604595, 20.3722272 , 18.55651073, 25.20357646,\n",
       "       32.59249667, 28.00596188, 17.41155408, 25.97512463, 24.46276061,\n",
       "       19.13038877, 21.36457599, 22.15105744, 25.99599806, 19.65874849,\n",
       "       34.76585706, 17.32551735, 28.53498541, 21.80415936, 30.18170089,\n",
       "       31.85140566, 19.46653685, 29.2350791 , 24.43061619, 19.40567454,\n",
       "       22.32273939, 21.07799573, 25.88615052, 23.17821654, 18.14068881,\n",
       "       17.2933096 , 19.95276741, 17.30615877, 27.67073665, 36.17331937,\n",
       "       20.18697036, 21.41014303, 18.4549387 , 27.73445512, 20.88397325,\n",
       "       31.9316352 , 25.47038556, 16.03488136, 23.9416765 , 33.10121612,\n",
       "       33.67548641, 22.2590407 , 26.68060636, 22.23966947, 28.87919412,\n",
       "       30.89467757, 18.29495333, 37.10704609, 31.07482539, 25.31002811,\n",
       "       20.29585179, 23.32596225, 35.88395027, 27.39446823, 39.33920263,\n",
       "       24.03572708, 22.32864778, 22.75161158, 32.44387841, 19.02521804,\n",
       "       18.75502554, 47.87457195, 27.07103642, 33.76943983, 20.15475021,\n",
       "       23.69346309, 26.7710492 , 32.85940502, 32.76274543, 21.13083263,\n",
       "       26.45084735, 35.04889447, 22.33379199, 33.88767917, 23.0883135 ,\n",
       "       38.69110138, 20.87426208, 18.90243656, 22.87555876, 30.85716656,\n",
       "       32.36654889, 22.42652341, 32.37738129, 19.99310314, 21.0039861 ,\n",
       "       36.77386158, 14.83435842, 20.94746455, 29.85685842, 20.04304321,\n",
       "       33.30237955, 31.55101553, 28.49066553, 32.01602491, 21.19705593,\n",
       "       18.09232838, 32.30002938, 27.16507649, 28.89355999, 44.39688632,\n",
       "       20.1846955 , 40.50934003, 26.36694736, 19.33039809, 46.86949357,\n",
       "       25.59473864, 21.7421012 , 19.39723288, 23.46170312, 18.96277035,\n",
       "       37.06184778, 17.46841374, 22.74113035, 28.79952012, 19.58257796,\n",
       "       20.42696728, 20.05691487, 14.38066236, 24.43685321, 28.05976803,\n",
       "       36.63332985, 23.99462629, 22.03182218, 17.15106476, 33.76112098,\n",
       "       32.28641274, 28.7484213 , 48.1229083 , 25.51066455, 18.98776292,\n",
       "       32.50379213, 27.96831698, 22.93245336, 41.71546129, 21.5098171 ,\n",
       "       23.17084227, 33.26275763, 20.79745083, 23.13393648, 27.80616829,\n",
       "       17.95909793, 23.57937323, 18.66240663, 17.21117176, 21.9296665 ,\n",
       "       22.06006608, 27.33177531, 27.13005534, 21.65218616, 36.88136322,\n",
       "       15.59092563, 33.99618823, 20.02539288, 21.39782741, 22.91972218,\n",
       "       17.93299769, 29.34319809, 19.22320526, 18.02157534, 20.78275559,\n",
       "       17.73599964, 18.94746003, 19.88581473, 28.86581268, 24.35500932,\n",
       "       21.55890063, 19.06194598, 31.93090876, 47.5216559 , 32.62772494,\n",
       "       29.82459285, 29.80002904, 26.51209254, 21.93050161, 21.10080065,\n",
       "       37.01330252, 24.5959671 , 24.58975543, 28.28716406, 25.69068213,\n",
       "       33.11832968, 32.30420271, 21.38305187, 19.25085154, 18.92662664,\n",
       "       31.05376341, 19.49589988, 43.74630961, 23.52172536, 18.31194137,\n",
       "       32.95952614, 19.59778626, 23.90377504, 22.89107972, 28.17864636,\n",
       "       18.96111787, 19.43385737, 18.8228286 , 28.91158285, 31.87845023,\n",
       "       22.33073006, 22.01351308, 18.10751203, 26.90136126, 18.81702436,\n",
       "       23.59244421, 17.06056693, 33.7022503 , 38.31468316, 18.88480097,\n",
       "       31.22849156, 36.43063825, 32.37848772, 21.05554958, 36.56152897,\n",
       "       29.66278447, 19.99270312, 20.5329304 , 34.25747738, 22.51021206,\n",
       "       22.27634457, 26.15806029, 21.34497643, 41.62079087, 26.97765106,\n",
       "       22.03442865, 26.2088538 , 29.79066641, 20.26407815, 23.16499201,\n",
       "       24.47193593, 34.07571927, 39.83928682, 22.23509819, 34.95192527,\n",
       "       23.83903918, 26.84227801, 33.15919891, 31.23605356, 20.2405223 ,\n",
       "       29.72215374, 17.84887   , 43.51584716, 17.62718674, 19.10734838,\n",
       "       34.07340295, 28.40136754, 20.83816944, 18.52909978, 16.878228  ,\n",
       "       20.90518372, 38.56921015, 30.32429227])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(regressor.test, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 2.2e-05 s\n",
      "File: /var/folders/xz/f2gwbn5n3vs4pz044n49z3cw0000gn/T/ipykernel_79586/3696412895.py\n",
      "Function: func at line 3\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     3                                           def func():\n",
      "     4        11          3.0      0.3     13.6      for i in range(10):\n",
      "     5        10          9.0      0.9     40.9          i ** i\n",
      "     6         1         10.0     10.0     45.5      print(\"hi\")"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 0.000111 s\n",
      "File: /var/folders/xz/f2gwbn5n3vs4pz044n49z3cw0000gn/T/ipykernel_79586/2797496209.py\n",
      "Function: func at line 3\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     3                                           def func():\n",
      "     4         1          1.0      1.0      0.9      a = 0\n",
      "     5        11          3.0      0.3      2.7      for i in range(10):\n",
      "     6        10          7.0      0.7      6.3          a *= i ** i\n",
      "     7         1        100.0    100.0     90.1      print(\"hi\")"
     ]
    }
   ],
   "source": [
    "%reload_ext line_profiler\n",
    "\n",
    "\n",
    "%lprun -f func func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9816996383925645"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x176732850>]"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = []\n",
    "xrange = 10000\n",
    "\n",
    "for x in range(1, xrange + 1):\n",
    "    y.append(5 * 1 / math.exp(x / (xrange / (10 * math.log10(xrange)))))\n",
    "\n",
    "plt.plot(list(range(xrange)), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x176666850>]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = []\n",
    "xrange = 10000\n",
    "\n",
    "for x in range(1, xrange + 1):\n",
    "    y.append(5 * 1 / math.exp(x / (xrange / 10)))\n",
    "\n",
    "plt.plot(list(range(xrange)), y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47d3b7ff548c1bae2d6b155a9b3d6f1122689b634566f833764ba5dd9fcfa2e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Deep-learning-Daniel-Petersson-bXusHwTH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
