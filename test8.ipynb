{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin_europe</th>\n",
       "      <th>origin_japan</th>\n",
       "      <th>origin_usa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0  18.0          8         307.0       130.0    3504          12.0   \n",
       "1  15.0          8         350.0       165.0    3693          11.5   \n",
       "2  18.0          8         318.0       150.0    3436          11.0   \n",
       "3  16.0          8         304.0       150.0    3433          12.0   \n",
       "4  17.0          8         302.0       140.0    3449          10.5   \n",
       "\n",
       "   model_year  origin_europe  origin_japan  origin_usa  \n",
       "0          70              0             0           1  \n",
       "1          70              0             0           1  \n",
       "2          70              0             0           1  \n",
       "3          70              0             0           1  \n",
       "4          70              0             0           1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset(\"mpg\")\n",
    "\n",
    "X_train, y_train = df[~df[\"horsepower\"].isna()][[\"displacement\", \"acceleration\"]], df[~df[\"horsepower\"].isna()][\"horsepower\"]\n",
    "X_pred = df[df[\"horsepower\"].isna()][[\"displacement\", \"acceleration\"]]\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_pred)\n",
    "y_pred = np.round(y_pred)\n",
    "df.loc[X_pred.index, \"horsepower\"] = y_pred\n",
    "df = pd.get_dummies(df.drop(\"name\", axis = 1), columns = [\"origin\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop([\"mpg\"], axis = 1).values, df[\"mpg\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_val = scaler.transform(X_val)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERegressor:\n",
    "    def __init__(self, n = 100, hidden_layers = False, activation = \"sigmoid\", random_state = None):\n",
    "\n",
    "        self.n = n // 2 * 2\n",
    "        self.nets = []\n",
    "        self.best_net = -1\n",
    "        self.best_result = None\n",
    "        self.validation_loss_history = []\n",
    "        self.training_loss_history = []\n",
    "        self.mutation_sigma = 0\n",
    "\n",
    "        if activation == \"sigmoid\":\n",
    "            #self.activation_function = np.vectorize(lambda x: 1 / (1 + math.exp(-x)))\n",
    "            self.activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif activation == \"relu\":\n",
    "            self.activation_function = lambda x: np.maximum(0, x)\n",
    "        \n",
    "        if hidden_layers:\n",
    "            self.layers = hidden_layers + [1]\n",
    "        else:\n",
    "            self.layers = [1]\n",
    "        \n",
    "        if random_state != None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "    \n",
    "    def fit(self, X_train, y_train, epochs = 100, validation_data = False, verbose = 0):\n",
    "        X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "\n",
    "        if validation_data:\n",
    "            X_val, y_val = validation_data\n",
    "        \n",
    "        self.layers = [X_train.shape[1]] + self.layers\n",
    "\n",
    "        for i in range(self.n):\n",
    "            self.nets += [[]]\n",
    "            for j in range(len(self.layers) - 1):\n",
    "                self.nets[i] += [np.random.uniform(-3, 3, (self.layers[j], self.layers[j + 1]))]\n",
    "\n",
    "        self.y_preds = np.zeros((len(self.nets), y_train.shape[0]))\n",
    "        self.nets_loss = np.zeros(len(self.nets))\n",
    "        self.sorted_indecies = np.zeros(len(self.nets))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(self.nets)):\n",
    "                forward_pass = X_train.T\n",
    "\n",
    "                for j in range(0, len(self.layers) - 2):\n",
    "                    forward_pass = self.activation_function(self.nets[i][j].T @ forward_pass)\n",
    "\n",
    "                forward_pass = self.nets[i][-1].T @ forward_pass\n",
    "\n",
    "                self.y_preds[i] = forward_pass.reshape(-1)\n",
    "                self.nets_loss[i] = np.mean(np.abs(y_train - self.y_preds[i]))\n",
    "            \n",
    "            self.sorted_indecies = np.argsort(regressor.nets_loss)\n",
    "            \n",
    "            self.mutation_sigma = 0.1 + 5 * 1 / math.exp(epoch / (epochs / (10 * math.log10(epochs))))\n",
    "            \n",
    "            for i in range(0, self.n // 2, 2):\n",
    "                for j in range(len(self.layers) - 1):\n",
    "                    self.nets[self.sorted_indecies[self.n // 2 + i]][j] = (self.nets[self.sorted_indecies[i]][j] + self.nets[self.sorted_indecies[1 + i]][j]) / 2 + np.random.normal(0, self.mutation_sigma, (self.layers[j], self.layers[j + 1]))\n",
    "                    self.nets[self.sorted_indecies[self.n // 2 + 1 + i]][j] = (self.nets[self.sorted_indecies[i]][j] + self.nets[self.sorted_indecies[1 + i]][j]) / 2 + np.random.normal(0, self.mutation_sigma, (self.layers[j], self.layers[j + 1]))\n",
    "\n",
    "            if self.best_net != self.sorted_indecies[0]:\n",
    "                self.best_net = self.sorted_indecies[0]\n",
    "                self.training_loss_history += [self.nets_loss[self.best_net]]\n",
    "\n",
    "                if validation_data:\n",
    "                    self.validation_loss_history += [mean_absolute_error(y_val, self.predict(X_val))]\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]}\")\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        forward_pass = X.T\n",
    "        for j in range(0, len(self.layers) - 2):\n",
    "            forward_pass = self.activation_function(self.nets[self.best_net][j].T @ forward_pass)\n",
    "\n",
    "        forward_pass = self.nets[self.best_net][-1].T @ forward_pass\n",
    "\n",
    "        return forward_pass.reshape(-1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 19.137517493180052 - val_loss: 17.50466573467203\n",
      "Epoch 1 - loss: 10.26336019265631 - val_loss: 9.780869309002219\n",
      "Epoch 2 - loss: 6.806615092241999 - val_loss: 6.859723960849026\n",
      "Epoch 4 - loss: 6.623670472435652 - val_loss: 6.442167138217644\n",
      "Epoch 6 - loss: 5.32850580261849 - val_loss: 4.306614178342753\n",
      "Epoch 9 - loss: 5.236261567559209 - val_loss: 4.496065040007599\n",
      "Epoch 10 - loss: 5.213537492268563 - val_loss: 5.355628686997986\n",
      "Epoch 11 - loss: 4.728176158504881 - val_loss: 4.84367500499104\n",
      "Epoch 13 - loss: 4.564566403923038 - val_loss: 4.136746762531968\n",
      "Epoch 16 - loss: 4.222608160341475 - val_loss: 4.795552232124322\n",
      "Epoch 17 - loss: 3.9427718361153214 - val_loss: 3.474769637025182\n",
      "Epoch 25 - loss: 3.9262337198055572 - val_loss: 3.599359702436044\n",
      "Epoch 26 - loss: 3.8115757254287805 - val_loss: 3.3222470993999322\n",
      "Epoch 31 - loss: 3.732249623999882 - val_loss: 3.819378162288433\n",
      "Epoch 33 - loss: 3.7252191852361056 - val_loss: 3.6115656404880836\n",
      "Epoch 35 - loss: 3.5038795228538104 - val_loss: 3.074985724463553\n",
      "Epoch 36 - loss: 3.502625901036238 - val_loss: 3.4260829585395505\n",
      "Epoch 38 - loss: 3.390553138630272 - val_loss: 3.439332710125676\n",
      "Epoch 39 - loss: 3.354156546875799 - val_loss: 3.283986559505675\n",
      "Epoch 42 - loss: 3.3372114919080085 - val_loss: 3.5029327904910437\n",
      "Epoch 43 - loss: 3.3115315531523577 - val_loss: 3.5889334347610693\n",
      "Epoch 44 - loss: 3.251128552261418 - val_loss: 3.268994495742586\n",
      "Epoch 45 - loss: 3.0987789790274083 - val_loss: 3.014689230413581\n",
      "Epoch 47 - loss: 2.966683242262502 - val_loss: 2.5868744749960317\n",
      "Epoch 51 - loss: 2.8685440487129448 - val_loss: 2.3658737645665644\n",
      "Epoch 55 - loss: 2.8395346759563123 - val_loss: 2.6799730811144684\n",
      "Epoch 56 - loss: 2.703024311593353 - val_loss: 2.7524989305106695\n",
      "Epoch 60 - loss: 2.6914372903730572 - val_loss: 2.815574724616424\n",
      "Epoch 61 - loss: 2.6662176893597804 - val_loss: 2.505810813367972\n",
      "Epoch 64 - loss: 2.5922929505964234 - val_loss: 2.27426634354991\n",
      "Epoch 66 - loss: 2.5781611961206394 - val_loss: 2.8432329945324306\n",
      "Epoch 68 - loss: 2.5008250581495353 - val_loss: 2.163553920606167\n",
      "Epoch 69 - loss: 2.4155050672636724 - val_loss: 2.590387770997899\n",
      "Epoch 76 - loss: 2.3972442960075417 - val_loss: 2.1596937360286836\n",
      "Epoch 80 - loss: 2.34884321021493 - val_loss: 2.3140647786218076\n",
      "Epoch 82 - loss: 2.331700976057106 - val_loss: 2.252020025063992\n",
      "Epoch 83 - loss: 2.290166530484576 - val_loss: 2.2531204505878613\n",
      "Epoch 90 - loss: 2.272166801245671 - val_loss: 2.182317303386305\n",
      "Epoch 94 - loss: 2.2371261231970005 - val_loss: 2.284928640512054\n",
      "Epoch 100 - loss: 2.2337424577747327 - val_loss: 2.209940666430715\n",
      "Epoch 101 - loss: 2.1851834586869185 - val_loss: 2.2862380022341875\n",
      "Epoch 107 - loss: 2.1807475935849805 - val_loss: 2.089820587160341\n",
      "Epoch 108 - loss: 2.125163046229502 - val_loss: 2.2590340894240506\n",
      "Epoch 112 - loss: 2.113829057028703 - val_loss: 2.230165975904953\n",
      "Epoch 115 - loss: 2.096119422377159 - val_loss: 2.1410393050778382\n",
      "Epoch 116 - loss: 2.093441185836316 - val_loss: 2.111742646993102\n",
      "Epoch 118 - loss: 2.0840048678602177 - val_loss: 2.12676137313998\n",
      "Epoch 119 - loss: 2.0678124909666544 - val_loss: 2.17596537477219\n",
      "Epoch 128 - loss: 2.0554921866337956 - val_loss: 2.1504728595049665\n",
      "Epoch 133 - loss: 2.052121066604759 - val_loss: 1.9810551527412688\n",
      "Epoch 134 - loss: 2.047256183722454 - val_loss: 2.038017982013322\n",
      "Epoch 137 - loss: 2.0326423397108555 - val_loss: 2.0237397738745075\n",
      "Epoch 141 - loss: 2.0316475547964363 - val_loss: 1.9868223059911458\n",
      "Epoch 143 - loss: 2.0288934540066426 - val_loss: 2.063048921323478\n",
      "Epoch 145 - loss: 2.0164790444715357 - val_loss: 2.0195748577711474\n",
      "Epoch 148 - loss: 2.0036101592905577 - val_loss: 2.0444714278627654\n",
      "Epoch 150 - loss: 2.0027008230351435 - val_loss: 1.9216311609700996\n",
      "Epoch 154 - loss: 1.9923387431861694 - val_loss: 2.0452992808847683\n",
      "Epoch 159 - loss: 1.9899241603617737 - val_loss: 2.060559803920568\n",
      "Epoch 161 - loss: 1.9881997597185292 - val_loss: 1.9796919389441257\n",
      "Epoch 163 - loss: 1.9775910414489475 - val_loss: 2.027977659502425\n",
      "Epoch 164 - loss: 1.9753522785535091 - val_loss: 1.8498592506061513\n",
      "Epoch 168 - loss: 1.9742393370304891 - val_loss: 1.9374075475151087\n",
      "Epoch 170 - loss: 1.9739526528220555 - val_loss: 1.9971332571039753\n",
      "Epoch 171 - loss: 1.966654681428607 - val_loss: 1.8626179894827797\n",
      "Epoch 176 - loss: 1.966594528279828 - val_loss: 1.9668416709624612\n",
      "Epoch 177 - loss: 1.9647298261943291 - val_loss: 1.853298428561485\n",
      "Epoch 178 - loss: 1.9613262564727765 - val_loss: 1.8891094400588428\n",
      "Epoch 179 - loss: 1.9586256895874645 - val_loss: 1.901008718486736\n",
      "Epoch 180 - loss: 1.9553768264112936 - val_loss: 1.877806867939271\n",
      "Epoch 181 - loss: 1.9483436964937222 - val_loss: 1.8997617396224222\n",
      "Epoch 191 - loss: 1.945450559908353 - val_loss: 1.7977256183826396\n",
      "Epoch 194 - loss: 1.942080076616229 - val_loss: 1.9981526822735005\n",
      "Epoch 195 - loss: 1.9391112097428436 - val_loss: 1.8743556449622976\n",
      "Epoch 202 - loss: 1.9365947590945038 - val_loss: 1.8487642851335124\n",
      "Epoch 208 - loss: 1.9362293718894974 - val_loss: 1.8616528661101277\n",
      "Epoch 210 - loss: 1.932581592351218 - val_loss: 1.8048702980993603\n",
      "Epoch 214 - loss: 1.9309712985596663 - val_loss: 1.8295557170385783\n",
      "Epoch 215 - loss: 1.9293115881395073 - val_loss: 1.9008752670893867\n",
      "Epoch 216 - loss: 1.9259732529439222 - val_loss: 1.8041975934577075\n",
      "Epoch 219 - loss: 1.9241900976418405 - val_loss: 1.8352284935317165\n",
      "Epoch 220 - loss: 1.9171311151453443 - val_loss: 1.840533019700049\n",
      "Epoch 231 - loss: 1.9168851273224292 - val_loss: 1.822730003014622\n",
      "Epoch 232 - loss: 1.914145596817671 - val_loss: 1.7708819558190558\n",
      "Epoch 236 - loss: 1.9129877815090177 - val_loss: 1.8342274684389153\n",
      "Epoch 240 - loss: 1.910688878101038 - val_loss: 1.860319620278188\n",
      "Epoch 248 - loss: 1.9047991961195283 - val_loss: 1.7990144228766713\n",
      "Epoch 249 - loss: 1.9037946534340635 - val_loss: 1.8191419841999372\n",
      "Epoch 253 - loss: 1.9023812386782617 - val_loss: 1.8095154340613515\n",
      "Epoch 266 - loss: 1.8983828236474858 - val_loss: 1.8194935668310912\n",
      "Epoch 273 - loss: 1.8921113592208343 - val_loss: 1.7553452230304658\n",
      "Epoch 289 - loss: 1.8882811416345995 - val_loss: 1.790667065566478\n",
      "Epoch 309 - loss: 1.8870563494694934 - val_loss: 1.7990771137459916\n",
      "Epoch 311 - loss: 1.8851216971363607 - val_loss: 1.75122639031945\n",
      "Epoch 319 - loss: 1.884913997035956 - val_loss: 1.8143410361877692\n",
      "Epoch 332 - loss: 1.8819500516012995 - val_loss: 1.7249382255979924\n",
      "Epoch 333 - loss: 1.880376759522152 - val_loss: 1.8314072982535987\n",
      "Epoch 338 - loss: 1.8775534816809547 - val_loss: 1.827072169456541\n",
      "Epoch 345 - loss: 1.877330443078921 - val_loss: 1.838079866597775\n",
      "Epoch 351 - loss: 1.8755766235709794 - val_loss: 1.7518779024031848\n",
      "Epoch 353 - loss: 1.8733307124468273 - val_loss: 1.789967738351153\n",
      "Epoch 356 - loss: 1.8717298787026384 - val_loss: 1.7657649569154472\n",
      "Epoch 359 - loss: 1.8706445187021234 - val_loss: 1.840890906981688\n",
      "Epoch 363 - loss: 1.8650801213513406 - val_loss: 1.8214419671363289\n",
      "Epoch 371 - loss: 1.8599306230663515 - val_loss: 1.7740356505817243\n",
      "Epoch 389 - loss: 1.8551805307472127 - val_loss: 1.7893137066486964\n",
      "Epoch 404 - loss: 1.8540144536696348 - val_loss: 1.790856724007743\n",
      "Epoch 421 - loss: 1.8492979778010399 - val_loss: 1.7960498914411196\n",
      "Epoch 422 - loss: 1.8471149529839659 - val_loss: 1.7535335108843377\n",
      "Epoch 450 - loss: 1.8465552933120974 - val_loss: 1.801365231640616\n",
      "Epoch 461 - loss: 1.8454992923127163 - val_loss: 1.7841923490231408\n",
      "Epoch 465 - loss: 1.8413270071758998 - val_loss: 1.7735714232412583\n",
      "Epoch 486 - loss: 1.8392149436327938 - val_loss: 1.7963797790849096\n",
      "Epoch 489 - loss: 1.838657385770667 - val_loss: 1.7986774148615772\n",
      "Epoch 498 - loss: 1.8377143818719814 - val_loss: 1.8506654860675513\n",
      "Epoch 505 - loss: 1.83439692391126 - val_loss: 1.7812751080585545\n",
      "Epoch 512 - loss: 1.8332412702515135 - val_loss: 1.771930539506436\n",
      "Epoch 523 - loss: 1.8318114044532194 - val_loss: 1.8254071722923775\n",
      "Epoch 526 - loss: 1.8310998167854344 - val_loss: 1.8004851992369475\n",
      "Epoch 534 - loss: 1.8284759062299998 - val_loss: 1.7883384617225353\n",
      "Epoch 536 - loss: 1.8267429711035088 - val_loss: 1.8385167450222735\n",
      "Epoch 555 - loss: 1.8229442440184742 - val_loss: 1.7848071961295346\n",
      "Epoch 558 - loss: 1.8221096208131446 - val_loss: 1.7811126651818896\n",
      "Epoch 569 - loss: 1.8217066513526543 - val_loss: 1.8014991666431825\n",
      "Epoch 570 - loss: 1.8209430259891186 - val_loss: 1.8362992539852308\n",
      "Epoch 573 - loss: 1.818475301905168 - val_loss: 1.7727102692019614\n",
      "Epoch 585 - loss: 1.8152452676588982 - val_loss: 1.7891243223339628\n",
      "Epoch 605 - loss: 1.8137894463602133 - val_loss: 1.7589690405621206\n",
      "Epoch 615 - loss: 1.8133788044659842 - val_loss: 1.8038866857120504\n",
      "Epoch 627 - loss: 1.8121534722042905 - val_loss: 1.8357739208892554\n",
      "Epoch 635 - loss: 1.811389026918914 - val_loss: 1.8174802454027355\n",
      "Epoch 640 - loss: 1.8110802330601639 - val_loss: 1.8319777645208997\n",
      "Epoch 645 - loss: 1.8099350111054933 - val_loss: 1.784359218250286\n",
      "Epoch 654 - loss: 1.8063791292527343 - val_loss: 1.7708311393255074\n",
      "Epoch 658 - loss: 1.8055468240206998 - val_loss: 1.8311581442915539\n",
      "Epoch 663 - loss: 1.8053520267512573 - val_loss: 1.7966038682451622\n",
      "Epoch 664 - loss: 1.8007949369021257 - val_loss: 1.7973010992237548\n",
      "Epoch 695 - loss: 1.799644808610579 - val_loss: 1.7732904554210527\n",
      "Epoch 716 - loss: 1.7970724156090692 - val_loss: 1.80922430166959\n",
      "Epoch 722 - loss: 1.7958355063118732 - val_loss: 1.8121158719192167\n",
      "Epoch 743 - loss: 1.7918722631184585 - val_loss: 1.7565111992570202\n",
      "Epoch 750 - loss: 1.7862604454812203 - val_loss: 1.8075749559487644\n",
      "Epoch 791 - loss: 1.7855527965389837 - val_loss: 1.7916227274705911\n",
      "Epoch 803 - loss: 1.784508745655117 - val_loss: 1.7575327211613812\n",
      "Epoch 816 - loss: 1.7841677857294822 - val_loss: 1.7697634905134898\n",
      "Epoch 829 - loss: 1.7836920397708864 - val_loss: 1.7883434062051784\n",
      "Epoch 840 - loss: 1.7824215726344994 - val_loss: 1.746786900904718\n",
      "Epoch 860 - loss: 1.7810886563884125 - val_loss: 1.8030519095621382\n",
      "Epoch 861 - loss: 1.7806970028762719 - val_loss: 1.792391620300441\n",
      "Epoch 888 - loss: 1.7798643461947574 - val_loss: 1.8029888520782749\n",
      "Epoch 906 - loss: 1.7795137524504219 - val_loss: 1.7766695186366035\n",
      "Epoch 924 - loss: 1.7788684432894404 - val_loss: 1.7418019816030632\n",
      "Epoch 946 - loss: 1.7781979259161034 - val_loss: 1.7899720118008147\n",
      "Epoch 957 - loss: 1.7768807462821303 - val_loss: 1.8213328559533977\n",
      "Epoch 967 - loss: 1.774762072521181 - val_loss: 1.8061195942617592\n",
      "Epoch 992 - loss: 1.7740498155620383 - val_loss: 1.777060942610039\n"
     ]
    }
   ],
   "source": [
    "regressor = ERegressor(n = 100, hidden_layers = [8], activation = \"sigmoid\", random_state = 42)\n",
    "regressor.fit(scaled_X_train, y_train, epochs = 1000, validation_data = (scaled_X_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(scaled_X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7767500197737633"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x176732850>]"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = []\n",
    "xrange = 10000\n",
    "\n",
    "for x in range(1, xrange + 1):\n",
    "    y.append(5 * 1 / math.exp(x / (xrange / (10 * math.log10(xrange)))))\n",
    "\n",
    "plt.plot(list(range(xrange)), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x176666850>]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = []\n",
    "xrange = 10000\n",
    "\n",
    "for x in range(1, xrange + 1):\n",
    "    y.append(5 * 1 / math.exp(x / (xrange / 10)))\n",
    "\n",
    "plt.plot(list(range(xrange)), y)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "47d3b7ff548c1bae2d6b155a9b3d6f1122689b634566f833764ba5dd9fcfa2e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('Deep-learning-Daniel-Petersson-bXusHwTH')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
